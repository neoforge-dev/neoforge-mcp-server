This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-03-31T13:48:37.186Z

<file_summary>
This section contains a summary of this file.

<purpose>
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.
</purpose>

<file_format>
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Repository files, each consisting of:
  - File path as an attribute
  - Full contents of the file
</file_format>

<usage_guidelines>
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.
</usage_guidelines>

<notes>
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.
</notes>

<additional_info>

</additional_info>

</file_summary>

<directory_structure>
code_understanding/
  javascript_adapter.py
  language_parser.py
  semantic_analysis.py
  swift_adapter.py
docker/
memory-bank/
  .neorules
  active-context.md
  implementation_plan_ai_tools.md
  implementation_summary.md
  missing_functions.md
  product-context.md
  progress.md
  project-brief.md
  system-patterns.md
  tech-context.md
  test_implementation_plan.md
  test_suite_summary.md
monitoring/
  alertmanager-templates/
    slack.tmpl
  alertmanager.yml
  dashboard.py
  grafana-dashboard.json
  grafana-datasource.yml
  otel-collector-config.yaml
  prometheus-rules.yml
  prometheus.yml
output/
scripts/
  build_languages.py
server/
  code_understanding/
    tests/
      test_data_generators_test.py
      test_data_generators.py
    __init__.py
    analyzer.py
    build_languages.py
    common_types.py
    context_mapper.py
    extractor.py
    graph.py
    language_adapters.py
    mock_parser.py
    module_resolver.py
    parser.py
    relationship_extractor.py
    relationships.py
    semantic_analyzer.py
    symbols.py
  __init__.py
  core.py
  llm.py
tests/
  code_understanding/
    conftest.py
    test_analyzer.py
    test_extractor.py
    test_integration.py
    test_symbols.py
  data/
  test_l3_agent/
    test_code_generation.py
    test_code_validation.py
  conftest.py
  debug_js_variable.py
  missing_functions.py
  test_analyzer.py
  test_command_execution.py
  test_context_mapper.py
  test_file_operations.py
  test_graph.py
  test_javascript_parser.py
  test_javascript_support.py
  test_language_adapters.py
  test_language_support.py
  test_module_resolver.py
  test_observability_tools.py
  test_parser.py
  test_relationship_extractor.py
  test_relationships.py
  test_semantic_analysis.py
  test_semantic_analyzer.py
  test_system_utilities.py
.gitignore
.python-version
build_grammars.py
build.py
cli.py
cursor-tools.config.json
debug_js_ast.py
debugger.py
decorators.py
docker-compose.yml
llm_server.md
metrics.py
pyproject.toml
README.md
requirements.txt
run_servers.py
run_tests.py
test_manually.py
test_system_manually.py
workspace.py
</directory_structure>

<files>
This section contains the contents of the repository's files.

<file path="code_understanding/javascript_adapter.py">
'''python
"""JavaScript adapter for code parsing."""
def parse_javascript(code: str) -> dict:
    """
    Stub function for JavaScript code parsing.
    Args:
        code (str): JavaScript source code.
    Returns:
        dict: Dummy parsed representation of the JavaScript code.
    """
    return {
        "language": "javascript",
        "symbols": [],
        "functions": [],
        "classes": []
    }
'''
</file>

<file path="code_understanding/language_parser.py">
'''python
"""Module to handle multi-language parsing using language-specific adapters."""
from typing import Dict, Any
# Assuming existing python_adapter is available for Python parsing
from code_understanding.python_adapter import parse_python
from code_understanding.javascript_adapter import parse_javascript
from code_understanding.swift_adapter import parse_swift
class MultiLanguageParser:
    def parse_code(self, code: str, language: str) -> Dict[str, Any]:
        """Parses the provided code based on the specified language.
        Args:
            code: Source code to parse.
            language: The language of the source code (e.g., 'python', 'javascript', 'swift').
        Returns:
            A dictionary containing parsed code information.
        """
        lang = language.lower()
        if lang == 'python':
            return parse_python(code)
        elif lang == 'javascript':
            return parse_javascript(code)
        elif lang == 'swift':
            return parse_swift(code)
        else:
            raise ValueError(f"Unsupported language: {language}")
'''
</file>

<file path="code_understanding/semantic_analysis.py">
'''python
"""Semantic analysis module for performing type inference and building control flow graphs."""
def perform_type_inference(code: str) -> dict:
    """
    Stub function for performing type inference on given source code.
    Args:
        code (str): Source code to analyze.
    Returns:
        dict: Dummy type inference result with a placeholder 'types' key.
    """
    return {"types": {}}
def build_cfg(code: str) -> dict:
    """
    Stub function for building a control flow graph (CFG) for the given source code.
    Args:
        code (str): Source code to analyze.
    Returns:
        dict: Dummy control flow graph representation with a placeholder 'cfg' key.
    """
    return {"cfg": {}}
'''
</file>

<file path="code_understanding/swift_adapter.py">
'''python
"""Swift adapter for code parsing."""
def parse_swift(code: str) -> dict:
    """
    Stub function for Swift code parsing.
    Args:
        code (str): Swift source code.
    Returns:
        dict: Dummy parsed representation of the Swift code.
    """
    return {
        "language": "swift",
        "symbols": [],
        "functions": [],
        "classes": []
    }
'''
</file>

<file path="memory-bank/.neorules">
# Terminal Command Runner MCP - Project Rules and Patterns

## Dependency Management

- ALWAYS use `uv add` for installing Python packages
- NEVER use pip or requirements.txt
- ALL dependencies must be specified in pyproject.toml
- When suggesting dependency changes:
  1. Use `uv add [package]` for installation
  2. Update pyproject.toml as needed
  3. Document the change in active-context.md

## Code Quality

- Use ruff as the ONLY linting and formatting tool
- NEVER suggest using flake8, isort, or other linting tools
- All code quality configurations go in pyproject.toml
- Run ruff format before committing changes
- Run ruff lint to check for issues

## Testing Patterns

- Use pytest for all unit and integration tests
- Create test files with naming pattern `test_*.py`
- Use fixtures for test setup and teardown
- Mock system operations when possible to maintain test isolation
- Separate unit tests from integration tests
- Use temporary directories and files for file operation tests
- For command execution, use safe commands like `echo`, `cat`, etc.
- Use Docker for isolated testing environments
- Use Makefile commands for standardized test execution
- Always run tests in parallel with pytest-xdist when possible
- Collect coverage data for all test runs
- Mark tests with appropriate markers (unit, integration)

## Code Analysis

- Run static analysis before committing changes:
  1. Use `analyze_codebase` with type="all"
  2. Address any security issues immediately
  3. Review complexity hotspots
  4. Document dependencies in tech-context.md

- Monitor system performance during development:
  1. Use `monitor_performance` for resource-intensive operations
  2. Track baseline metrics
  3. Document performance regressions

- Manage LLM context effectively:
  1. Use `manage_llm_context` for large outputs
  2. Follow optimization suggestions
  3. Keep content within model limits
  4. Prioritize code blocks and error messages

## Code Style

- Follow ruff's style guide (which includes PEP 8)
- Use type hints for function parameters and return values
- Document functions with docstrings (Google style)
- Keep functions focused on a single responsibility
- Use meaningful variable and function names
- Keep cyclomatic complexity below 15
- Address complexity hotspots proactively

## Project Organization

- Keep main server code in server.py
- Place tests in a separate tests/ directory
- Use conftest.py for shared pytest fixtures
- Keep test data in tests/data/ directory
- Maintain pyproject.toml for all project configuration
- Document all new tools in tech-context.md

## Development Workflow

- Run tests before committing changes:
  1. Use `enhanced_testing` with parallel execution
  2. Review coverage reports
  3. Address any test failures

- Monitor system resources:
  1. Use `monitor_performance` during development
  2. Track resource usage trends
  3. Optimize resource-intensive operations

- Manage code quality:
  1. Run static analysis regularly
  2. Address security issues immediately
  3. Review complexity metrics
  4. Keep dependencies up to date

- Track LLM context:
  1. Monitor token usage
  2. Optimize large outputs
  3. Follow context management suggestions

## Common Commands

- Install dependency: `uv add [package]`
- Format code: `ruff format .`
- Lint code: `ruff check .`
- Run all tests: `enhanced_testing(test_type="all", parallel=True)`
- Run specific test: `enhanced_testing(test_type="unit", parallel=True)`
- Run with coverage: `enhanced_testing(coverage=True)`
- Analyze code: `analyze_codebase(analysis_type="all")`
- Monitor performance: `monitor_performance(duration=60)`
- Manage context: `manage_llm_context(content, model="claude-3-sonnet")`
- Format output: Use filter_output tool for long outputs
</file>

<file path="memory-bank/active-context.md">
# Terminal Command Runner MCP - Active Context

## Current Focus
- Finishing Code Understanding Tool implementation
  - **JavaScript Support:** Basic parser adapter implemented and passing initial tests (`test_language_adapters.py`). Need to run more comprehensive tests (`test_javascript_parser.py`, `test_javascript_support.py`) and ensure integration with analyzer.
  - Python Analyzer: Core functionality works correctly with the mock parser.
  - **Next Steps:**
    1. **Run comprehensive JavaScript parser tests.**
    2. Address Test Coverage: Increase overall coverage, focusing on JS adapter and Python analyzer/parser components.
- Intelligent Refactoring Tool implementation
- Test Generation Tool development 
- Dependency Impact Analysis Tool creation
- Code Review Automation Tool implementation
- Server connectivity and deployment troubleshooting
- Ensuring stable connections to the MCP server from remote clients
- Transport protocol compatibility (SSE vs WebSocket)
- Network configuration and firewall settings
- Port availability and configuration
- Proxy setup with Traefik
- Code generation system implementation
- Profiling system implementation
- Validation system implementation
- Model management system
- Performance optimization
- Security hardening
- Documentation updates
- Testing coverage
- Monitoring improvements
- Debugging enhancements
- Implementing multi-language support (Python, JavaScript, Swift)
- Adding semantic analysis capabilities
- Improving test coverage and documentation

## Recent Changes
- **Fixed Language Adapter Tests:** Resolved failures in `test_language_adapters.py` by correcting JS root node type conversion and adding `ValueError` for empty input to both JS and Swift parsers.
- **Fixed Mock Parser Fallback:** Corrected `CodeParser.parse` to properly use the `MockParser` instance from `mock_parser.py` during fallback.
- **Fixed Import Parsing:** 
  - Updated `MockParser` to correctly handle relative import levels (`.` and `..`).
  - Updated `CodeAnalyzer` to look for `import_statement` and `import_from_statement` node types.
- **Fixed Class Method Extraction:** Updated `CodeAnalyzer._extract_class` to correctly find the `body` node and extract `function_definition` children (methods) within it.
- **Corrected Test Assertions:** Adjusted assertions in `test_analyze_code` and `test_analyze_file` to reflect the design where class methods are stored within the class structure, not in the top-level functions list.
- **Added Debugging:** Enhanced logging and added print statements to trace execution flow during parser fallback.
- Evaluated and prioritized new MCP tools for AI Coding Agents
- Created detailed implementation plan for Code Understanding Tool
- Developed architecture for new AI agent tools
- Fixed server connectivity issue by changing transport from WebSocket to SSE
- Verified server is running on port 9001 with SSE transport
- Configured Traefik to route traffic to the MCP server
- Identified and fixed server startup errors
- Added code generation system with multiple model support
- Implemented advanced profiling system
- Added code validation system
- Enhanced monitoring with model metrics
- Improved security with model access control
- Added profiling dashboard
- Enhanced debugging with model tools
- Updated documentation
- Added new test cases
- Improved error handling
- Implemented core components:
  - Added `CodeParser` with tree-sitter integration and mock parser fallback
  - Created `CodeAnalyzer` for syntax tree analysis
  - Developed `SymbolExtractor` for symbol extraction
- Set up test infrastructure
- Added tree-sitter integration with robust error handling and fallback mechanism
- Fixed symbol extraction in `server/code_understanding/symbols.py`
  - Updated `_process_identifier` method to handle scopes correctly and track references
  - Added proper scope handling in `_is_parameter` method
  - Improved error handling in `extract_symbols` method
  - Enhanced `_get_node_text` to handle various text types
- Enhanced import handling in `server/code_understanding/analyzer.py`
  - Modified `_extract_imports` to properly split from-imports into individual imports
  - Ensured correct handling of import statements with multiple imports
- Improved node processing in `server/code_understanding/extractor.py`
  - Updated the `_process_node` method to manage scopes correctly
  - Enhanced `_process_import` to correctly extract module names
  - Fixed `_process_function` to properly extract parameter names
  - Added `_process_assignment` method to handle assignments
  - Set default scope to 'global' in constructor
- Added comprehensive implementation plan for multi-language support
- Created detailed semantic analysis architecture
- Updated testing strategy for new features

## Implementation Plan
1. AI Coding Agent MCP Tools (NEW HIGHEST PRIORITY)
   - [ ] Code Understanding Tool
     - [x] Core code analysis engine implementation (Parser, Analyzer, SymbolExtractor)
     - [x] Basic tree-sitter integration with mock fallback
     - [x] Symbol extraction system basics
     - [x] **Analyzer tests passing with mock parser**
     - [ ] Relationship graph builder development
     - [ ] Semantic mapper implementation
     - [ ] Code indexer development
     - [ ] MCP tool interface integration
     - [ ] Comprehensive testing (increase coverage)
   - [ ] Intelligent Refactoring Tool
   - [ ] Test Generation Tool
   - [ ] Dependency Impact Analysis Tool
   - [ ] Code Review Automation Tool

2. Server Connectivity (HIGH PRIORITY)
   - [x] Diagnose transport protocol issues
   - [x] Configure server to use supported SSE transport
   - [x] Verify server is listening on correct ports
   - [x] Test connectivity from various clients
   - [ ] Implement proper error handling for connection failures
   - [ ] Document connection process and troubleshooting steps
   - [ ] Set up monitoring for connection status

3. Code Generation System
   - [x] Basic model integration
   - [x] Model management
   - [x] Generation pipeline
   - [x] Validation integration
   - [ ] Advanced features
   - [ ] Performance optimization
   - [ ] Security hardening
   - [ ] Documentation
   - [ ] Testing

4. Profiling System
   - [x] Basic profiling
   - [x] Metrics collection
   - [x] Dashboard
   - [x] Analysis tools
   - [ ] Advanced features
   - [ ] Performance optimization
   - [ ] Security hardening
   - [ ] Documentation
   - [ ] Testing

5. Validation System
   - [x] Basic validation
   - [x] Check integration
   - [x] Analysis tools
   - [x] Reporting
   - [ ] Advanced features
   - [ ] Performance optimization
   - [ ] Security hardening
   - [ ] Documentation
   - [ ] Testing

6. Model Management
   - [x] API integration
   - [x] Local model support
   - [x] Resource management
   - [x] Performance tracking
   - [ ] Advanced features
   - [ ] Optimization
   - [ ] Security
   - [ ] Documentation
   - [ ] Testing

7. Performance Optimization
   - [x] Basic profiling
   - [x] Metrics collection
   - [x] Analysis tools
   - [ ] Advanced optimization
   - [ ] Resource management
   - [ ] Scaling
   - [ ] Documentation
   - [ ] Testing

8. Security Hardening
   - [x] Basic security
   - [x] Access control
   - [x] Input validation
   - [ ] Advanced security
   - [ ] Compliance
   - [ ] Auditing
   - [ ] Documentation
   - [ ] Testing

9. Documentation
   - [x] Basic documentation
   - [x] API documentation
   - [x] User guides
   - [ ] Advanced documentation
   - [ ] Examples
   - [ ] Tutorials
   - [ ] Testing

10. Testing
    - [x] Basic tests
    - [x] Integration tests
    - [x] System tests
    - [ ] Advanced tests
    - [ ] Performance tests
    - [ ] Security tests
    - [ ] Documentation

## Next Steps
1. **Run tests in `tests/test_javascript_parser.py`.**
2. **Address Test Coverage:** Focus on increasing coverage for `language_adapters.py` (JS part), `analyzer.py`, `mock_parser.py`, and `parser.py` based on the `coverage html` report.
3. Implement relationship graph building based on extracted symbols/references (for Python first).
4. Develop semantic mapping capabilities.
5. Create persistent indexing system.
6. Build MCP tool interface:
   - Design API interface
   - Implement command handlers
   - Create response formatters
7. Expand language support:
   - Add more tree-sitter parsers
   - Enhance language-specific analysis
8. Improve test coverage:
   - Add more unit tests
   - Implement integration tests
   - Test edge cases
9. Optimize performance:
   - Profile code execution
   - Optimize memory usage
   - Add caching where beneficial

## Active Decisions
- Confirmed `CodeAnalyzer` separates top-level functions from class methods.
- Confirmed `MockParser` from `mock_parser.py` is now correctly used during fallback.

## Known Issues
- ⚠️ **Low Code Coverage:** Overall coverage is ~12%, significantly below the 90% target. Many components have low or zero coverage, **including the new JS adapter code.**
- Server Connectivity
   - Transport protocol limitations (WebSocket not supported)
   - Potential firewall restrictions on cloud provider
   - Proxy configuration complexity
   - Connection timeouts from certain networks
   - Error handling limitations
   - Lack of comprehensive documentation

## Dependencies
1. External Services
   - Anthropic API
   - OpenAI API
   - Local model servers
   - Monitoring stack
   - Alert system
   - Debug interface
   - Profiling system
   - Validation system

2. Internal Components
   - Tool registry
   - Workspace management
   - Configuration system
   - Event system
   - Metrics collection
   - Logging system
   - Model management
   - Profiling management
   - Validation management

3. Development Tools
   - Code analysis
   - Testing framework
   - Debugging tools
   - Documentation system
   - Model development
   - Profiling tools
   - Validation tools

4. Monitoring Tools
   - Metrics collection
   - Alert management
   - Dashboard system
   - Model monitoring
   - Profiling monitoring
   - Validation monitoring

5. Security Tools
   - Access control
   - Input validation
   - Command filtering
   - Model security
   - Profiling security
   - Validation security

6. Performance Tools
   - Resource monitoring
   - Load testing
   - Benchmarking
   - Model optimization
   - Profiling optimization
   - Validation optimization

7. Documentation Tools
   - API documentation
   - Code documentation
   - System documentation
   - Model documentation
   - Profiling documentation
   - Validation documentation

8. Testing Tools
   - Unit testing
   - Integration testing
   - System testing
   - Model testing
   - Profiling testing
   - Validation testing

9. Deployment Tools
   - Docker support
   - Configuration management
   - Environment setup
   - Model deployment
   - Profiling deployment
   - Validation deployment

10. Maintenance Tools
    - Log management
    - Backup system
    - Update management
    - Model maintenance
    - Profiling maintenance
    - Validation maintenance

## Less Relevant Areas (Deprioritized)
- Advanced UI customization features
- Support for deprecated protocols
- Low-priority integrations with external systems
- Optimization for resource-constrained environments
- Legacy compatibility features 

## Current Status
- ✅ All 34 tests are now passing
- ⚠️ Code coverage is at 24%, below the required 90% threshold
- Core functionality for symbol extraction is working correctly
- Import handling is fixed to meet test expectations
- Scope management for identifiers and references is working properly

## Active Decisions
- Focusing on implementing core functionality before addressing code coverage
- Using tree-sitter integration for code parsing where available
- Falling back to AST when tree-sitter is not available
- Maintaining scope tracking during node traversal to ensure proper symbol resolution

## Next Steps
1. Address the code coverage issue:
   - Implement additional tests for untested code paths in all modules
   - Focus on `graph.py` and `relationships.py` which currently have 0% coverage
   - Add tests for error handling paths in all modules

2. Complete remaining functionality:
   - Implement relationship extraction for building call graphs
   - Add semantic analysis functionality
   - Create visualization exports for code graphs
   - Develop persistent storage for code analysis results

3. Integration and performance:
   - Integrate with the MCP tool interface
   - Optimize performance for large codebases
   - Implement incremental updates for efficient re-analysis

## Current Implementation Status

### Relationship Builder
- ✅ Enhanced `_process_references` method with comprehensive relationship handling
  - ✅ Function/method call tracking with scope awareness
  - ✅ Variable reference handling with proper scoping
  - ✅ Attribute reference support with class context
  - ✅ Edge properties for line numbers and scopes
- ✅ Improved node creation logic
  - ✅ External function nodes
  - ✅ Scoped variable nodes
  - ✅ Attribute nodes with class context
- ✅ Enhanced test coverage
  - ✅ Comprehensive test cases for graph operations
  - ✅ Detailed relationship building tests
  - ✅ Edge case handling tests

### Known Issues
- ⚠️ Test failures in relationship extraction
  - No nodes being created during file analysis
  - Directory analysis not producing expected nodes
  - Reference processing not creating edges
- ⚠️ Low code coverage (24%)
  - Core components need additional test coverage
  - Error handling paths require testing
  - Edge cases need coverage

### Next Actions
1. Debug relationship extraction failures:
   - Investigate node creation in file analysis
   - Debug directory traversal and analysis
   - Fix reference processing for edge creation

2. Improve test coverage:
   - Add tests for error handling paths
   - Create edge case test scenarios
   - Implement integration tests

3. Enhance error handling:
   - Add robust error recovery
   - Improve error logging
   - Implement validation checks

4. Documentation:
   - Document relationship types
   - Create usage examples
   - Add troubleshooting guide

## Active Decisions
1. Language Support Strategy:
   - Using tree-sitter for all languages
   - Implementing language-specific adapters
   - Creating unified symbol resolution system

2. Semantic Analysis Approach:
   - Layered analysis (syntactic → semantic)
   - Language-specific type systems
   - Context-sensitive analysis

3. Testing Strategy:
   - Language-specific test suites
   - Cross-language integration tests
   - Performance benchmarking

## Next Steps
1. Language Support Implementation:
   - Set up tree-sitter grammars for JavaScript and Swift
   - Create language detection system
   - Implement language-specific adapters
   - Build cross-language reference handling

2. Semantic Analysis Development:
   - Implement type system components
   - Create control flow analysis
   - Build semantic graph construction
   - Develop query API

3. Testing and Documentation:
   - Create comprehensive test suites
   - Develop performance benchmarks
   - Write detailed documentation
   - Create examples and tutorials

## Current Status
- Core Python support: Complete
- JavaScript support: Planning phase
- Swift support: Planning phase
- Semantic analysis: Design phase
- Test coverage: 24% (needs improvement)

## Implementation Timeline
1. Phase 1: Core Language Support (Weeks 1-5)
2. Phase 2: Basic Semantic Analysis (Weeks 6-10)
3. Phase 3: Advanced Semantic Features (Weeks 11-16)
4. Phase 4: Testing and Optimization (Weeks 17-22)

## Known Issues
1. Low test coverage needs immediate attention
2. Relationship extraction failures in complex cases
3. Performance optimization needed for large codebases
4. Documentation needs updating for new features

## Dependencies
- tree-sitter grammars for JavaScript and Swift
- Performance profiling tools
- Test coverage tools
- Documentation generation tools
</file>

<file path="memory-bank/implementation_plan_ai_tools.md">
# AI Coding Agent MCP Tools - Implementation Plan

## Overview

This document details the implementation plan for adding five new AI Coding Agent tools to the MCP platform. These tools will significantly enhance the capabilities of AI coding assistants by providing deeper code understanding and manipulation abilities.

## Tools Summary

1. **Code Understanding Tool** (Priority: Highest)
   - Deep code analysis with semantic relationship mapping
   - Language-agnostic parsing with language-specific analyzers
   - Persistent indexing for performance
   - Graph-based visualization of code relationships

2. **Intelligent Refactoring Tool** (Priority: High)
   - Behavior-preserving code modifications
   - Support for common refactoring operations
   - Validation of refactoring correctness
   - Multi-file refactoring capabilities

3. **Test Generation Tool** (Priority: Medium)
   - Automated test case generation
   - Edge case discovery and coverage optimization
   - Support for multiple testing frameworks
   - Test suite management

4. **Dependency Impact Analysis Tool** (Priority: Medium)
   - Dependency graph construction and visualization
   - Impact prediction for dependency changes
   - Security vulnerability detection
   - Change recommendation based on impact analysis

5. **Code Review Automation Tool** (Priority: Medium)
   - Style checking against team standards
   - Best practice verification
   - Performance hotspot detection
   - Security vulnerability scanning

## Detailed Implementation Plan

### Phase 1: Code Understanding Tool (Weeks 1-5)

#### Week 1: Setup & Core Analysis
- Set up project structure with proper packaging
- Implement tree-sitter integration for parsing
- Create language-specific parsers for Python and JavaScript
- Build core analysis infrastructure
- Implement basic symbol extraction

**Deliverables:**
- Working parser for Python and JavaScript files
- Symbol extraction system (functions, classes, variables)
- Initial project structure with tests

#### Week 2: Relationship Graph
- Implement graph data structure for code relationships
- Build relationship extraction logic
- Create visualization export formats (JSON, GraphViz)
- Develop incremental graph updates for performance
- Implement cross-file relationship tracking

**Deliverables:**
- Relationship graph builder
- Visualization export system
- Cross-file reference tracking
- Incremental update system

#### Week 3: Semantic Mapping
- Implement semantic extraction from code comments and identifiers
- Build context mapping system linking code to natural language
- Create embedding-based search capabilities
- Integrate semantic mapper with analysis engine
- Implement identifier meaning extraction

**Deliverables:**
- Semantic mapping system
- Embedding-based code search
- Context extraction from comments
- Identifier meaning inference

#### Week 4: Indexing & Integration
- Implement persistent index storage for analysis results
- Build incremental index update system
- Create MCP tool interface with proper parameters and returns
- Write comprehensive tests for all components
- Implement error handling and logging

**Deliverables:**
- Persistent indexing system
- Complete MCP tool interface
- Comprehensive test suite
- Error handling system

#### Week 5: Testing & Documentation
- Implement end-to-end tests with real-world codebases
- Write comprehensive documentation
- Optimize performance for large codebases
- Create example workflows and use cases
- Fine-tune memory usage and processing efficiency

**Deliverables:**
- End-to-end tests with performance benchmarks
- Comprehensive documentation
- Optimized implementation
- Example workflows

### Phase 2: Intelligent Refactoring Tool (Weeks 6-8)

#### Week 6: Core Refactoring Engine
- Implement abstract syntax tree (AST) manipulation engine
- Create refactoring operation framework
- Implement basic refactorings (rename, extract method)
- Build validation system for behavior preservation

**Deliverables:**
- AST manipulation engine
- Refactoring operation framework
- Basic refactoring implementations
- Validation system

#### Week 7: Advanced Refactoring
- Implement advanced refactorings (move method, extract class)
- Create multi-file refactoring capabilities
- Build refactoring preview system
- Implement refactoring plan generation

**Deliverables:**
- Advanced refactoring operations
- Multi-file refactoring support
- Refactoring preview system
- Plan generation system

#### Week 8: Testing & Integration
- Write comprehensive tests for all refactorings
- Integrate with Code Understanding Tool
- Implement MCP tool interface
- Create documentation and examples

**Deliverables:**
- Comprehensive test suite
- Integration with Code Understanding Tool
- Complete MCP tool interface
- Documentation and examples

### Phase 3: Test Generation Tool (Weeks 9-10)

#### Week 9: Core Test Generation
- Implement test case analysis and generation
- Create test template system for different frameworks
- Build coverage analysis and optimization
- Implement edge case detection

**Deliverables:**
- Test case generation engine
- Framework-specific templates
- Coverage analysis system
- Edge case detection

#### Week 10: Integration & Testing
- Write tests for the test generator
- Integrate with Code Understanding Tool
- Implement MCP tool interface
- Create documentation and examples

**Deliverables:**
- Comprehensive test suite
- Integration with Code Understanding Tool
- Complete MCP tool interface
- Documentation and examples

### Phase 4: Dependency Impact Analysis Tool (Weeks 11-12)

#### Week 11: Dependency Analysis
- Implement dependency graph construction
- Create impact prediction system
- Build visualization for dependency relationships
- Implement change recommendation system

**Deliverables:**
- Dependency graph builder
- Impact prediction system
- Visualization system
- Change recommendation engine

#### Week 12: Integration & Testing
- Write tests for dependency analysis
- Integrate with Code Understanding Tool
- Implement MCP tool interface
- Create documentation and examples

**Deliverables:**
- Comprehensive test suite
- Integration with Code Understanding Tool
- Complete MCP tool interface
- Documentation and examples

### Phase 5: Code Review Automation Tool (Weeks 13-14)

#### Week 13: Review Engine
- Implement style checking system
- Create best practice verification
- Build performance analysis
- Implement security scanning

**Deliverables:**
- Style checking system
- Best practice verification
- Performance analysis
- Security scanning

#### Week 14: Integration & Testing
- Write tests for code review automation
- Integrate with Code Understanding Tool
- Implement MCP tool interface
- Create documentation and examples

**Deliverables:**
- Comprehensive test suite
- Integration with Code Understanding Tool
- Complete MCP tool interface
- Documentation and examples

### Phase 6: Finalization (Week 15)

#### Week 15: System Integration & Documentation
- Ensure all tools work together seamlessly
- Optimize performance of the entire system
- Create comprehensive documentation
- Build demonstration examples

**Deliverables:**
- Integrated system with all tools
- Performance optimization
- Comprehensive documentation
- Demonstration examples

## Dependencies & Resources

### External Libraries
- tree-sitter: For language-agnostic parsing
- networkx: For graph data structures and algorithms
- sentence-transformers: For embedding generation
- pytest: For testing framework
- sqlitedict: For persistent storage
- typing-extensions: For enhanced type annotations

### Development Environment
- Python 3.8+
- Git for version control
- CI/CD integration for automated testing
- Documentation generation tools

### Team Resources
- 2 Senior Python Developers
- 1 Software Architect
- 1 QA Engineer (part-time)
- Code review sessions twice weekly

## Success Metrics

1. **Code Understanding Tool**
   - Successfully analyze codebases up to 500K LOC
   - Support for at least 5 programming languages
   - Query response time under 100ms for indexed code
   - Memory usage under 1GB for 100K LOC

2. **Intelligent Refactoring Tool**
   - Support for at least 10 common refactoring operations
   - Success rate of at least 95% for automated refactorings
   - Validation accuracy of at least 99%

3. **Test Generation Tool**
   - Generate tests with at least 80% coverage for typical code
   - Support for at least 3 testing frameworks
   - At least 90% of generated tests pass when run

4. **Dependency Impact Analysis Tool**
   - Accurately identify at least 95% of impacted code
   - Support for at least 3 dependency management systems
   - Visualization rendering under 2 seconds for large projects

5. **Code Review Automation Tool**
   - At least 90% agreement with human reviewers on issues
   - Support for at least 5 common style guides
   - Processing time under 5 seconds for typical files

## Risk Assessment

### Technical Risks
- Complex codebase analysis may be resource-intensive
- Language support requires significant expertise in each language
- Maintaining behavior preservation during refactoring is challenging
- Test generation requires deep understanding of code semantics

### Mitigation Strategies
- Implement incremental analysis and caching for performance
- Start with core languages (Python, JavaScript) and expand gradually
- Use robust testing to validate behavior preservation
- Begin with simpler test generation scenarios and expand capabilities

## Rollout Plan

1. **Alpha Testing** (Internal)
   - Code Understanding Tool: Week 5
   - Intelligent Refactoring Tool: Week 8
   - Test Generation Tool: Week 10
   - Dependency Impact Analysis Tool: Week 12
   - Code Review Automation Tool: Week 14

2. **Beta Testing** (Limited External)
   - Release all tools to selected early adopters by Week 15
   - Gather feedback and make improvements for 2 weeks

3. **General Availability**
   - Release all tools to general users by Week 18

## Conclusion

This implementation plan outlines a comprehensive approach to developing five powerful AI Coding Agent tools that will significantly enhance the capabilities of the MCP platform. By following this phased approach, we can ensure that each tool is built with quality, tested thoroughly, and integrated seamlessly into the existing system.
</file>

<file path="memory-bank/implementation_summary.md">
# Terminal Command Runner MCP - Implementation Summary

## Accomplished

1. **Function Implementation**:
   - Implemented all file operations functions (read, write, create, list, move, search, info)
   - Implemented all process management functions (list, kill, sessions)
   - Implemented command control functions (block, unblock)
   - Implemented utility functions (system_info, calculate, edit_block)
   - Added proper error handling and security checks

2. **Testing**:
   - Created comprehensive test files covering all functionality
   - Implemented manual test scripts that verify functionality
   - Successfully tested all implemented functions
   - Fixed server startup issue that was causing test environment problems

3. **Documentation**:
   - Updated Memory Bank with implementation details
   - Created detailed test suite documentation
   - Documented known issues and next steps
   - Updated README with testing information

## Next Steps

1. **Test Environment Improvements**:
   - Fix the pytest integration to enable automated testing
   - Configure the virtual environment properly
   - Implement better test isolation

2. **Enhanced Testing**:
   - Add more edge case tests
   - Create more comprehensive platform-specific tests
   - Add performance tests for long-running operations

3. **CI/CD Integration**:
   - Configure GitHub Actions for automated testing
   - Set up test coverage reporting
   - Implement linting and code quality checks

4. **Documentation Enhancements**:
   - Complete API documentation for all functions
   - Create usage examples
   - Develop a comprehensive deployment guide

## Technical Debt

1. **File Path Validation**: Need stronger validation to prevent directory traversal
2. **Output Buffering**: Need to handle very large command output more efficiently
3. **Process Management**: Need to handle edge cases in process termination
4. **Cross-Platform Compatibility**: Need to improve handling of platform-specific behavior
5. **Error Handling**: Need more robust error handling for edge cases

## Conclusion

The Terminal Command Runner MCP now has a complete implementation of all the required functionality. Manual tests confirm that the core features work as expected. The next focus should be on improving the test environment to enable automated testing and addressing the identified technical debt issues.
</file>

<file path="memory-bank/missing_functions.md">
# Missing Functions for Test Suite

Based on analysis of the test suite, the following functions need to be implemented in `server.py` to make the tests work:

## Global Variables
- `active_sessions`: Dictionary to track running command sessions
- `session_lock`: Threading lock for thread-safe access to active_sessions
- `blacklisted_commands`: Set of commands that are blocked for security reasons

## Functions

### Process Management
- `list_sessions()`: List all active command sessions
- `list_processes()`: List all system processes
- `kill_process()`: Kill a process by PID with configurable signal

### File Operations
- `read_file()`: Read file contents with size limits
- `write_file()`: Write content to a file
- `create_directory()`: Create a new directory
- `list_directory()`: List directory contents
- `move_file()`: Move or rename files
- `search_files()`: Find files matching a pattern
- `get_file_info()`: Get metadata about a file

### Command Control
- `block_command()`: Add a command to the blacklist
- `unblock_command()`: Remove a command from the blacklist

### Utilities
- `system_info()`: Get system information
- `calculate()`: Evaluate a mathematical expression
- `edit_block()`: Apply edits to a file with a diff-like syntax

## Implementation Guidelines

Each function should:
- Be decorated with `@mcp.tool()`
- Include proper docstrings with parameter descriptions
- Return results in a dictionary format
- Include appropriate error handling
- Implement security checks where needed

Example template for a new function:

```python
@mcp.tool()
def function_name(param1: type, param2: type = default) -> Dict[str, Any]:
    """
    Description of what the function does.
    
    Args:
        param1: Description of param1
        param2: Description of param2
    
    Returns:
        Dictionary with operation results
    """
    try:
        # Implementation
        result = {"success": True, "key": value}
    except Exception as e:
        result = {"success": False, "error": str(e)}
    
    return result
```
</file>

<file path="memory-bank/product-context.md">
# Terminal Command Runner MCP - Product Context

## Purpose

The Terminal Command Runner MCP serves as a powerful bridge between AI systems and the operating system, enabling safe, controlled execution of terminal commands, file operations, and system utilities. It provides a standardized API that maintains security while allowing flexible access to system resources.

## Problems Solved

1. **Security Concerns**: Provides a controlled environment for executing terminal commands with appropriate safeguards
2. **Standardized Access**: Offers a consistent API for system operations across different platforms
3. **Process Management**: Enables long-running background processes with reliable output streaming
4. **File Operations**: Facilitates secure file access and manipulation with proper error handling
5. **AI Code Generation Limitations**: Empowers AI Coding Agents with deeper code understanding and advanced capabilities beyond simple text generation

## User Experience Goals

1. **Reliability**: Ensure commands execute predictably with proper error handling
2. **Security**: Prevent dangerous operations while allowing legitimate work
3. **Flexibility**: Support a wide range of terminal operations
4. **Performance**: Execute commands with minimal overhead
5. **Intelligence**: Enable AI Coding Agents to understand, modify, and test code with human-like comprehension

## How It Works

The MCP server acts as an intermediary between clients (like AI systems) and the operating system. When a client makes a request:

1. The request is received through the SSE transport layer
2. The appropriate tool handler processes the request
3. Security checks are performed
4. The operation is executed
5. Results are streamed back to the client
6. Resources are properly cleaned up

For AI Coding Agents, the system provides specialized tools that go beyond simple terminal commands:

1. **Code Understanding Tool**: Parses and analyzes code to create relationship graphs and semantic maps
2. **Intelligent Refactoring Tool**: Safely modifies code while preserving behavior
3. **Test Generation Tool**: Creates comprehensive test suites based on code analysis
4. **Dependency Impact Analysis Tool**: Predicts the effects of dependency changes
5. **Code Review Automation Tool**: Evaluates code against best practices and standards

## Integration Points

1. **Client Applications**: Connect via the SSE transport protocol
2. **Operating System**: Execute commands and access system resources
3. **File System**: Perform read/write operations
4. **Process Manager**: Start, monitor, and terminate processes
5. **Code Analysis Systems**: Integrate with parsing and static analysis tools
6. **Testing Frameworks**: Connect with testing infrastructure
7. **Version Control Systems**: Access and modify code repositories

## Target Users

1. **AI Systems**: LLMs and other AI systems requiring system access
2. **Developers**: Engineers building AI-powered applications
3. **System Administrators**: Managing and monitoring system resources
4. **DevOps Engineers**: Automating workflows and deployments
5. **AI Coding Agents**: Advanced systems requiring deep code understanding and manipulation

## Benefits

1. **Enhanced Security**: Controlled access to system resources
2. **Improved Reliability**: Consistent handling of commands and processes
3. **Simplified Integration**: Standardized API for system operations
4. **Efficient Process Management**: Reliable handling of background processes
5. **Deeper Code Understanding**: AI agents can comprehend code structure and relationships
6. **Safer Code Modifications**: Intelligent refactoring with behavior preservation
7. **Improved Test Coverage**: Automated test generation with edge case detection
8. **Better Dependency Management**: Impact analysis for dependency changes
9. **Higher Code Quality**: Automated code review and best practice verification

## Future Directions

1. **Enhanced Security**: Additional safeguards and fine-grained permissions
2. **Extended Tool Support**: More specialized tools for specific domains
3. **Platform Expansion**: Support for additional operating systems
4. **Performance Optimization**: Reduced overhead and improved efficiency
5. **Advanced Code Understanding**: Support for more languages and frameworks
6. **Intelligent Code Generation**: Context-aware code creation capabilities
7. **Interactive Refactoring**: Multi-step refactoring with user feedback
8. **Comprehensive Testing**: Advanced test generation with behavior verification
9. **Real-time Collaboration**: Support for multiple agents working together

## Expected Workflow

1. Client connects to the MCP server via SSE (Server-Sent Events)
2. Client invokes tools to execute commands or perform file operations
3. Server executes the requested operations with appropriate security checks
4. Results are returned to the client with standardized output format
5. Long-running processes can be monitored and managed independently

## Integration Context

The MCP server is designed to be used as part of a larger system, often integrated with:
- Development tools and IDEs
- DevOps and CI/CD pipelines
- System administration utilities
- Automation frameworks
</file>

<file path="memory-bank/progress.md">
# Terminal Command Runner MCP - Progress Report

## Completed Features

### Server Connectivity
- ✅ Diagnosed server connectivity issues
- ✅ Fixed transport protocol configuration (switched from WebSocket to SSE)
- ✅ Verified server listening on port 9001
- ✅ Configured SSH access for server management
- ✅ Set up Traefik proxy for routing

### Core Functionality
- ✅ Basic command execution
- ✅ File operations
- ✅ Process management
- ✅ System information
- ✅ Workspace management
- ✅ Configuration system
- ✅ Event system
- ✅ Metrics collection
- ✅ Logging system
- ✅ Debug interface
- ✅ Model management
- ✅ Profiling system
- ✅ Validation system

### Code Understanding Tool
- ✅ Parser layer implementation
  - ✅ Tree-sitter integration
  - ✅ AST fallback mechanism
  - ✅ Mock parser for testing
  - ✅ Node traversal utilities
- ✅ Analyzer layer implementation
  - ✅ Import extraction (including relative imports)
  - ✅ Function extraction (top-level)
  - ✅ Class extraction (including methods)
  - ✅ Variable extraction (top-level)
- ✅ Symbol extraction layer implementation
  - ✅ Symbol table management
  - ✅ Scope handling
  - ✅ Reference tracking
  - ✅ Type handling
- ✅ **Basic Analyzer Testing**
  - ✅ All tests in `tests/test_analyzer.py` passing with mock parser.

### Code Generation
- ✅ Basic model integration
- ✅ Model management
- ✅ Generation pipeline
- ✅ Validation integration
- ✅ Error handling
- ✅ Resource management
- ✅ Performance tracking
- ✅ Security controls
- ✅ Documentation
- ✅ Testing

### Profiling
- ✅ Basic profiling
- ✅ Metrics collection
- ✅ Dashboard
- ✅ Analysis tools
- ✅ Error handling
- ✅ Resource management
- ✅ Performance tracking
- ✅ Security controls
- ✅ Documentation
- ✅ Testing

### Validation
- ✅ Basic validation
- ✅ Check integration
- ✅ Analysis tools
- ✅ Reporting
- ✅ Error handling
- ✅ Resource management
- ✅ Performance tracking
- ✅ Security controls
- ✅ Documentation
- ✅ Testing

### Model Management
- ✅ API integration
- ✅ Local model support
- ✅ Resource management
- ✅ Performance tracking
- ✅ Error handling
- ✅ Security controls
- ✅ Documentation
- ✅ Testing

### Performance
- ✅ Basic profiling
- ✅ Metrics collection
- ✅ Analysis tools
- ✅ Error handling
- ✅ Resource management
- ✅ Security controls
- ✅ Documentation
- ✅ Testing

### Security
- ✅ Basic security
- ✅ Access control
- ✅ Input validation
- ✅ Error handling
- ✅ Resource protection
- ✅ Documentation
- ✅ Testing

### Documentation
- ✅ Basic documentation
- ✅ API documentation
- ✅ User guides
- ✅ Error handling
- ✅ Resource management
- ✅ Security controls
- ✅ Testing

### Testing
- ✅ Basic tests
- ✅ Integration tests
- ✅ System tests
- ✅ Error handling
- ✅ Resource management
- ✅ Security controls
- ✅ Documentation

## In Progress

### AI Coding Agent MCP Tools
- 🔄 **Code Understanding Tool - Test Coverage & Enhancement**
  - 🔄 Addressing code coverage for `analyzer.py`, `parser.py`, `mock_parser.py` (currently low ~15%).
  - 🔄 Implementing relationship extraction for code understanding.
  - 🔄 Developing graph representation for code relationships.
  - 🔄 Creating visualization exports for code graphs.

### Test Coverage Implementation
- 🔄 Evaluation of current test coverage
- 🔄 Implementation of missing tests for graph.py
- 🔄 Implementation of missing tests for relationships.py
- 🔄 Adding error handling test cases
- 🔄 Creating edge case test scenarios
- 🔄 Developing integration tests

### Server Connectivity
- 🔄 Remote client connectivity testing
- 🔄 Connection error handling
- 🔄 Network troubleshooting documentation
- 🔄 Connection monitoring implementation
- 🔄 Connectivity resilience features

### Code Generation
- 🔄 Advanced features
- 🔄 Performance optimization
- 🔄 Security hardening
- 🔄 Documentation updates
- 🔄 Test coverage

### Profiling
- 🔄 Advanced features
- 🔄 Performance optimization
- 🔄 Security hardening
- 🔄 Documentation updates
- 🔄 Test coverage

### Validation
- 🔄 Advanced features
- 🔄 Performance optimization
- 🔄 Security hardening
- 🔄 Documentation updates
- 🔄 Test coverage

### Model Management
- 🔄 Advanced features
- 🔄 Performance optimization
- 🔄 Security hardening
- 🔄 Documentation updates
- 🔄 Test coverage

### Performance
- 🔄 Advanced optimization
- 🔄 Resource management
- 🔄 Scaling
- 🔄 Documentation updates
- 🔄 Test coverage

### Security
- 🔄 Advanced security
- 🔄 Compliance
- 🔄 Auditing
- 🔄 Documentation updates
- 🔄 Test coverage

### Documentation
- 🔄 Advanced documentation
- 🔄 Examples
- 🔄 Tutorials
- 🔄 Resource management
- 🔄 Test coverage

### Testing
- 🔄 Advanced tests
- 🔄 Performance tests
- 🔄 Security tests
- 🔄 Resource management
- 🔄 Documentation

## Planned Features

### AI Coding Agent MCP Tools
- 📋 Code Understanding Tool
  - 📋 **Test Coverage Improvement**
    - 📋 Implement tests for currently uncovered paths in `analyzer.py`, `parser.py`, `mock_parser.py`.
    - 📋 Implement tests for graph.py (currently 0% coverage).
    - 📋 Implement tests for relationships.py (currently 0% coverage).
    - 📋 Add tests for error handling paths.
    - 📋 Create tests for edge cases.
    - 📋 Develop integration tests for end-to-end workflows.
  - 📋 Relationship Graph Implementation
    - 📋 Implement relationship extraction logic.
    - 📋 Complete graph data structure implementation.
    - 📋 Build call graph representation.
    - 📋 Create inheritance hierarchy visualization.
    - 📋 Implement dependency tracking.
  - 📋 Semantic Mapping
  - 📋 Indexing & Integration
  - 📋 Performance Optimization
- 📋 Intelligent Refactoring Tool
- 📋 Test Generation Tool
- 📋 Dependency Impact Analysis Tool
- 📋 Code Review Automation Tool

### Test Coverage Implementation
- 📋 Implement tests for observability tools
  - 📋 Test get_trace_info functionality
  - 📋 Test configure_tracing functionality
  - 📋 Test get_metrics_info functionality
  - 📋 Test configure_metrics functionality
- 📋 Implement tests for development tools
  - 📋 Test install_dependency functionality
  - 📋 Test run_tests functionality
  - 📋 Test format_code functionality
  - 📋 Test lint_code functionality
- 📋 Implement tests for monitoring tools
  - 📋 Test monitor_performance functionality
- 📋 Implement tests for documentation tools
  - 📋 Test generate_documentation functionality
- 📋 Implement tests for project management tools
  - 📋 Test setup_validation_gates functionality
  - 📋 Test analyze_project functionality
  - 📋 Test manage_changes functionality

### Server Connectivity
- 📋 Implement auto-reconnection
- 📋 Add comprehensive connection logging
- 📋 Create detailed troubleshooting guide
- 📋 Set up connection status dashboard
- 📋 Implement connection health checks

### Code Generation
- 📋 Advanced model integration
- 📋 Custom model support
- 📋 Advanced validation
- 📋 Performance optimization
- 📋 Security hardening
- 📋 Documentation updates
- 📋 Test coverage

### Profiling
- 📋 Advanced profiling
- 📋 Custom metrics
- 📋 Advanced analysis
- 📋 Performance optimization
- 📋 Security hardening
- 📋 Documentation updates
- 📋 Test coverage

### Validation
- 📋 Advanced validation
- 📋 Custom checks
- 📋 Advanced analysis
- 📋 Performance optimization
- 📋 Security hardening
- 📋 Documentation updates
- 📋 Test coverage

### Model Management
- 📋 Advanced management
- 📋 Custom providers
- 📋 Advanced optimization
- 📋 Security hardening
- 📋 Documentation updates
- 📋 Test coverage

### Performance
- 📋 Advanced optimization
- 📋 Custom metrics
- 📋 Advanced scaling
- 📋 Security hardening
- 📋 Documentation updates
- 📋 Test coverage

### Security
- 📋 Advanced security
- 📋 Custom controls
- 📋 Advanced compliance
- 📋 Documentation updates
- 📋 Test coverage

### Documentation
- 📋 Advanced documentation
- 📋 Custom guides
- 📋 Advanced examples
- 📋 Resource management
- 📋 Test coverage

### Testing
- 📋 Advanced tests
- 📋 Custom scenarios
- 📋 Advanced coverage
- 📋 Resource management
- 📋 Documentation

## Known Issues

### Test Coverage Gaps
- ⚠️ Overall coverage low (~15%). Needs significant improvement across `analyzer.py`, `parser.py`, `mock_parser.py`, `graph.py`, `relationships.py`, `core.py`, etc.
- ⚠️ Missing tests for many MCP tools outside of the code understanding module.
- ⚠️ Limited error condition testing.
- ⚠️ Lack of integration tests between tools.

### Server Connectivity
- ⚠️ WebSocket transport not supported
- ⚠️ Cloud firewall restrictions
- ⚠️ Network connectivity timeouts
- ⚠️ Lack of comprehensive connection error handling
- ⚠️ Limited documentation for troubleshooting

### Code Generation
- ⚠️ Model API rate limits
- ⚠️ Local model resource usage
- ⚠️ Generation latency
- ⚠️ Output validation
- ⚠️ Security concerns

### Profiling
- ⚠️ Overhead impact
- ⚠️ Data storage
- ⚠️ Analysis complexity
- ⚠️ Resource usage
- ⚠️ Security concerns

### Validation
- ⚠️ Check performance
- ⚠️ False positives
- ⚠️ Resource usage
- ⚠️ Analysis complexity
- ⚠️ Security concerns

### Model Management
- ⚠️ Resource allocation
- ⚠️ Performance tracking
- ⚠️ Error handling
- ⚠️ Security controls
- ⚠️ Scaling issues

### Performance
- ⚠️ Resource bottlenecks
- ⚠️ Scaling limitations
- ⚠️ Latency issues
- ⚠️ Memory usage
- ⚠️ CPU usage

### Security
- ⚠️ Access control gaps
- ⚠️ Input validation
- ⚠️ Resource protection
- ⚠️ Audit logging
- ⚠️ Compliance issues

### Documentation
- ⚠️ Coverage gaps
- ⚠️ Outdated content
- ⚠️ Missing examples
- ⚠️ Incomplete tutorials
- ⚠️ Test coverage

### Testing
- ⚠️ Coverage gaps
- ⚠️ Performance tests
- ⚠️ Security tests
- ⚠️ Integration tests
- ⚠️ System tests

## Success Metrics

### Code Generation
- ✅ Generation success rate
- ✅ Average generation time
- ✅ Token usage
- ✅ Validation accuracy
- ✅ Resource utilization
- ✅ Error rates

### Profiling
- ✅ Profiling accuracy
- ✅ Metrics collection
- ✅ Analysis speed
- ✅ Resource usage
- ✅ Error rates

### Validation
- ✅ Validation accuracy
- ✅ Check performance
- ✅ Analysis speed
- ✅ Resource usage
- ✅ Error rates

### Model Management
- ✅ Model availability
- ✅ Resource usage
- ✅ Performance tracking
- ✅ Error rates
- ✅ Security compliance

### Performance
- ✅ Response time
- ✅ Resource usage
- ✅ Scaling efficiency
- ✅ Error rates
- ✅ Security compliance

### Security
- ✅ Access control
- ✅ Input validation
- ✅ Resource protection
- ✅ Audit logging
- ✅ Compliance

### Documentation
- ✅ Coverage
- ✅ Accuracy
- ✅ Completeness
- ✅ Resource usage
- ✅ Security compliance

### Testing
- ✅ Coverage
- ✅ Performance
- ✅ Security
- ✅ Resource usage
- ✅ Documentation

## Next Steps

### Code Generation
1. Implement advanced features
2. Optimize performance
3. Enhance security
4. Update documentation
5. Add tests

### Profiling
1. Implement advanced features
2. Optimize performance
3. Enhance security
4. Update documentation
5. Add tests

### Validation
1. Implement advanced features
2. Optimize performance
3. Enhance security
4. Update documentation
5. Add tests

### Model Management
1. Implement advanced features
2. Optimize performance
3. Enhance security
4. Update documentation
5. Add tests

### Performance
1. Implement advanced features
2. Optimize resources
3. Enhance scaling
4. Update documentation
5. Add tests

### Security
1. Implement advanced features
2. Enhance compliance
3. Add auditing
4. Update documentation
5. Add tests

### Documentation
1. Add advanced documentation
2. Create examples
3. Write tutorials
4. Add tests

### Testing
1. Add advanced tests
2. Add performance tests
3. Add security tests
4. Update documentation

## Deprioritized Features

### UI Customization
- 🔽 Advanced theming
- 🔽 Custom layouts
- 🔽 Interactive widgets
- 🔽 Animation effects
- 🔽 Responsive design for mobile

### Legacy Support
- 🔽 Compatibility with older Python versions
- 🔽 Support for deprecated protocols
- 🔽 Backward compatibility layers
- 🔽 Legacy API support
- 🔽 Migration tools

## Code Understanding Tool Progress

### Completed Components
- Core engine implementation (CodeParser, CodeAnalyzer, SymbolExtractor)
- Basic test infrastructure
- Tree-sitter integration with fallback to mock parser
- Symbol extraction with scope tracking
- Code analysis for syntax tree processing
- Basic relationship graph implementation
- Initial test suite for core functionality
- Relationship graph building
  - Import node creation refinement
  - Edge creation optimization
  - Graph traversal implementation
- Test coverage improvements
  - Fixed import node tests
  - Fixed directory analysis tests
  - Added comprehensive tests for analyzer.py
  - Added comprehensive tests for extractor.py
  - Added comprehensive tests for symbols.py

### In Progress
- Language-specific parsers
- Integration tests
- Test coverage for remaining components:
  - core.py
  - llm.py

### Pending
- Semantic mapping system
- Persistent code indexing
- MCP tool interface
- Performance optimization
- Documentation

### Known Issues
- Limited language support (currently Python only)
- Documentation gaps
- Test coverage for core.py and llm.py

### Next Steps
1. Complete language-specific parsers:
   - Add tree-sitter parsers for additional languages
   - Implement language-specific analysis rules
   - Add tests for each language parser

2. Implement semantic mapping:
   - Design embedding-based search
   - Create context mapping system
   - Implement similarity functions
   - Add semantic search capabilities

3. Create persistent indexing:
   - Design index structure
   - Implement incremental analysis
   - Add index management tools
   - Optimize for large codebases

4. Build MCP tool interface:
   - Design API interface
   - Implement command handlers
   - Create response formatters
   - Add error handling

5. Improve documentation:
   - Add API documentation
   - Create usage examples
   - Write developer guide
   - Add architecture overview

### Implementation Notes for Next Developer
1. Language Parser Implementation:
   ```python
   def setup_language_parser(language: str) -> Parser:
       """Set up a language-specific parser.
       
       Args:
           language: Language identifier (e.g., 'python', 'javascript')
           
       Returns:
           Configured parser for the language
       """
       # TODO: Implement language-specific parser setup
       pass
   ```

2. Semantic Mapping Implementation:
   ```python
   def create_semantic_mapping(code: str) -> Dict[str, Any]:
       """Create semantic mapping for code.
       
       Args:
           code: Source code to analyze
           
       Returns:
           Semantic mapping information
       """
       # TODO: Implement semantic mapping
       pass
   ```

3. Persistent Index Implementation:
   ```python
   def create_persistent_index(workspace: str) -> None:
       """Create persistent index for a workspace.
       
       Args:
           workspace: Path to workspace
       """
       # TODO: Implement persistent indexing
       pass
   ```

4. MCP Tool Interface Implementation:
   ```python
   def create_mcp_interface() -> None:
       """Create MCP tool interface."""
       # TODO: Implement MCP interface
       pass
   ```

## Code Organization
- Keep language-specific code in separate modules
- Use consistent error handling and logging
- Follow type hints and documentation standards
- Maintain test coverage above 90%

## Other Components
[To be added as other components are developed]

## Progress

### Completed Tasks

#### Core Implementation
- [x] Basic project structure
- [x] Tree-sitter integration
- [x] Python parser implementation
- [x] Symbol extraction system
- [x] Basic relationship building
- [x] Graph representation
- [x] Test framework setup

#### Python Support
- [x] Basic Python syntax parsing
- [x] Function and class extraction
- [x] Import handling
- [x] Variable scope tracking
- [x] Basic type inference
- [x] Control flow analysis

#### Testing
- [x] Unit test framework
- [x] Mock parser implementation
- [x] Basic test cases
- [x] Error handling tests
- [x] Performance tests

### In Progress

#### Language Support
- [x] **JavaScript parser adapter basics implemented and tested (`test_language_adapters.py`)**
- [ ] Test JavaScript parser thoroughly (`test_javascript_parser.py`, `test_javascript_support.py`)
- [ ] Swift parser adapter (stubbed, basic tests pass)
- [ ] Language detection system

#### Semantic Analysis
- [ ] Type system implementation
- [ ] Control flow analysis
- [ ] Data flow analysis
- [ ] Context-sensitive analysis
- [ ] Semantic graph construction

#### Testing
- [ ] Language-specific test suites
- [ ] Cross-language integration tests
- [ ] Performance benchmarks
- [ ] Documentation tests
- [ ] Edge case coverage

### Planned Tasks

#### Core Improvements
- [ ] Caching system for large codebases
- [ ] Incremental analysis support
- [ ] Enhanced error recovery
- [ ] Improved logging system
- [ ] Performance optimization

#### Language Support
- [ ] Additional language support (TypeScript, Kotlin)
- [ ] Language-specific optimizations
- [ ] Cross-language type inference
- [ ] Language-specific documentation
- [ ] Language-specific examples

#### Semantic Analysis
- [ ] Advanced type inference
- [ ] Call graph analysis
- [ ] Data dependency analysis
- [ ] Impact analysis
- [ ] Code quality metrics

#### Documentation
- [ ] API documentation
- [ ] Architecture guide
- [ ] Language support guide
- [ ] Troubleshooting guide
- [ ] Performance guide

### Current Status
- Core Python support: Complete
- **JavaScript support: Basic parsing adapter implemented, needs further testing and integration.**
- Swift support: Planning phase (adapter stubbed)
- Semantic analysis: Design phase
- Test coverage: ~12% (needs significant improvement)

### Known Issues
1. Low test coverage needs immediate attention (**including for JS adapter**)
2. Relationship extraction failures in complex cases
3. Performance optimization needed for large codebases
4. Documentation needs updating for new features

### Next Milestone
- Complete JavaScript parser adapter
- Implement basic type system
- Improve test coverage to 50%
- Update documentation for new features

### Timeline
1. Phase 1: Core Language Support (Weeks 1-5)
2. Phase 2: Basic Semantic Analysis (Weeks 6-10)
3. Phase 3: Advanced Semantic Features (Weeks 11-16)
4. Phase 4: Testing and Optimization (Weeks 17-22)
</file>

<file path="memory-bank/project-brief.md">
# Terminal Command Runner MCP - Project Brief

## Overview

This project implements a Terminal Command Runner using the Message Control Protocol (MCP) server. It provides a set of tools to execute shell commands, manage files, perform system operations, and power AI Coding Agents through a standardized API interface.

## Core Requirements

1. **Command Execution**: Safely execute terminal commands with timeout control and background processing
2. **File Management**: Read, write, move, and search files with appropriate security measures
3. **Process Management**: List, monitor, and terminate running processes
4. **System Information**: Provide system details and utilities for diagnostic purposes
5. **AI Coding Agent Tools**: Enable advanced capabilities for AI-powered code assistance and development

## Goals

- Create a secure interface for executing shell commands remotely
- Provide robust error handling and security measures
- Enable long-running background processes with output streaming
- Support standard file system operations with appropriate safeguards
- Implement safety features to prevent dangerous commands execution
- Empower AI Coding Agents with deep code understanding capabilities
- Enable intelligent code refactoring with behavior preservation
- Facilitate automated test generation for comprehensive test coverage
- Support dependency impact analysis for safer code modifications
- Automate code review to maintain quality and standards

## Scope

The MCP server implements a REST API with the following capabilities:
- Command execution with configurable timeouts
- Process monitoring and management
- File system operations (read, write, search, etc.)
- System information retrieval
- Safety features (command blacklisting)
- AI Coding Agent capabilities:
  - Code understanding and relationship mapping
  - Intelligent refactoring with behavior validation
  - Automated test generation with edge case detection
  - Dependency impact analysis with visualization
  - Code review automation with best practice verification

## Non-Goals

- GUI interface (command-line and API only)
- Complex permission systems (basic safety checks only)
- Platform-specific features (focus on cross-platform compatibility)
- Complete IDE replacement (focused on specific agent capabilities)
</file>

<file path="memory-bank/system-patterns.md">
# System Architecture and Design Patterns

## Core Architecture

### MCP Server
- FastMCP-based server implementation
- RESTful API design with tool-based interface
- Server-Sent Events (SSE) transport protocol
- Asynchronous command execution
- Event-driven output streaming
- Thread-safe process management
- Advanced profiling system
- OpenTelemetry integration
- Code generation capabilities
- AI Coding Agent tools integration

### AI Coding Agent Tools Architecture
- Code understanding and analysis system
- Intelligent refactoring framework
- Automated test generation system
- Dependency impact analysis engine
- Code review automation platform
- Language-agnostic parsing with tree-sitter
- Semantic code mapping system
- Relationship graph visualization
- Persistent indexing for large codebases
- Embedding-based semantic search

### Connectivity Architecture
- SSE-based communication protocol
- Port configuration for external access
- Traefik proxy integration for routing
- Firewall configuration for security
- SSH tunnel capability for secure access
- Connection error handling 
- Client reconnection strategies

### Component Structure
```
server.py
├── Core Tools
│   ├── Command Execution
│   ├── Process Management
│   ├── File Operations
│   └── Code Generation
├── Development Tools
│   ├── Code Analysis
│   ├── Performance Monitoring
│   ├── Testing Support
│   └── Profiling
├── AI Coding Agent Tools
│   ├── Code Understanding
│   ├── Intelligent Refactoring
│   ├── Test Generation
│   ├── Dependency Impact Analysis
│   └── Code Review Automation
├── Connectivity Tools
│   ├── Connection Management
│   ├── Transport Configuration
│   ├── Proxy Integration
│   └── Network Troubleshooting
└── Utility Tools
    ├── System Info
    ├── Calculations
    └── Context Management
```

## Design Patterns

### AI Coding Agent Tool Patterns
- **Code Analysis Pattern**: Language-agnostic parsing with custom extractors
- **Graph-Based Code Representation**: Nodes for entities, edges for relationships
- **Semantic Mapping**: Embedding-based connectivity between code and natural language
- **Incremental Analysis**: Update only changed components for performance
- **Persistent Indexing**: Serialize and store analysis results for quick access
- **Language Adapter**: Plug-in architecture for multi-language support
- **Analysis Pipeline**: Multi-stage processing with progressive refinement
- **Entity Resolver**: Connect references across different files and modules

### Communication Patterns
- Server-Sent Events (SSE) for unidirectional streaming
- RESTful API for bidirectional communication
- Protocol negotiation for client compatibility
- Connection pooling for performance
- Graceful degradation for network issues
- Reconnection strategies for resilience

### Command Pattern
- Each tool is implemented as a discrete command
- Standardized input/output interface
- Error handling and validation
- Resource cleanup
- Profiling integration
- Metrics collection

### Observer Pattern
- Process output streaming
- Performance metrics collection
- Event-based notifications
- Profiling data collection
- Code generation events
- Validation results

### Factory Pattern
- Tool registration and instantiation
- Dynamic command creation
- Plugin system support
- Model factory for code generation
- Profiler factory
- Metrics collector factory

### Strategy Pattern
- Configurable execution strategies
- Platform-specific implementations
- Test environment isolation
- Model selection strategy
- Profiling strategy
- Validation strategy

## Implementation Patterns for AI Coding Agent Tools

### Code Understanding Tool

#### Core Components

1. CodeParser
```python
class CodeParser:
    """Parses source code into syntax trees using tree-sitter."""
    
    def __init__(self):
        self.parser = None
        self.language = None
        
    def setup_language(self, language: str):
        """Set up parser for a specific language."""
        pass
        
    def parse(self, source_code: str) -> Tree:
        """Parse source code into a syntax tree."""
        pass
```

2. CodeAnalyzer
```python
class CodeAnalyzer:
    """Analyzes code using tree-sitter syntax trees."""
    
    def __init__(self):
        self.reset_state()
        
    def analyze_tree(self, tree: Tree) -> Dict[str, List[Dict[str, Any]]]:
        """Analyze a syntax tree and extract code information."""
        pass
        
    def _extract_imports(self, node: Node) -> List[Dict[str, Any]]:
        """Extract import statements."""
        pass
        
    def _extract_functions(self, node: Node) -> List[Dict[str, Any]]:
        """Extract function definitions."""
        pass
        
    def _extract_classes(self, node: Node) -> List[Dict[str, Any]]:
        """Extract class definitions."""
        pass
```

3. SymbolExtractor
```python
class SymbolExtractor:
    """Extracts symbols from syntax trees."""
    
    def __init__(self):
        self.current_scope = None
        self.symbols = {}
        self.references = {}
        
    def extract_symbols(self, tree: Tree) -> Dict[str, Any]:
        """Extract symbols from a syntax tree."""
        pass
        
    def _process_node(self, node: Node, parent_scope: Optional[str] = None):
        """Process a syntax tree node and extract symbols."""
        pass
```

#### Design Patterns

1. **Modular Architecture**
   - Separate concerns into Parser, Analyzer, and Extractor
   - Each component has a single responsibility
   - Easy to extend and maintain

2. **State Management**
   - Components maintain internal state
   - State is reset between operations
   - Scope tracking for accurate symbol resolution

3. **Error Handling**
   - Comprehensive try-except blocks
   - Detailed error logging
   - Graceful fallbacks for failures

4. **Visitor Pattern**
   - Tree traversal using visitor pattern
   - Node type-specific processing
   - Maintains context during traversal

5. **Builder Pattern**
   - Incremental construction of analysis results
   - Separate builders for different aspects
   - Clean separation of building logic

#### Data Structures

1. **Syntax Tree**
```python
class Tree:
    """Represents a syntax tree."""
    def __init__(self, root_node: Node):
        self.root_node = root_node
```

2. **Node**
```python
class Node:
    """Represents a syntax tree node."""
    def __init__(self, type: str, text: str = "", children: List["Node"] = None):
        self.type = type
        self.text = text
        self.children = children or []
```

3. **Symbol Table**
```python
SymbolTable = Dict[str, Dict[str, Any]]
"""
{
    'symbol_name': {
        'type': str,  # 'function', 'class', 'variable', 'import'
        'scope': str,  # Scope where symbol is defined
        'start': Tuple[int, int],  # Start position
        'end': Tuple[int, int],  # End position
        'params': List[str],  # For functions
        'bases': List[str],  # For classes
    }
}
"""
```

4. **Reference Table**
```python
ReferenceTable = Dict[str, List[Dict[str, Any]]]
"""
{
    'symbol_name': [
        {
            'scope': str,  # Scope where reference occurs
            'start': Tuple[int, int],  # Start position
            'end': Tuple[int, int],  # End position
        }
    ]
}
"""
```

#### Future Extensions

1. **Language Support**
```python
class LanguageParser:
    """Base class for language-specific parsers."""
    
    def parse(self, source: str) -> Tree:
        """Parse source code into a syntax tree."""
        pass

class PythonParser(LanguageParser):
    """Python-specific parser implementation."""
    pass

class JavaScriptParser(LanguageParser):
    """JavaScript-specific parser implementation."""
    pass
```

2. **Relationship Graph**
```python
class RelationshipGraph:
    """Builds and manages code relationship graphs."""
    
    def __init__(self):
        self.nodes = {}
        self.edges = []
        
    def add_relationship(self, source: str, target: str, type: str):
        """Add a relationship between symbols."""
        pass
        
    def get_dependencies(self, symbol: str) -> List[str]:
        """Get dependencies for a symbol."""
        pass
```

3. **Semantic Mapper**
```python
class SemanticMapper:
    """Maps code to semantic representations."""
    
    def __init__(self, model: str = "default"):
        self.model = model
        
    def embed_code(self, code: str) -> np.ndarray:
        """Generate embeddings for code."""
        pass
        
    def find_similar(self, query: str, threshold: float = 0.8) -> List[str]:
        """Find semantically similar code."""
        pass
```

### Intelligent Refactoring Tool
```python
class RefactoringPlanner:
    """Plans code refactoring operations."""
    
    def plan_refactoring(self, code_graph: CodeGraph, target: str, refactoring_type: str) -> RefactoringPlan:
        """Create a plan for refactoring the specified target."""
        # Implementation details...
        pass
        
class BehaviorValidator:
    """Validates that refactoring preserves behavior."""
    
    def validate(self, original_code: str, refactored_code: str) -> ValidationResult:
        """Check if the refactored code preserves the behavior of the original code."""
        # Implementation details...
        pass
```

### Test Generation Tool
```python
class TestGenerator:
    """Generates tests for code."""
    
    def generate_tests(self, source_code: str, coverage_target: float = 0.8) -> List[TestCase]:
        """Generate test cases for the given source code."""
        # Implementation details...
        pass
        
class EdgeCaseDiscoverer:
    """Discovers edge cases for testing."""
    
    def discover(self, source_code: str) -> List[EdgeCase]:
        """Find potential edge cases in the source code."""
        # Implementation details...
        pass
```

## Security Patterns

### Network Security
- Port-specific firewall rules
- Transport-level encryption options
- Proxy-based request filtering
- Connection source validation
- Rate limiting for connection attempts
- Connection monitoring for anomalies

### Command Validation
- Blacklist-based command filtering
- Path traversal prevention
- Resource limit enforcement
- Code generation safety
- Model access control
- Input sanitization

### Process Isolation
- Separate process spaces
- Timeout enforcement
- Resource cleanup
- Model execution isolation
- Profiling isolation
- Validation isolation

### Error Handling
- Comprehensive error capture
- Structured error responses
- Graceful degradation
- Model error handling
- Profiling error recovery
- Validation error reporting

## Performance Patterns

### Connection Management
- Connection pool optimization
- Connection timeout handling
- Reconnection backoff strategies
- Load balancing for multiple clients
- Connection metrics collection
- Health checking for connections

### Resource Management
- Thread pool management
- Process lifecycle control
- Memory usage optimization
- Model resource management
- Profiling overhead control
- Validation resource limits

### Output Handling
- Streaming large outputs
- Buffer management
- Smart truncation
- Model output streaming
- Profiling data streaming
- Validation result streaming

### Caching
- Command result caching
- File content caching
- System info caching
- Model response caching
- Profiling data caching
- Validation result caching

## Testing Patterns

### Test Categories
- Unit tests for core functionality
- Integration tests for tool interaction
- System tests for end-to-end validation
- Model integration tests
- Profiling tests
- Validation tests
- Observability tool tests
- Development tool tests
- Documentation generation tests
- Project management tool tests

### Test Structure for MCP Tools
```python
def test_tool_name_success_case(fixture_setup):
    """Test successful operation of the tool."""
    # Arrange
    test_params = {...}
    expected_output = {...}
    
    # Act
    result = server.tool_name(**test_params)
    
    # Assert
    assert result["status"] == "success"
    assert result["expected_key"] == expected_output
    
def test_tool_name_error_case(fixture_setup):
    """Test error handling of the tool."""
    # Arrange
    invalid_params = {...}
    
    # Act
    result = server.tool_name(**invalid_params)
    
    # Assert
    assert result["status"] == "error"
    assert "error" in result
```

### Test Isolation
- Docker-based test environments
- Mock system operations
- Resource cleanup
- Model mocking
- Profiling isolation
- Validation isolation
- API endpoint mocking
- File system sandboxing
- Dependency injection for testability

### Performance Testing
- Load testing framework
- Resource usage monitoring
- Benchmark suite
- Model performance tests
- Profiling overhead tests
- Validation performance tests
- Metrics collection validation
- Tracing overhead assessment

### Missing Test Implementation Patterns
```python
# Observability Tools Testing Pattern
def test_observability_tool(mocker):
    """Test pattern for observability tools."""
    # Mock dependencies
    mock_dependency = mocker.patch('module.dependency')
    mock_dependency.return_value = expected_data
    
    # Call the tool
    result = server.observability_tool(params)
    
    # Verify result
    assert result["status"] == "success"
    assert "expected_data" in result
    mock_dependency.assert_called_once_with(params)

# Development Tools Testing Pattern
def test_development_tool(tmp_path, mocker):
    """Test pattern for development tools."""
    # Set up test environment
    test_file = tmp_path / "test_file.py"
    test_file.write_text("def test(): pass")
    
    # Mock subprocess calls
    mock_run = mocker.patch('subprocess.run')
    mock_run.return_value = Mock(returncode=0, stdout="Success", stderr="")
    
    # Call the tool
    result = server.development_tool(str(test_file))
    
    # Verify result
    assert result["status"] == "success"
    assert mock_run.called
    
# Documentation Tools Testing Pattern
def test_documentation_tool(tmp_path):
    """Test pattern for documentation generation tools."""
    # Set up test project
    test_dir = tmp_path / "test_project"
    test_dir.mkdir()
    (test_dir / "test_module.py").write_text("def function(): \"\"\"Docstring\"\"\"")
    
    # Call the tool
    result = server.documentation_tool(str(test_dir))
    
    # Verify result
    assert result["status"] == "success"
    assert (test_dir / "docs").exists()
```

## Development Patterns

### Connectivity Testing
```python
def test_server_connectivity(host: str, port: int) -> Dict[str, Any]:
    """
    Test server connectivity
    
    Args:
        host: Server hostname
        port: Server port
    
    Returns:
        Dictionary with connectivity test results
    """
    try:
        # Try to connect
        with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
            s.settimeout(5)
            result = s.connect_ex((host, port))
            
        if result == 0:
            return {
                'status': 'success',
                'message': f'Successfully connected to {host}:{port}'
            }
        else:
            return {
                'status': 'error',
                'error': f'Failed to connect to {host}:{port}',
                'code': result
            }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
```

### Transport Configuration
```python
def configure_transport(transport_type: str) -> Dict[str, Any]:
    """
    Configure server transport
    
    Args:
        transport_type: Transport type ("sse" or "websocket")
    
    Returns:
        Dictionary with configuration result
    """
    try:
        # Validate transport type
        valid_transports = ["sse", "websocket"]
        if transport_type not in valid_transports:
            return {
                'status': 'error',
                'error': f'Invalid transport: {transport_type}. Must be one of {valid_transports}'
            }
            
        # Update configuration
        config = {
            'transport': transport_type,
            'path': '/sse' if transport_type == 'sse' else '/ws',
            'reconnect_interval': 1000
        }
        
        # Save configuration
        with open('config.json', 'w') as f:
            json.dump(config, f)
            
        return {
            'status': 'success',
            'config': config
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
```

### Code Organization
- Modular tool implementation
- Clear separation of concerns
- Consistent file structure
- Model integration structure
- Profiling integration
- Validation organization

### Error Handling
```python
try:
    # Operation-specific logic
    result = perform_operation()
    return {
        'status': 'success',
        'result': result
    }
except SpecificError as e:
    return {
        'status': 'error',
        'error': str(e),
        'type': 'specific_error'
    }
except Exception as e:
    return {
        'status': 'error',
        'error': str(e),
        'type': 'general_error'
    }
```

### Function Structure
```python
@mcp.tool()
def tool_name(param1: Type1, param2: Type2 = default) -> Dict[str, Any]:
    """
    Tool description
    
    Args:
        param1: Description
        param2: Description
    
    Returns:
        Dictionary with operation result
    """
    try:
        # Validation
        if not validate_params(param1, param2):
            return {'status': 'error', 'error': 'Invalid parameters'}
            
        # Core logic
        result = process_operation(param1, param2)
        
        # Result formatting
        return {
            'status': 'success',
            'result': result
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
```

## Integration Patterns

### Tool Communication
- Standardized result format
- Error propagation
- Context sharing
- Model communication
- Profiling data sharing
- Validation result sharing

### Resource Sharing
- Thread-safe queues
- Shared state management
- Resource pools
- Model resource sharing
- Profiling resource sharing
- Validation resource sharing

### Event Handling
- Process lifecycle events
- Error events
- Status updates
- Model events
- Profiling events
- Validation events

## Monitoring Patterns

### Performance Metrics
- CPU usage tracking
- Memory utilization
- I/O operations
- Network traffic
- Model performance
- Profiling metrics
- Validation metrics

### Error Tracking
- Error categorization
- Stack trace collection
- Error rate monitoring
- Model errors
- Profiling errors
- Validation errors

### Health Checks
- Service availability
- Resource utilization
- System status
- Model health
- Profiling health
- Validation health

## Documentation Patterns

### Code Documentation
- Google-style docstrings
- Type hints
- Usage examples
- Model documentation
- Profiling documentation
- Validation documentation

### API Documentation
- OpenAPI/Swagger specs
- Example requests/responses
- Error scenarios
- Model API docs
- Profiling API docs
- Validation API docs

### System Documentation
- Architecture overview
- Component interaction
- Deployment guides
- Model deployment
- Profiling setup
- Validation setup

## Observability Patterns

### Distributed Tracing Architecture
1. **Trace Collection**
   - OpenTelemetry SDK for trace generation
   - OTLP exporter for trace export
   - Configurable collection endpoint
   - Batch processing for efficient trace export
   - Model tracing
   - Profiling tracing
   - Validation tracing

2. **Tool Tracing**
   - Decorator pattern for automatic tool tracing
   - Consistent span naming convention: `mcp.tool.<tool_name>`
   - Automatic error and exception tracking
   - Tool-specific attributes:
     - Tool name
     - Arguments
     - Status
     - Error details
     - Model details
     - Profiling details
     - Validation details

3. **Resource Attribution**
   - Service name and version tracking
   - OpenTelemetry semantic conventions
   - Configurable resource attributes
   - Dynamic resource updates
   - Model resources
   - Profiling resources
   - Validation resources

4. **Error Handling**
   - Exception capture in spans
   - Error attribute propagation
   - Status code tracking
   - Automatic error context collection
   - Model error context
   - Profiling error context
   - Validation error context

### Metrics Architecture
1. **Metrics Collection**
   - OpenTelemetry SDK for metrics generation
   - OTLP exporter for metrics export
   - Configurable collection endpoint
   - Periodic metric export

2. **Tool Metrics**
   - Decorator pattern for automatic metrics collection
   - Standard metric types:
     - Histograms for durations
     - Counters for calls and errors
     - Up/down counters for active sessions
     - Observable gauges for system metrics
   - Consistent naming convention: `mcp.tool.*`
   - Automatic error tracking

3. **System Metrics**
   - Memory usage monitoring via psutil
   - Observable gauge implementation
   - Real-time resource tracking
   - Low-overhead collection

4. **Metric Configuration**
   - Dynamic endpoint configuration
   - Metric recreation on config changes
   - Global meter provider management
   - Metric reader configuration

## Profiling Architecture

### Core Profiling System
- **Global Profiler**: Singleton `MCPProfiler` instance manages profiling sessions
- **Profiling States**: Active/inactive state management with session persistence
- **Stats Management**: Automatic stats collection and file management
- **Tool Integration**: Automatic profiling integration via decorators

### Tool Profiling
- **Decorator Pattern**: `@profile_tool` decorator for automatic profiling
- **Stats Collection**:
  - Execution time tracking
  - Call count monitoring
  - Function-level statistics
  - Cumulative performance data
- **File Management**:
  - Temporary stats files for each profiling session
  - Automatic cleanup on errors
  - Stats file persistence for analysis

### Code Block Profiling
- **Dynamic Code Execution**: Profile arbitrary Python code blocks
- **Context Management**:
  - Isolated execution environment
  - Custom globals dictionary support
  - Temporary file handling for code and stats
- **Results Format**:
  - Human-readable stats output
  - Stats file references
  - Code file preservation

### Integration Points
- **Tool Registration**: Automatic profiling wrapper for all MCP tools
- **Session Management**: Start/stop profiling on demand
- **Stats Analysis**: Tools for examining profiling data
- **Error Handling**: Graceful error management with cleanup

### Best Practices
- **Resource Management**: Automatic cleanup of temporary files
- **State Handling**: Clear state transitions in profiling sessions
- **Data Access**: Structured access to profiling statistics
- **Tool Integration**: Non-intrusive profiling of existing tools

## Code Understanding Tool Architecture

The Code Understanding Tool follows a layered architecture with the following components:

### Parser Layer
- **CodeParser** (`parser.py`)
  - Handles code parsing using tree-sitter (with fallback to AST)
  - Provides a unified interface for accessing syntax trees
  - Abstracts away differences between tree-sitter and AST
  - Uses mock objects for testing when tree-sitter is unavailable

### Analyzer Layer
- **CodeAnalyzer** (`analyzer.py`)
  - Extracts high-level code structures (imports, functions, classes, variables)
  - Provides a simplified view of code for quick analysis
  - Focuses on structural elements rather than detailed semantics

### Symbol Extraction Layer
- **SymbolExtractor** (`symbols.py` and `extractor.py`)
  - Extracts detailed symbol information from syntax trees
  - Handles scoping rules for proper symbol resolution
  - Tracks symbol references for relationship building
  - Maintains a scope hierarchy during tree traversal

### Relationship Graph Layer (Planned)
- **RelationshipExtractor** (`relationships.py`)
  - Identifies relationships between symbols (calls, inherits, imports, etc.)
  - Builds a graph representation of code relationships
  - Supports queries for finding dependencies and impacts

### Graph Representation Layer (Planned)
- **CodeGraph** (`graph.py`)
  - Provides a graph data structure for representing code relationships
  - Supports queries for navigating the code structure
  - Enables visualization and export capabilities

## Design Patterns

The Code Understanding Tool uses several design patterns:

1. **Visitor Pattern**
   - Tree traversal via `_process_node` methods
   - Node-specific processing methods for different node types
   - Maintains context during traversal (scope, etc.)

2. **Strategy Pattern**
   - Different strategies for processing different node types
   - Allows for language-specific extensions

3. **Composite Pattern**
   - Tree structure representation of code
   - Uniform interface for working with nodes

4. **Adapter Pattern**
   - Tree-sitter to internal representation adaptation
   - AST to internal representation adaptation
   - Mock objects for testing

5. **Factory Method**
   - Creating appropriate node handlers based on node type
   - Extensible for new languages or node types

## Error Handling Pattern

The code follows a consistent error handling approach:

1. Top-level methods have try-except blocks
2. Errors are logged with appropriate context
3. Fallback values are provided to maintain system stability
4. Error information is propagated to the caller for informed decisions
5. Custom error types for specific error scenarios

## Testing Strategy

The testing approach includes:

1. Mocking syntax trees for deterministic testing
2. Unit tests for individual components
3. Integration tests for end-to-end functionality
4. Error handling tests to ensure robustness
5. Parameterized tests for language-specific behavior

## Relationship Builder Patterns

### Node Creation Pattern
```python
def create_node(self, name: str, type: str, context: Context) -> Node:
    """Create a node with proper context and validation."""
    # Check if node already exists
    existing = self._find_existing_node(name, type)
    if existing:
        return existing
        
    # Create new node with context
    node = Node(
        name=name,
        type=type,
        file_path=context.file_path,
        start_line=context.start_line,
        end_line=context.end_line,
        properties=context.properties
    )
    
    # Validate and store
    self._validate_node(node)
    self._store_node(node)
    return node
```

### Edge Creation Pattern
```python
def create_edge(self, source: Node, target: Node, type: RelationType, context: Context) -> Edge:
    """Create an edge with proper validation and properties."""
    # Validate nodes exist
    if not self._node_exists(source) or not self._node_exists(target):
        raise ValueError("Source or target node does not exist")
        
    # Create edge with context
    edge = Edge(
        source=source,
        target=target,
        type=type,
        properties={
            'line_number': context.line_number,
            'scope': context.scope
        }
    )
    
    # Validate and store
    self._validate_edge(edge)
    self._store_edge(edge)
    return edge
```

### Reference Processing Pattern
```python
def process_reference(self, ref: Reference, context: Context) -> None:
    """Process a code reference with proper scoping."""
    try:
        # Find or create nodes
        source_node = self._get_scope_node(ref.scope)
        target_node = self._get_target_node(ref.name, ref.type)
        
        # Create relationship
        self.create_edge(
            source=source_node,
            target=target_node,
            type=self._get_relation_type(ref),
            context=context
        )
    except Exception as e:
        self._handle_reference_error(e, ref)
```

### Error Handling Pattern
```python
def _handle_reference_error(self, error: Exception, ref: Reference) -> None:
    """Handle errors during reference processing."""
    logger.error(f"Failed to process reference {ref.name}: {str(error)}")
    
    if isinstance(error, NodeNotFoundError):
        # Create placeholder node
        self._create_placeholder_node(ref)
    elif isinstance(error, ValidationError):
        # Log validation failure
        self._log_validation_failure(error)
    else:
        # Unexpected error
        raise ProcessingError(f"Failed to process reference: {str(error)}")
```

### Validation Patterns

1. **Node Validation**
```python
def _validate_node(self, node: Node) -> None:
    """Validate node properties and relationships."""
    if not node.name:
        raise ValidationError("Node must have a name")
        
    if not node.type:
        raise ValidationError("Node must have a type")
        
    if node.type not in VALID_NODE_TYPES:
        raise ValidationError(f"Invalid node type: {node.type}")
```

2. **Edge Validation**
```python
def _validate_edge(self, edge: Edge) -> None:
    """Validate edge properties and relationships."""
    if not edge.source or not edge.target:
        raise ValidationError("Edge must have source and target")
        
    if edge.type not in RelationType:
        raise ValidationError(f"Invalid edge type: {edge.type}")
        
    if edge.source.id == edge.target.id:
        raise ValidationError("Self-referential edges not allowed")
```

### Testing Patterns

1. **Fixture Pattern**
```python
@pytest.fixture
def sample_context():
    """Create a test context with common test data."""
    return Context(
        file_path="test.py",
        start_line=1,
        end_line=10,
        scope="global",
        properties={}
    )
```

2. **Test Case Pattern**
```python
def test_process_reference(builder, sample_context):
    """Test reference processing with validation."""
    # Arrange
    ref = Reference(name="test", type="call", scope="main")
    
    # Act
    builder.process_reference(ref, sample_context)
    
    # Assert
    graph = builder.get_relationships()
    assert len(graph.nodes) > 0
    assert len(graph.edges) > 0
    
    # Verify relationships
    edge = next(iter(graph.edges))
    assert edge.source.name == "main"
    assert edge.target.name == "test"
    assert edge.type == RelationType.CALLS
```

## Architecture Overview

### Core Components
1. **Parser Layer**
   - Language-specific parsers (Python, JavaScript, Swift)
   - Tree-sitter integration
   - Abstract syntax tree (AST) generation
   - Error handling and recovery

2. **Analyzer Layer**
   - Code structure analysis
   - Symbol extraction
   - Type inference
   - Control flow analysis
   - Data flow analysis

3. **Semantic Layer**
   - Type system
   - Context tracking
   - Symbol resolution
   - Cross-language references
   - Impact analysis

4. **Graph Layer**
   - Relationship building
   - Graph representation
   - Query interface
   - Visualization support

## Design Patterns

### Language Support Patterns
1. **Adapter Pattern**
   - Language-specific parser adapters
   - Unified AST representation
   - Common symbol interface
   - Cross-language type mapping

2. **Strategy Pattern**
   - Language-specific analysis strategies
   - Parser selection strategy
   - Type inference strategy
   - Reference resolution strategy

3. **Factory Pattern**
   - Language-specific component creation
   - Parser factory
   - Analyzer factory
   - Graph factory

### Semantic Analysis Patterns
1. **Visitor Pattern**
   - AST traversal
   - Type inference
   - Control flow analysis
   - Data flow analysis

2. **Observer Pattern**
   - Analysis progress tracking
   - Error reporting
   - Performance monitoring
   - Cache invalidation

3. **Builder Pattern**
   - Semantic graph construction
   - Type system building
   - Context building
   - Relationship building

### Testing Patterns
1. **Mock Pattern**
   - Parser mocking
   - File system mocking
   - Cache mocking
   - Performance mocking

2. **Factory Pattern**
   - Test data generation
   - Mock object creation
   - Test scenario building
   - Performance test setup

3. **Strategy Pattern**
   - Test execution strategy
   - Coverage strategy
   - Performance strategy
   - Language-specific test strategy

## Implementation Guidelines

### Language Support
1. **Parser Implementation**
   - Use tree-sitter for all languages
   - Implement language detection
   - Create unified AST representation
   - Handle language-specific errors

2. **Symbol Extraction**
   - Language-specific scope rules
   - Cross-language references
   - Type information extraction
   - Context preservation

3. **Type System**
   - Language-specific type rules
   - Type inference algorithms
   - Type compatibility checking
   - Cross-language type mapping

### Semantic Analysis
1. **Control Flow**
   - CFG construction
   - Path analysis
   - Reachability analysis
   - Dead code detection

2. **Data Flow**
   - Variable tracking
   - Value propagation
   - Use-def chains
   - Live variable analysis

3. **Context Analysis**
   - Scope tracking
   - Symbol resolution
   - Type context
   - Call context

## Testing Strategy

### Unit Testing
1. **Parser Tests**
   - Language-specific syntax
   - Error handling
   - Edge cases
   - Performance

2. **Analyzer Tests**
   - Symbol extraction
   - Type inference
   - Control flow
   - Data flow

3. **Graph Tests**
   - Node creation
   - Edge creation
   - Relationship building
   - Query interface

### Integration Testing
1. **Cross-language Tests**
   - Multi-language projects
   - Reference resolution
   - Type compatibility
   - Performance impact

2. **End-to-end Tests**
   - Complete analysis pipeline
   - Large codebases
   - Real-world scenarios
   - Performance benchmarks

### Performance Testing
1. **Scalability Tests**
   - Large codebases
   - Multiple languages
   - Complex relationships
   - Memory usage

2. **Optimization Tests**
   - Caching effectiveness
   - Incremental analysis
   - Parallel processing
   - Resource utilization

## Error Handling

### Error Types
1. **Parser Errors**
   - Syntax errors
   - Grammar errors
   - File access errors
   - Encoding errors

2. **Analysis Errors**
   - Type errors
   - Scope errors
   - Reference errors
   - Context errors

3. **Graph Errors**
   - Node errors
   - Edge errors
   - Cycle errors
   - Query errors

### Recovery Strategies
1. **Parser Recovery**
   - Error location
   - Partial parsing
   - Error reporting
   - Recovery suggestions

2. **Analysis Recovery**
   - Partial analysis
   - Error isolation
   - Context preservation
   - Incremental updates

3. **Graph Recovery**
   - Partial graph
   - Error isolation
   - State preservation
   - Incremental updates

## Performance Considerations

### Optimization Strategies
1. **Caching**
   - Parse results
   - Analysis results
   - Graph state
   - Type information

2. **Incremental Analysis**
   - Changed files
   - Affected symbols
   - Updated relationships
   - Modified types

3. **Parallel Processing**
   - File parsing
   - Symbol analysis
   - Graph building
   - Type inference

### Resource Management
1. **Memory Usage**
   - AST size
   - Graph size
   - Cache size
   - Temporary objects

2. **CPU Usage**
   - Parse time
   - Analysis time
   - Graph time
   - Query time

3. **I/O Usage**
   - File access
   - Cache access
   - Graph storage
   - Result output
</file>

<file path="memory-bank/tech-context.md">
# Terminal Command Runner MCP - Tech Context

## Technology Stack

### Core Technologies
- Python 3.8+
- FastAPI for API server
- Server-Sent Events (SSE) for transport protocol
- Traefik for proxy and routing
- Click for CLI framework
- Rich for terminal UI
- OpenTelemetry for observability
- pytest for testing
- cmd for debugging interface
- JSON for configuration storage
- Anthropic API for code generation
- OpenAI API for code generation
- Transformers for local models
- cProfile for profiling

### Networking Stack
- FastAPI for HTTP server
- Server-Sent Events (SSE) for streaming
- Traefik for reverse proxy and routing
- SSH for secure server management
- Socket connections for health checks
- Firewall configuration with iptables
- tcpdump for network diagnostics
- Port configuration management
- Network monitoring tools

### Monitoring Stack
- OpenTelemetry SDK for metrics collection
- Prometheus for metrics storage and alerting
- AlertManager for alert management
- Grafana for visualization
- Docker Compose for deployment
- psutil for system metrics
- Model performance tracking
- Profiling data collection
- Validation metrics
- Connection status monitoring

### Debugging Stack
- cmd module for CLI interface
- Rich for output formatting
- inspect for introspection
- pdb for debugging support
- threading for synchronization
- functools for decorators
- Model debugging tools
- Profiling analysis
- Validation debugging

### Development Tools
- Black for code formatting
- flake8 for linting
- mypy for type checking
- pytest for testing
- coverage.py for code coverage
- Bandit for security scanning
- Ruff for style checking
- cProfile for profiling
- Model development tools

## Workspace Management
- WorkspaceConfig dataclass for configuration
- WorkspaceManager class for workspace operations
- JSON-based configuration persistence
- Directory-based workspace isolation
- Tool-specific workspace directories
- Environment variable management
- Path management
- Settings management
- Rich console output
- Model workspace management
- Profiling workspace management
- Validation workspace management

## Development Setup

### Prerequisites
1. Python 3.8 or higher
2. pip package manager
3. virtualenv or similar virtual environment tool
4. Docker and Docker Compose
5. CUDA support (optional, for local models)
6. 16GB+ RAM (recommended for local models)

### Installation
1. Create and activate virtual environment
2. Install dependencies: `pip install -r requirements.txt`
3. Install development dependencies: `pip install -r requirements-dev.txt`
4. Start monitoring stack: `docker-compose up -d`
5. Set up API keys for code generation models
6. Configure local model paths

### Running the Server
1. Start the server: `python server.py [--debug]`
2. Access API at `http://localhost:9001`
3. SSE endpoint at `http://localhost:9001/sse`
4. Access Grafana at `http://localhost:3000`
5. Access Prometheus at `http://localhost:9090`
6. Access AlertManager at `http://localhost:9093`
7. Access profiling dashboard at `http://localhost:9001/profiling`

### Using the CLI
1. Run CLI commands: `python cli.py [command] [options] [--debug]`
2. Available command groups:
   - `command`: Execute and manage commands
   - `file`: File operations
   - `system`: System operations
   - `dev`: Development tools
   - `debug`: Debugging tools
   - `generate`: Code generation tools
   - `validate`: Code validation tools
   - `profile`: Profiling tools

### Debugging Tools
1. Start debugger: Add `--debug` flag
2. Available commands:
   - `list_tools`: Show available tools
   - `inspect`: View tool implementation
   - `break`: Set breakpoints
   - `watch`: Watch variables
   - `info`: Show debug info
   - `step`: Step execution
   - `continue`: Resume execution
   - `locals`: Show variables
   - `stack`: Show call stack
   - `model`: Model debugging
   - `profile`: Profiling analysis
   - `validate`: Validation debugging

## Workspace Commands
```bash
# List workspaces
mcp workspace list

# Create workspace
mcp workspace create <name> [description]

# Delete workspace
mcp workspace delete <name>

# Activate workspace
mcp workspace activate <name>

# Deactivate workspace
mcp workspace deactivate

# Add tool to workspace
mcp workspace tool add <name> <path>

# Remove tool from workspace
mcp workspace tool remove <name>

# Set environment variable
mcp workspace env set <key> <value>

# Add path to workspace
mcp workspace path add <path>

# Remove path from workspace
mcp workspace path remove <path>

# Update settings
mcp workspace settings update <key> <value>

# Show workspace info
mcp workspace info

# Configure model settings
mcp workspace model config <model> <key> <value>

# Set up profiling
mcp workspace profile setup <options>

# Configure validation
mcp workspace validate config <options>
```

## Debugging Commands
```bash
# List available tools
debug list_tools

# Inspect tool implementation
debug inspect <tool_name>

# Set breakpoint
debug break <tool_name> <line_number>

# Watch variable
debug watch <variable_name>

# Show debug info
debug info

# Step through execution
debug step

# Continue execution
debug continue

# Show local variables
debug locals

# Show call stack
debug stack

# Quit debugger
debug quit

# Model debugging
debug model <command> [options]

# Profiling analysis
debug profile <command> [options]

# Validation debugging
debug validate <command> [options]
```

## Technical Constraints

### Performance
- Command execution timeout limits
- Resource usage monitoring
- Profiling overhead management
- Metric collection overhead
- Alert processing latency
- Debug mode overhead
- Workspace switching overhead
- Configuration persistence latency
- Tool isolation impact
- Model inference time limits
- Profiling data size limits
- Validation performance impact

### Security
- Input validation
- Command execution restrictions
- File access controls
- Monitoring access control
- Alert access control
- Debug access control
- Workspace isolation
- Tool access control
- Environment separation
- Model access control
- Profiling data security
- Validation security

### Scalability
- Concurrent command execution
- Resource management
- Connection pooling
- Metric storage scaling
- Alert handling capacity
- Debug session management
- Workspace state consistency
- Tool availability
- Configuration persistence
- Model scaling
- Profiling data scaling
- Validation scaling

## Dependencies

### Core Dependencies
- FastAPI: Web framework
- uvicorn: ASGI server
- Click: CLI framework
- Rich: Terminal UI
- requests: HTTP client
- psutil: System monitoring
- OpenTelemetry: Observability
- anthropic: Claude API
- openai: OpenAI API
- transformers: Local models
- torch: Deep learning
- cProfile: Profiling

### Monitoring Dependencies
- opentelemetry-api: Core API
- opentelemetry-sdk: Implementation
- opentelemetry-exporter-prometheus: Prometheus export
- opentelemetry-exporter-otlp: OTLP export
- prometheus-client: Prometheus integration
- psutil: System metrics
- Model metrics collector
- Profiling metrics collector
- Validation metrics collector

### Debugging Dependencies
- cmd: Command interface
- inspect: Code introspection
- rich: Output formatting
- pdb: Python debugger
- threading: Synchronization
- functools: Decorators
- Model debugging tools
- Profiling analysis tools
- Validation debugging tools

### Development Dependencies
- pytest: Testing framework
- black: Code formatter
- flake8: Linter
- mypy: Type checker
- coverage: Code coverage
- Bandit: Security scanner
- Ruff: Style checker
- cProfile: Profiler
- Model development tools

### Alerting Dependencies
- Prometheus: Alert rules engine
- AlertManager: Alert routing
- Slack API: Notifications
- Email: Alternative notifications
- Templates: Alert formatting
- Model alerting
- Profiling alerts
- Validation alerts

### Workspace Dependencies
- pathlib: Path management
- dataclasses: Workspace configuration
- typing: Type hints
- shutil: Environment management
- json: Configuration storage
- Model workspace management
- Profiling workspace management
- Validation workspace management

## Integration Points

### External Systems
- Command execution system
- File system
- Process management
- System monitoring
- Alert notification services
- Debug interface
- Model APIs
- Profiling system
- Validation system

### Internal Components
- API server
- CLI interface
- Tool registry
- Profiling system
- Metrics collection
- Alert management
- Debug system
- Model management
- Profiling management
- Validation management

### Monitoring Components
- OpenTelemetry SDK
- OpenTelemetry Collector
- Prometheus server
- Grafana dashboard
- AlertManager
- Notification channels
- Metrics collection
- Alert management
- Dashboard system
- Model monitoring
- Profiling monitoring
- Validation monitoring

### Debugging Components
- Interactive console
- Breakpoint manager
- Variable watcher
- History tracker
- Stack inspector
- Tool inspector
- Model debugging tools
- Profiling analysis tools
- Validation debugging tools

## Configuration

### Server Configuration
- Host: localhost (default)
- Port: 9001 (default)
- Debug mode toggle
- Logging level

### CLI Configuration
- Default timeout values
- Output formatting options
- Color scheme customization
- Command history

### Monitoring Configuration
- Metric collection interval
- Retention period
- Dashboard refresh rate
- Alert thresholds
- Export endpoints

### Debug Configuration
- Tool registration
- Breakpoint settings
- Watch variables
- History size
- Output format
- Session persistence

### Alert Configuration
- Rule definitions
- Notification routing
- Channel settings
- Template customization
- Grouping policies

### Workspace Configuration
- Base directory: ~/.mcp/workspaces
- Config file: config.json
- Tool directory: tools/
- Data directory: data/
- Log directory: logs/
- Temp directory: temp/

## Deployment

### Requirements
- Python runtime
- System dependencies
- Configuration files
- Access permissions
- Docker environment
- Debug capabilities

### Process
1. Install dependencies
2. Configure environment
3. Start monitoring stack
4. Configure alerts
5. Enable debugging
6. Start server
7. Verify connectivity

## Monitoring

### System Metrics
- CPU usage
- Memory usage
- Disk usage
- Process count
- Network I/O

### Tool Metrics
- Execution count
- Error rate
- Response time
- Active tools
- Resource usage

### Alert Types
- Resource alerts
- Performance alerts
- Error rate alerts
- Prediction alerts
- Health check alerts

### Alert Channels
- Slack notifications
- Email notifications
- Web hooks
- Custom channels
- Alert history

## Testing

### Test Types
- Unit tests
- Integration tests
- System tests
- Load tests
- Alert tests
- Debug tests

### Test Coverage
- Code coverage targets
- Critical path testing
- Error handling
- Edge cases
- Alert validation
- Debug scenarios

## Documentation

### Code Documentation
- Docstrings
- Type hints
- Comments
- Examples
- Alert descriptions
- Debug instructions

### User Documentation
- API documentation
- CLI usage guide
- Tool descriptions
- Alert response guide
- Debug guide
- Runbook templates

## Future Considerations

### Planned Features
- Interactive debugging
- Workspace management
- Advanced profiling
- Custom tool development
- Alert correlation
- Remote debugging
- Templates
- Sharing
- Versioning
- Migration
- Plugins

### Technical Debt
- Test coverage improvements
- Documentation updates
- Code organization
- Performance optimization
- Alert refinement
- Debug optimization
- Configuration format
- Tool isolation
- Error handling

## API Structure

The API consists of tool functions that can be invoked remotely:

1. **Command Execution**:
   - `execute_command`: Run a command with timeout control
   - `read_output`: Stream output from a running command
   - `force_terminate`: Stop a running command

2. **Process Management**:
   - `list_sessions`: Show active command sessions
   - `list_processes`: List system processes
   - `kill_process`: Terminate a process by PID

3. **Command Control**:
   - `block_command`: Add a command to the blacklist
   - `unblock_command`: Remove a command from the blacklist

4. **File Operations**:
   - `read_file`: Read file contents with size limits
   - `write_file`: Write content to a file
   - `create_directory`: Create a new directory
   - `list_directory`: List directory contents
   - `move_file`: Move or rename files
   - `search_files`: Find files matching a pattern
   - `get_file_info`: Get metadata about a file

5. **Development Tools**:
   - `install_dependency`: Install Python packages using uv
   - `run_tests`: Execute tests with proper isolation
   - `format_code`: Run ruff formatting
   - `lint_code`: Run ruff linting
   - `filter_output`: Process and format long command outputs

6. **Code Analysis**:
   - `analyze_codebase`: Static code analysis
   - `monitor_performance`: System performance tracking
   - `manage_llm_context`: LLM context optimization
   - `enhanced_testing`: Advanced test execution

7. **Utilities**:
   - `system_info`: Get system information
   - `calculate`: Evaluate a mathematical expression
   - `context_length`: Track LLM context usage

## Performance Considerations

- **Output Streaming**: Efficient handling of process output
- **Resource Management**: Proper cleanup of resources for long-running processes
- **Memory Usage**: Careful handling of large file content or command output
- **Threading**: Proper synchronization for concurrent operations
- **Context Length**: Monitoring and managing LLM context usage
- **Test Performance**: Parallel test execution and efficient coverage tracking
- **System Monitoring**: Low-overhead performance metrics collection

## Observability Stack

### Distributed Tracing
- OpenTelemetry integration for distributed tracing
- OTLP exporter configured for trace collection
- Automatic tracing for all MCP tools via decorator pattern
- Configurable service name and version
- Default endpoint: http://localhost:4317

### Dependencies
- OpenTelemetry packages:
  - opentelemetry-api==1.31.1
  - opentelemetry-sdk==1.31.1
  - opentelemetry-exporter-otlp==1.31.1
  - opentelemetry-semantic-conventions==0.52b1

### Metrics Collection
- OpenTelemetry Metrics integration
- OTLP exporter for metrics collection
- Default endpoint: http://localhost:4317
- Key metrics:
  - Tool execution duration (histogram)
  - Tool call count (counter)
  - Error count (counter)
  - Active sessions (up/down counter)
  - Memory usage (observable gauge)

### System Dependencies
- psutil for system metrics collection
- OpenTelemetry metrics packages:
  - opentelemetry-sdk-metrics
  - opentelemetry-exporter-otlp-proto-grpc

### Profiling Tools
- cProfile integration for Python profiling
- Custom MCP profiler for tool-specific profiling
- Features:
  - Global profiling session management
  - Per-tool profiling with stats collection
  - Code block profiling
  - Stats file management
- Profiling data:
  - Execution time
  - Call counts
  - Cumulative statistics
  - Function-level performance

### Development Tools
- Built-in profiling tools:
  - start_profiling: Start global profiling session
  - stop_profiling: Stop profiling and get results
  - get_profiling_stats: Analyze stats files
  - profile_code: Profile arbitrary Python code

## L3 Coding Agent Tools

### Autonomous Execution Tools
- Plan Generation
  - Task breakdown algorithms
  - Dependency resolution
  - Resource estimation
  - Risk assessment
  - Checkpoint planning

- Execution Management
  - State tracking
  - Progress monitoring
  - Error detection
  - Recovery strategies
  - Rollback mechanisms

- Feature Implementation
  - Code scaffolding
  - Template generation
  - Best practices enforcement
  - Integration patterns
  - Testing strategies

### Validation Tools
- Code Quality Analysis
  - Static analysis
  - Dynamic analysis
  - Style checking
  - Complexity metrics
  - Best practices validation

- Test Simulation
  - Unit test generation
  - Integration test simulation
  - Edge case detection
  - Coverage analysis
  - Performance testing

- Impact Analysis
  - Dependency impact
  - Performance impact
  - Security implications
  - Resource utilization
  - Compatibility checking

### Context Awareness Tools
- Codebase Analysis
  - AST parsing
  - Semantic analysis
  - Pattern detection
  - Architecture mapping
  - Dependency tracking

- Context Management
  - State persistence
  - History tracking
  - Pattern learning
  - Knowledge base
  - Context restoration

- System Understanding
  - Architecture analysis
  - Component relationships
  - Interface mapping
  - Data flow analysis
  - Control flow analysis

### Iterative Problem-Solving
- Solution Management
  - Version control
  - Alternative tracking
  - Progress monitoring
  - Success metrics
  - Failure analysis

- Learning System
  - Pattern recognition
  - Solution optimization
  - Feedback integration
  - Knowledge persistence
  - Adaptation strategies

- Optimization Engine
  - Performance analysis
  - Resource optimization
  - Code simplification
  - Refactoring suggestions
  - Best practices application

## Technical Requirements

### Autonomous Execution
- Safe execution environment
- State management system
- Rollback capabilities
- Progress tracking
- Error recovery mechanisms

### Validation System
- Real-time validation
- Test simulation framework
- Impact analysis tools
- Security scanning
- Performance profiling

### Context Management
- Graph database for relationships
- Pattern recognition models
- Knowledge persistence
- State management
- History tracking

### Iterative Learning
- Version control system
- Pattern matching engine
- Learning persistence
- Feedback processing
- Optimization algorithms

## Dependencies

### Core Systems
- Graph databases (Neo4j/TigerGraph)
- Machine learning frameworks (PyTorch/TensorFlow)
- AST parsing tools (ast/astroid)
- Pattern matching engines (regex/automata)
- State management systems

### Analysis Tools
- Static analyzers (pylint/mypy)
- Dynamic analyzers
- Security scanners
- Performance profilers
- Coverage tools

### Learning Systems
- Pattern recognition models
- Optimization algorithms
- Knowledge bases
- Feedback processors
- Adaptation engines

### Integration Tools
- Version control systems
- CI/CD pipelines
- Testing frameworks
- Documentation generators
- Code formatters

## Performance Considerations

### Execution Speed
- Task breakdown optimization
- Parallel execution
- Resource management
- Cache utilization
- State persistence

### Memory Usage
- Context storage optimization
- Pattern database efficiency
- History management
- Cache strategies
- Resource cleanup

### Processing Overhead
- Analysis optimization
- Validation efficiency
- Learning system performance
- Pattern matching speed
- State tracking overhead

## Security Considerations

### Code Execution
- Sandboxed environments
- Permission management
- Resource limits
- Input validation
- Output sanitization

### Data Management
- Secure storage
- Access control
- Encryption
- Audit logging
- Compliance checking

### Integration Security
- API security
- Authentication
- Authorization
- Rate limiting
- Data validation

## Technical Constraints

### Performance
- LLM response times
- Token usage optimization
- Resource management
- Cache implementation

### Security
- API key management
- Code execution isolation
- Web access controls
- Dependency validation

### Scalability
- Model switching
- Parallel processing
- Resource allocation
- Cache management

## Performance Considerations

- **Output Streaming**: Efficient handling of process output
- **Resource Management**: Proper cleanup of resources for long-running processes
- **Memory Usage**: Careful handling of large file content or command output
- **Threading**: Proper synchronization for concurrent operations
- **Context Length**: Monitoring and managing LLM context usage
- **Test Performance**: Parallel test execution and efficient coverage tracking
- **System Monitoring**: Low-overhead performance metrics collection

## Observability Stack

### Distributed Tracing
- OpenTelemetry integration for distributed tracing
- OTLP exporter configured for trace collection
- Automatic tracing for all MCP tools via decorator pattern
- Configurable service name and version
- Default endpoint: http://localhost:4317

### Dependencies
- OpenTelemetry packages:
  - opentelemetry-api==1.31.1
  - opentelemetry-sdk==1.31.1
  - opentelemetry-exporter-otlp==1.31.1
  - opentelemetry-semantic-conventions==0.52b1

### Metrics Collection
- OpenTelemetry Metrics integration
- OTLP exporter for metrics collection
- Default endpoint: http://localhost:4317
- Key metrics:
  - Tool execution duration (histogram)
  - Tool call count (counter)
  - Error count (counter)
  - Active sessions (up/down counter)
  - Memory usage (observable gauge)

### System Dependencies
- psutil for system metrics collection
- OpenTelemetry metrics packages:
  - opentelemetry-sdk-metrics
  - opentelemetry-exporter-otlp-proto-grpc

### Profiling Tools
- cProfile integration for Python profiling
- Custom MCP profiler for tool-specific profiling
- Features:
  - Global profiling session management
  - Per-tool profiling with stats collection
  - Code block profiling
  - Stats file management
- Profiling data:
  - Execution time
  - Call counts
  - Cumulative statistics
  - Function-level performance

### Development Tools
- Built-in profiling tools:
  - start_profiling: Start global profiling session
  - stop_profiling: Stop profiling and get results
  - get_profiling_stats: Analyze stats files
  - profile_code: Profile arbitrary Python code

## Code Generation & Analysis Tools

### Code Generation
- **Models**:
  - API-based:
    - Claude-3-Sonnet (Anthropic)
    - GPT-4 (OpenAI)
  - Local:
    - Code Llama (34B Python)
    - StarCoder
- **Features**:
  - Multi-model support
  - Context-aware generation
  - Token tracking
  - Performance metrics
  - Error handling

### Code Validation
- **Checks**:
  - Syntax validation (AST-based)
  - Style checking (Ruff)
  - Complexity analysis (McCabe)
  - Security scanning (Bandit)
  - Performance analysis
- **Features**:
  - Comprehensive validation suite
  - Human-readable summaries
  - Detailed recommendations
  - Multi-language support (planned)

## Technical Requirements

### System Requirements
- Python 3.8+
- CUDA support (optional, for local models)
- 16GB+ RAM (32GB+ recommended for local models)
- SSD storage for model weights

### Dependencies
- **Core Libraries**:
  - anthropic
  - openai
  - torch
  - transformers
  - bandit
  - ruff
- **Optional Libraries**:
  - pytest (for testing)
  - black (for formatting)
  - mypy (for type checking)

### API Requirements
- Anthropic API key (for Claude)
- OpenAI API key (for GPT-4)
- Internet connection for API models

## Performance Constraints

### Code Generation
- API models:
  - Response time: < 2s
  - Token limit: Model-specific
  - Rate limits: Provider-specific
- Local models:
  - Response time: Hardware-dependent
  - Memory usage: 16GB+ per model
  - GPU memory: 24GB+ recommended

### Code Validation
- Validation time: < 100ms per check
- Memory usage: < 1GB
- CPU usage: Moderate

## Security Considerations

### API Security
- Secure API key storage
- Rate limiting
- Request validation
- Response sanitization

### Code Security
- Sandboxed execution
- Input validation
- Output sanitization
- Dependency scanning

## Implementation Details

### Relationship Builder
- **Core Components**
  - `Graph`: Base data structure for storing nodes and edges
  - `Node`: Represents code elements (functions, classes, variables)
  - `Edge`: Represents relationships between nodes
  - `RelationType`: Enum for relationship types
  - `RelationshipBuilder`: Main class for building relationships

- **Node Types**
  - `function`: Function definitions
  - `method`: Class methods
  - `class`: Class definitions
  - `variable`: Variables and parameters
  - `module`: Imported modules
  - `import`: Imported symbols
  - `attribute`: Class attributes

- **Relationship Types**
  - `CONTAINS`: Parent-child relationships
  - `CALLS`: Function/method calls
  - `INHERITS`: Class inheritance
  - `IMPORTS`: Module imports
  - `REFERENCES`: Variable/attribute references

### Current Implementation Status
- **Working Features**
  - Basic graph operations (add/remove nodes/edges)
  - Node and edge property management
  - Relationship type handling
  - Test infrastructure

- **In Progress**
  - Reference extraction and processing
  - Scope management
  - Error handling improvements
  - Test coverage expansion

- **Known Issues**
  - Node creation during file analysis
  - Edge creation for references
  - Directory analysis completeness
  - Test coverage gaps

### Technical Debt
1. **Error Handling**
   - Need more comprehensive error types
   - Better error recovery mechanisms
   - Improved error logging

2. **Validation**
   - Input validation for node/edge creation
   - Relationship validation rules
   - Property validation

3. **Testing**
   - More edge case coverage
   - Error handling tests
   - Integration tests

4. **Documentation**
   - API documentation
   - Usage examples
   - Error handling guide

## Architecture Patterns

### Model Management
- Factory pattern for model selection
- Strategy pattern for generation
- Observer pattern for metrics
- Decorator pattern for validation

### Error Handling
- Comprehensive error types
- Graceful degradation
- Detailed error messages
- Recovery strategies

### Metrics Collection
- Generation time
- Token usage
- Success rates
- Resource utilization

## Development Guidelines

### Code Style
- PEP 8 compliance
- Type hints required
- Docstrings required
- Comprehensive tests

### Testing Strategy
- Unit tests for core functions
- Integration tests for workflows
- Performance benchmarks
- Security tests

### Documentation
- API documentation
- Usage examples
- Configuration guide
- Troubleshooting guide

## Future Considerations

### Planned Enhancements
- Additional model support
- Language-specific validation
- Performance optimization
- Caching system
- Distributed execution

### Technical Debt
- Token tracking for local models
- Security scanning rules
- Performance analysis patterns
- Environment configuration
- Test coverage

## Server Configuration

### Transport Protocols
- **SSE (Server-Sent Events)**: Primary transport protocol for streaming data from server to client
- **HTTP REST API**: Used for standard request-response communication
- **WebSocket**: Not currently supported but being considered for future implementations
- **SSH**: Used for server management and tunneling

### Port Configuration
- Port 9001: MCP server (previously using port 7443)
- Port 8080: Reserved for additional services
- Port 80: Traefik HTTP entrypoint
- Port 443: Traefik HTTPS entrypoint
- Port 22: SSH access
- Port 8000: Development server

### Proxy Setup
- Traefik as reverse proxy
- Dynamic configuration via config files
- Path-based routing (/mcp for MCP server)
- Middleware for path stripping
- Health checks for services
- Automatic TLS certificate handling
- Load balancing capability

### Firewall Configuration
- iptables for firewall management
- Specific port allowances for required services
- Default deny policy for security
- SSH access always enabled
- Monitoring of connection attempts
- Rate limiting for protection

### SSH Tunnel Setup
```bash
# Create SSH tunnel to access MCP server
ssh -L 9001:localhost:9001 user@server

# Access MCP server locally
curl http://localhost:9001
```

### Server Hosting
- DigitalOcean cloud hosting
- Linux-based OS (Ubuntu)
- Root access for management
- Automated service management
- systemd for service control
- Centralized logging
- Regular backups

## Network Troubleshooting Commands
```bash
# Check if server is running
ps aux | grep mcp

# Test server connectivity
curl -v http://localhost:9001

# Check open ports
ss -tulpn | grep 9001

# Check firewall status
iptables -L -n -v

# Monitor network traffic
tcpdump -i any port 9001 -n

# Trace network route
traceroute host

# Set up SSH tunnel
ssh -L 9001:localhost:9001 user@server

# Test proxy configuration
curl -v http://server/mcp

# Check proxy logs
journalctl -u traefik

# Restart MCP server
kill -9 PID && cd /path/to/server && . .venv/bin/activate && mcp dev server.py &
```

## Development Environment

### Language & Runtime
- Python 3.13.0
- Virtual environment management with venv
- Package management with pip

### Dependencies
- tree-sitter: Language-agnostic parser
- pytest: Testing framework
- pytest-cov: Coverage reporting
- logging: Standard library logging

### Development Tools
- VSCode/Cursor as primary IDE
- Git for version control
- pytest for testing
- Coverage.py for test coverage

## Project Structure

### Core Directories
```
server/
├── code_understanding/
│   ├── __init__.py
│   ├── parser.py
│   ├── analyzer.py
│   ├── extractor.py
│   ├── symbols.py
│   └── build_languages.py
├── core.py
└── llm.py

tests/
├── __init__.py
└── test_parser.py

memory-bank/
├── project-brief.md
├── product-context.md
├── active-context.md
├── system-patterns.md
├── tech-context.md
└── progress.md
```

### Key Files
- `parser.py`: Code parsing with tree-sitter
- `analyzer.py`: Syntax tree analysis
- `extractor.py`: Symbol extraction
- `symbols.py`: Symbol management
- `build_languages.py`: Tree-sitter language building

## Technical Constraints

### Performance
- Memory usage for large codebases
- Parse time for large files
- Graph traversal efficiency
- Cache management

### Security
- Code execution safety
- Input validation
- Resource limits
- Access control

### Scalability
- Incremental analysis
- Parallel processing
- Resource management
- Cache invalidation

## Integration Points

### Tree-sitter
- Language-agnostic parsing
- Syntax tree generation
- Language support management
- Error handling

### MCP Interface
- Command handling
- Response formatting
- Error reporting
- State management

### Testing Framework
- Unit tests
- Integration tests
- Coverage reporting
- Performance testing

## Development Practices

### Code Style
- PEP 8 compliance
- Type hints
- Comprehensive docstrings
- Clear error messages

### Testing
- Unit tests for components
- Integration tests for workflows
- Coverage targets (>80%)
- Performance benchmarks

### Documentation
- Inline documentation
- API documentation
- Usage examples
- Architecture docs

### Version Control
- Feature branches
- Pull requests
- Code review
- Version tagging

## Future Considerations

### Language Support
- Additional tree-sitter parsers
- Language-specific analysis
- Custom parsing rules
- Language detection

### Performance Optimization
- Caching strategies
- Parallel processing
- Memory management
- Resource pooling

### Tool Integration
- IDE plugins
- CI/CD integration
- API endpoints
- Monitoring tools

### Scalability
- Distributed processing
- Load balancing
- Resource scaling
- Data partitioning

# Technical Context

## Technology Stack

### Core Technologies
1. **Python 3.13**
   - Main implementation language
   - Type hints support
   - Async/await support
   - Performance improvements

2. **Tree-sitter**
   - Code parsing
   - Language support
   - Incremental parsing
   - Error recovery

3. **FastAPI**
   - REST API
   - WebSocket support
   - OpenAPI documentation
   - Performance optimization

### Language Support
1. **Python**
   - Tree-sitter Python grammar
   - Type hints
   - Decorators
   - Async/await

2. **JavaScript**
   - Tree-sitter JavaScript grammar
   - ES6+ features
   - Modules
   - TypeScript support

3. **Swift**
   - Tree-sitter Swift grammar
   - Type system
   - Protocols
   - Extensions

### Analysis Tools
1. **Static Analysis**
   - Type inference
   - Control flow analysis
   - Data flow analysis
   - Symbol resolution

2. **Graph Processing**
   - NetworkX
   - Graph visualization
   - Query optimization
   - Path analysis

3. **Performance Tools**
   - Profiling
   - Memory tracking
   - Cache analysis
   - Resource monitoring

## Development Setup

### Environment
1. **Python Environment**
   - Virtual environment
   - Dependency management
   - Development tools
   - Testing framework

2. **Language Support**
   - Tree-sitter setup
   - Grammar compilation
   - Language detection
   - Cross-language testing

3. **Analysis Tools**
   - Static analysis setup
   - Graph processing
   - Performance tools
   - Visualization tools

### Dependencies
1. **Core Dependencies**
   - tree-sitter
   - fastapi
   - pydantic
   - networkx

2. **Language Dependencies**
   - tree-sitter-python
   - tree-sitter-javascript
   - tree-sitter-swift
   - language-specific tools

3. **Analysis Dependencies**
   - type inference tools
   - flow analysis tools
   - graph processing tools
   - visualization tools

### Development Tools
1. **Code Quality**
   - Linters
   - Formatters
   - Type checkers
   - Security scanners

2. **Testing Tools**
   - Unit testing
   - Integration testing
   - Performance testing
   - Coverage tools

3. **Documentation Tools**
   - API documentation
   - Architecture docs
   - User guides
   - Examples

## Technical Constraints

### Performance Requirements
1. **Response Time**
   - API endpoints < 100ms
   - Analysis < 1s per file
   - Graph queries < 50ms
   - Cache hits < 10ms

2. **Resource Usage**
   - Memory < 1GB per analysis
   - CPU < 80% utilization
   - Disk < 10GB cache
   - Network < 100MB/s

3. **Scalability**
   - Support 100k+ files
   - Handle multiple languages
   - Process large graphs
   - Concurrent analysis

### Language Support
1. **Python Features**
   - Type hints
   - Decorators
   - Async/await
   - Metaclasses

2. **JavaScript Features**
   - ES6+ syntax
   - Modules
   - Classes
   - Promises

3. **Swift Features**
   - Type system
   - Protocols
   - Extensions
   - Generics

### Analysis Capabilities
1. **Static Analysis**
   - Type inference
   - Control flow
   - Data flow
   - Symbol resolution

2. **Graph Analysis**
   - Node relationships
   - Edge types
   - Path finding
   - Cycle detection

3. **Performance Analysis**
   - Memory usage
   - CPU usage
   - I/O patterns
   - Cache efficiency

## Testing Requirements

### Unit Testing
1. **Parser Tests**
   - Syntax parsing
   - Error handling
   - Edge cases
   - Performance

2. **Analyzer Tests**
   - Symbol extraction
   - Type inference
   - Flow analysis
   - Context tracking

3. **Graph Tests**
   - Node creation
   - Edge creation
   - Relationships
   - Queries

### Integration Testing
1. **Cross-language Tests**
   - Multi-language projects
   - Reference resolution
   - Type compatibility
   - Performance impact

2. **End-to-end Tests**
   - Complete pipeline
   - Large codebases
   - Real scenarios
   - Benchmarks

### Performance Testing
1. **Scalability Tests**
   - Large projects
   - Multiple languages
   - Complex graphs
   - Resource usage

2. **Optimization Tests**
   - Caching
   - Incremental analysis
   - Parallel processing
   - Resource efficiency

## Documentation Requirements

### API Documentation
1. **Endpoints**
   - REST API
   - WebSocket
   - Graph queries
   - Analysis options

2. **Data Models**
   - Request/response
   - Graph structure
   - Analysis results
   - Error types

3. **Usage Examples**
   - Basic usage
   - Advanced features
   - Performance tips
   - Troubleshooting

### Architecture Documentation
1. **Components**
   - Parser layer
   - Analyzer layer
   - Semantic layer
   - Graph layer

2. **Design Patterns**
   - Language support
   - Analysis patterns
   - Testing patterns
   - Error handling

3. **Implementation Guide**
   - Setup guide
   - Development guide
   - Testing guide
   - Deployment guide

### User Documentation
1. **Getting Started**
   - Installation
   - Configuration
   - Basic usage
   - Examples

2. **Advanced Usage**
   - Language support
   - Analysis options
   - Graph queries
   - Performance tuning

3. **Troubleshooting**
   - Common issues
   - Error messages
   - Performance problems
   - Solutions
</file>

<file path="memory-bank/test_implementation_plan.md">
# Test Implementation Plan for MCP Tools

## Overview

This document outlines the plan for implementing tests for MCP tools that currently lack test coverage. The implementation follows best practices for Python testing, using pytest as the primary testing framework.

## Test Coverage Analysis

Based on our code analysis, the following 13 MCP tools require test implementation:

1. get_trace_info
2. configure_tracing
3. get_metrics_info
4. configure_metrics
5. install_dependency
6. run_tests
7. format_code
8. lint_code
9. monitor_performance
10. generate_documentation
11. setup_validation_gates
12. analyze_project
13. manage_changes

## Implementation Approach

### Phase 1: Observability Tools

#### get_trace_info Tests
```python
def test_get_trace_info_success(mocker):
    """Test successful retrieval of tracing information."""
    # Mock tracer and span
    mock_span = mocker.MagicMock()
    mock_span.name = "test_span"
    mock_span.get_span_context = mocker.MagicMock(return_value="context")
    mocker.patch('opentelemetry.trace.get_current_span', return_value=mock_span)
    
    # Execute the tool
    result = server.get_trace_info()
    
    # Verify result
    assert result["status"] == "success"
    assert result["tracer"]["name"] == tracer.name
    assert result["current_span"]["name"] == "test_span"
    assert result["current_span"]["active"] == True

def test_get_trace_info_no_span(mocker):
    """Test tracing info when no span is active."""
    # Mock no active span
    mocker.patch('opentelemetry.trace.get_current_span', return_value=None)
    
    # Execute the tool
    result = server.get_trace_info()
    
    # Verify result
    assert result["status"] == "success"
    assert result["current_span"]["name"] == None
    assert result["current_span"]["active"] == False

def test_get_trace_info_error(mocker):
    """Test error handling in get_trace_info."""
    # Mock exception
    mocker.patch('opentelemetry.trace.get_current_span', 
                 side_effect=Exception("Test error"))
    
    # Execute the tool
    result = server.get_trace_info()
    
    # Verify result
    assert result["status"] == "error"
    assert "error" in result
    assert "Test error" in result["error"]
```

#### configure_tracing Tests
```python
def test_configure_tracing_endpoint(mocker):
    """Test configuring tracing with custom endpoint."""
    # Mock dependencies
    mock_exporter = mocker.MagicMock()
    mock_processor = mocker.MagicMock()
    mock_provider = mocker.MagicMock()
    
    mocker.patch('opentelemetry.exporter.otlp.proto.grpc.trace_exporter.OTLPSpanExporter', 
                 return_value=mock_exporter)
    mocker.patch('opentelemetry.sdk.trace.export.BatchSpanProcessor', 
                 return_value=mock_processor)
    mocker.patch('opentelemetry.sdk.trace.TracerProvider', 
                 return_value=mock_provider)
    mocker.patch('opentelemetry.trace.set_tracer_provider')
    
    # Execute the tool
    result = server.configure_tracing(exporter_endpoint="http://custom:4317")
    
    # Verify result
    assert result["status"] == "success"
    assert "config" in result
    assert result["config"]["exporter_endpoint"] == "http://custom:4317"

def test_configure_tracing_service_info(mocker):
    """Test configuring tracing with service information."""
    # Mock dependencies
    mock_resource = mocker.MagicMock()
    mock_provider = mocker.MagicMock()
    
    mocker.patch('opentelemetry.sdk.resources.Resource', return_value=mock_resource)
    mocker.patch('opentelemetry.sdk.trace.TracerProvider', return_value=mock_provider)
    mocker.patch('opentelemetry.trace.set_tracer_provider')
    
    # Execute the tool
    result = server.configure_tracing(
        service_name="test-service", 
        service_version="1.0.0"
    )
    
    # Verify result
    assert result["status"] == "success"
    assert "config" in result

def test_configure_tracing_error(mocker):
    """Test error handling in configure_tracing."""
    # Mock exception
    mocker.patch('opentelemetry.exporter.otlp.proto.grpc.trace_exporter.OTLPSpanExporter', 
                 side_effect=Exception("Test error"))
    
    # Execute the tool
    result = server.configure_tracing(exporter_endpoint="http://custom:4317")
    
    # Verify result
    assert result["status"] == "error"
    assert "error" in result
    assert "Test error" in result["error"]
```

Similar test implementations will be provided for get_metrics_info and configure_metrics.

### Phase 2: Development Tools

#### install_dependency Tests
```python
def test_install_dependency_success(mocker, tmp_path):
    """Test successful package installation."""
    # Setup mock environment
    mock_run = mocker.patch('subprocess.run')
    mock_run.return_value = mocker.MagicMock(
        stdout="Successfully installed test-package-1.0.0",
        stderr="",
        returncode=0
    )
    
    # Create mock pyproject.toml
    pyproject_path = tmp_path / "pyproject.toml"
    pyproject_path.write_text('[tool.poetry.dependencies]\npython = "^3.8"\ntest-package = "1.0.0"\n')
    mocker.patch('builtins.open', mocker.mock_open(read_data=pyproject_path.read_text()))
    
    # Execute the tool
    result = server.install_dependency("test-package")
    
    # Verify result
    assert result["status"] == "success"
    assert "test-package" in result["package"]
    mock_run.assert_called_once()
    assert "uv" in mock_run.call_args[0][0][0]
    assert "add" in mock_run.call_args[0][0][1]
    assert "test-package" in mock_run.call_args[0][0][-1]

def test_install_dependency_dev(mocker, tmp_path):
    """Test installing package as dev dependency."""
    # Setup mock
    mock_run = mocker.patch('subprocess.run')
    mock_run.return_value = mocker.MagicMock(
        stdout="Successfully installed test-package-1.0.0",
        stderr="",
        returncode=0
    )
    
    # Create mock pyproject.toml
    pyproject_path = tmp_path / "pyproject.toml"
    pyproject_path.write_text('[tool.poetry.dev-dependencies]\ntest-package = "1.0.0"\n')
    mocker.patch('builtins.open', mocker.mock_open(read_data=pyproject_path.read_text()))
    
    # Execute the tool
    result = server.install_dependency("test-package", dev=True)
    
    # Verify result
    assert result["status"] == "success"
    assert "test-package" in result["package"]
    mock_run.assert_called_once()
    assert "--dev" in mock_run.call_args[0][0]

def test_install_dependency_error(mocker):
    """Test error handling during package installation."""
    # Setup mock for failed installation
    mock_run = mocker.patch('subprocess.run')
    mock_run.side_effect = subprocess.CalledProcessError(
        returncode=1,
        cmd=["uv", "add", "non-existent-package"],
        stderr="Package not found"
    )
    
    # Execute the tool
    result = server.install_dependency("non-existent-package")
    
    # Verify result
    assert result["status"] == "error"
    assert "error" in result
    assert "Package not found" in result["error"]
```

Similar test implementations will be provided for run_tests, format_code, and lint_code.

### Phase 3: Monitoring and Documentation Tools

#### monitor_performance Tests
```python
def test_monitor_performance_basic(mocker):
    """Test basic performance monitoring functionality."""
    # Mock psutil functions
    mock_cpu = mocker.patch('psutil.cpu_percent', return_value=10.5)
    mock_cpu_count = mocker.patch('psutil.cpu_count', return_value=8)
    mock_cpu_freq = mocker.patch('psutil.cpu_freq', return_value=mocker.MagicMock(_asdict=lambda: {"current": 2400}))
    mock_memory = mocker.patch('psutil.virtual_memory', return_value=mocker.MagicMock(
        total=16000000000, available=8000000000, percent=50.0, used=8000000000, free=8000000000
    ))
    mock_disk = mocker.patch('psutil.disk_usage', return_value=mocker.MagicMock(
        total=512000000000, used=128000000000, free=384000000000, percent=25.0
    ))
    mock_net = mocker.patch('psutil.net_io_counters', side_effect=[
        mocker.MagicMock(bytes_sent=1000, bytes_recv=2000, packets_sent=10, packets_recv=20),
        mocker.MagicMock(bytes_sent=1500, bytes_recv=3000, packets_sent=15, packets_recv=30)
    ])
    
    # Mock time functions to control test duration
    mocker.patch('time.sleep', return_value=None)
    time_values = [0, 0.5, 1.0, 1.5, 2.0]
    time_mock = mocker.patch('time.time')
    time_mock.side_effect = time_values
    
    mock_datetime = mocker.patch('datetime.datetime')
    mock_datetime.now.return_value = mocker.MagicMock(isoformat=lambda: "2023-01-01T12:00:00")
    
    # Execute the tool with short duration for testing
    result = server.monitor_performance(duration=2, interval=0.5)
    
    # Verify result
    assert result["status"] == "success"
    assert "metrics" in result
    assert "summary" in result
    assert len(result["metrics"]["cpu"]) > 0
    assert len(result["metrics"]["memory"]) > 0
    assert len(result["metrics"]["disk"]) > 0
    assert len(result["metrics"]["network"]) > 0
    
    # Verify summary calculations
    assert "avg" in result["summary"]["cpu"]
    assert "avg_percent" in result["summary"]["memory"]
    assert "avg_percent" in result["summary"]["disk"]
    assert "total_sent" in result["summary"]["network"]
    assert "total_recv" in result["summary"]["network"]

def test_monitor_performance_error(mocker):
    """Test error handling in performance monitoring."""
    # Mock psutil to raise exception
    mocker.patch('psutil.cpu_percent', side_effect=Exception("Test error"))
    
    # Execute the tool
    result = server.monitor_performance(duration=1)
    
    # Verify result
    assert result["status"] == "error"
    assert "error" in result
    assert "Test error" in result["error"]
```

#### generate_documentation Tests
```python
def test_generate_api_docs(mocker, tmp_path):
    """Test API documentation generation."""
    # Mock dependencies
    mock_pdoc = mocker.patch('pdoc.doc.Module')
    mock_pdoc.return_value.html.return_value = "<html>Test API docs</html>"
    mocker.patch('importlib.import_module', return_value=mocker.MagicMock())
    mocker.patch('os.makedirs')
    mock_open = mocker.patch('builtins.open', mocker.mock_open())
    
    # Execute the tool
    result = server.generate_documentation("test_module", doc_type="api")
    
    # Verify result
    assert result["status"] == "success"
    assert "output_file" in result
    assert mock_open.called
    mock_open().write.assert_called_once_with("<html>Test API docs</html>")

def test_generate_readme(mocker, tmp_path):
    """Test README generation."""
    # Mock dependencies
    mock_analyze = mocker.patch('server._analyze_project_info')
    mock_analyze.return_value = {
        "name": "Test Project",
        "description": "Test description",
        "setup": "Test setup",
        "usage": "Test usage",
        "api": "Test API",
        "contributing": "Test contributing"
    }
    mock_open = mocker.patch('builtins.open', mocker.mock_open())
    
    # Execute the tool
    result = server.generate_documentation("test_dir", doc_type="readme")
    
    # Verify result
    assert result["status"] == "success"
    assert "output_file" in result
    assert mock_open.called
    # Verify template expansion
    write_call = mock_open().write.call_args[0][0]
    assert "Test Project" in write_call
    assert "Test description" in write_call

def test_generate_documentation_error(mocker):
    """Test error handling in documentation generation."""
    # Mock exception
    mocker.patch('importlib.import_module', side_effect=ImportError("Module not found"))
    
    # Execute the tool
    result = server.generate_documentation("non_existent_module", doc_type="api")
    
    # Verify result
    assert result["status"] == "error"
    assert "error" in result
    assert "Module not found" in result["error"]
```

### Phase 4: Project Management Tools

Similar detailed test implementations will be provided for setup_validation_gates, analyze_project, and manage_changes tools.

### Phase 5: AI Coding Agent Tools

#### Test Goals
- Verify that each AI Coding Agent tool functions correctly
- Ensure tools handle various programming languages and project structures
- Test error handling and edge cases
- Validate performance characteristics meet requirements

#### understand_code Tests

```python
class TestUnderstandCode:
    """Tests for the understand_code tool."""

    def setup_method(self):
        """Set up test fixtures."""
        self.test_project = create_test_project()
        
    def teardown_method(self):
        """Clean up test resources."""
        clean_test_project(self.test_project)
        
    def test_understand_code_basic_analysis(self):
        """Test basic code analysis functionality."""
        # Arrange
        target_path = os.path.join(self.test_project, "sample.py")
        
        # Act
        result = server.understand_code(
            target_path=target_path,
            analysis_depth=1,
            include_external_deps=False,
            output_format="graph"
        )
        
        # Assert
        assert result["status"] == "success"
        assert "output" in result
        assert "graph" in result["output"]
        assert "metadata" in result
        assert result["metadata"]["files_analyzed"] > 0
        
    def test_understand_code_deep_analysis(self):
        """Test deep analysis with multiple files."""
        # Arrange
        target_path = self.test_project
        
        # Act
        result = server.understand_code(
            target_path=target_path,
            analysis_depth=3,
            include_external_deps=True,
            output_format="map"
        )
        
        # Assert
        assert result["status"] == "success"
        assert "output" in result
        assert "map" in result["output"]
        assert result["metadata"]["files_analyzed"] > 1
        assert result["metadata"]["relationships_found"] > 0
        
    def test_understand_code_invalid_path(self):
        """Test error handling for invalid path."""
        # Arrange
        target_path = "/path/does/not/exist"
        
        # Act
        result = server.understand_code(
            target_path=target_path,
            analysis_depth=1
        )
        
        # Assert
        assert result["status"] == "error"
        assert "error" in result
```

#### refactor_code Tests

```python
class TestRefactorCode:
    """Tests for the refactor_code tool."""
    
    def setup_method(self):
        """Set up test fixtures."""
        self.test_project = create_test_project()
        
    def teardown_method(self):
        """Clean up test resources."""
        clean_test_project(self.test_project)
        
    def test_refactor_code_rename_symbol(self):
        """Test refactoring that renames a symbol."""
        # Arrange
        target_path = os.path.join(self.test_project, "sample.py")
        refactoring_type = "rename"
        original_name = "example_function"
        new_name = "renamed_function"
        
        # Act
        result = server.refactor_code(
            target_path=target_path,
            refactoring_type=refactoring_type,
            original_name=original_name,
            new_name=new_name
        )
        
        # Assert
        assert result["status"] == "success"
        assert result["changes"] > 0
        assert "modified_files" in result
        
        # Verify the function was actually renamed
        with open(target_path, "r") as f:
            content = f.read()
            assert original_name not in content
            assert new_name in content
            
    def test_refactor_code_extract_method(self):
        """Test refactoring that extracts a method."""
        # Implementation details...
        pass
        
    def test_refactor_code_error_handling(self):
        """Test error handling in refactoring."""
        # Implementation details...
        pass
```

#### generate_tests Tests

```python
class TestGenerateTests:
    """Tests for the generate_tests tool."""
    
    def setup_method(self):
        """Set up test fixtures."""
        self.test_project = create_test_project()
        
    def teardown_method(self):
        """Clean up test resources."""
        clean_test_project(self.test_project)
        
    def test_generate_tests_basic_function(self):
        """Test generating tests for a basic function."""
        # Arrange
        target_path = os.path.join(self.test_project, "sample.py")
        
        # Act
        result = server.generate_tests(
            target_path=target_path,
            coverage_target=0.8
        )
        
        # Assert
        assert result["status"] == "success"
        assert "tests" in result
        assert len(result["tests"]) > 0
        assert "test_file_path" in result
        
        # Verify test file was created and contains tests
        assert os.path.exists(result["test_file_path"])
        with open(result["test_file_path"], "r") as f:
            content = f.read()
            assert "test_" in content
            assert "assert" in content
            
    def test_generate_tests_complex_class(self):
        """Test generating tests for a complex class."""
        # Implementation details...
        pass
        
    def test_generate_tests_edge_cases(self):
        """Test generation of edge case tests."""
        # Implementation details...
        pass
```

## Test Environment Setup

For all tests, we'll use the following setup:

1. **pytest Configuration**:
   - Use pytest fixtures for common setup
   - Enable pytest-cov for coverage reporting
   - Configure pytest-mock for mocking dependencies

2. **Mock External Dependencies**:
   - Subprocess calls
   - File system operations
   - Network requests
   - API client interactions

3. **Test Data**:
   - Create test fixtures with sample data
   - Use temporary directories for file operations
   - Set up mock responses for API calls

## Implementation Timeline

1. **Week 1**: Implement tests for observability tools
   - get_trace_info
   - configure_tracing
   - get_metrics_info
   - configure_metrics

2. **Week 2**: Implement tests for development tools
   - install_dependency
   - run_tests
   - format_code
   - lint_code

3. **Week 3**: Implement tests for monitoring and documentation tools
   - monitor_performance
   - generate_documentation

4. **Week 4**: Implement tests for project management tools
   - setup_validation_gates
   - analyze_project
   - manage_changes

5. **Week 5**: Implement tests for AI Coding Agent tools
   - understand_code
   - refactor_code
   - generate_tests
   - analyze_dependencies
   - review_code

6. **Week 6**: Finalize and optimize test suite
   - Improve test coverage where needed
   - Optimize test performance
   - Document testing patterns and practices

## Success Criteria

1. **Coverage Metrics**:
   - Minimum 80% test coverage for each tool
   - All primary functionality paths tested
   - Error handling paths tested

2. **Test Quality**:
   - Tests are independent and isolated
   - Tests are deterministic (no flaky tests)
   - Tests are fast (execution < 5 seconds per test)

3. **Documentation**:
   - Test patterns documented
   - Test approach explained
   - Coverage reports generated
</file>

<file path="memory-bank/test_suite_summary.md">
# Terminal Command Runner MCP - Test Suite Summary

## Implemented Components

1. **Test Directory Structure**:
   - `tests/` directory for all test files
   - `tests/data/` for test data files
   - `tests/conftest.py` for common fixtures

2. **Test Files**:
   - `test_command_execution.py`: Tests for command execution and process management
   - `test_file_operations.py`: Tests for file system operations
   - `test_system_utilities.py`: Tests for system utilities and miscellaneous features

3. **Test Utilities**:
   - `run_tests.py`: Script to run all tests with configurable options
   - `missing_functions.py`: Script to identify missing functions
   - `test_manually.py`: Manual test script for file operations
   - `test_system_manually.py`: Manual test script for system utilities

4. **Test Fixtures**:
   - `temp_dir`: Creates a temporary directory for file tests
   - `sample_text_file`: Creates a sample text file for testing
   - `long_running_process`: Sets up a long-running process for testing
   - Various mock fixtures for testing without side effects

5. **Memory Bank Documentation**:
   - Created core Memory Bank files with project documentation
   - Documented missing functions in `memory-bank/missing_functions.md`
   - Updated README.md with testing information

## Test Results

### Manual Tests

All manual tests for file operations and system utilities are now passing:

1. **File Operations Tests**:
   - Reading files ✅
   - Writing files ✅
   - Creating directories ✅
   - Listing directory contents ✅
   - Moving files ✅
   - Searching files ✅
   - Getting file information ✅

2. **System Utilities Tests**:
   - System information retrieval ✅
   - Mathematical expression evaluation ✅
   - Edit block functionality ✅
   - Process listing ✅
   - Command execution and control ✅
   - Command blacklisting ✅

### Automated Test Status

The pytest-based automated tests are not currently running due to environment issues. The following problems need to be addressed:

1. **Virtual Environment Configuration**: The virtual environment needs to be properly set up with pytest.
2. **Server Startup Issues**: The server starts automatically when imported, causing port conflicts.
3. **Test Isolation**: Need to ensure tests can run independently without interfering with each other.

## Test Categories

1. **Command Execution Tests**:
   - Basic command execution with different exit codes
   - Command output capture and validation
   - Timeout handling for long-running commands
   - Background process management
   - Process termination
   - Command blacklisting for security

2. **File Operation Tests**:
   - Reading files with size limits
   - Writing content to files
   - Creating directories
   - Listing directory contents
   - Moving/renaming files
   - Searching for files with patterns
   - Getting file metadata

3. **System Utility Tests**:
   - System information retrieval
   - Mathematical expression evaluation
   - Process listing and management
   - Session tracking
   - Output streaming verification

## Next Steps

1. **Improve Test Environment**:
   - Fix the virtual environment setup for pytest
   - Modify server.py to avoid auto-starting during tests
   - Implement proper test isolation with fixtures

2. **Expand Test Coverage**:
   - Add more edge case tests
   - Test error conditions more thoroughly
   - Add performance tests

3. **CI/CD Integration**:
   - Add GitHub Actions or other CI system for automated testing
   - Configure code coverage reporting

4. **Documentation**:
   - Add docstrings to all test functions
   - Create a detailed testing guide
</file>

<file path="monitoring/alertmanager-templates/slack.tmpl">
{{ define "slack.default.title" }}
[{{ .Status | toUpper }}{{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{ end }}] {{ .CommonLabels.alertname }}
{{ end }}

{{ define "slack.default.text" }}
{{ if gt (len .Alerts.Firing) 0 }}
*Alerts Firing:*
{{ range .Alerts.Firing }}
• {{ .Labels.severity | toUpper }}: {{ .Annotations.description }}
  - Started: {{ .StartsAt | since }}
  {{ if .Labels.tool }}- Tool: {{ .Labels.tool }}{{ end }}
  - Value: {{ .Value }}
{{ end }}
{{ end }}

{{ if gt (len .Alerts.Resolved) 0 }}
*Alerts Resolved:*
{{ range .Alerts.Resolved }}
• {{ .Labels.severity | toUpper }}: {{ .Annotations.description }}
  - Resolved: {{ .EndsAt | since }}
  {{ if .Labels.tool }}- Tool: {{ .Labels.tool }}{{ end }}
{{ end }}
{{ end }}
{{ end }}

{{ define "slack.default.footer" }}
{{ if .CommonLabels.job }}Job: {{ .CommonLabels.job }} | {{ end }}
{{ if .CommonLabels.instance }}Instance: {{ .CommonLabels.instance }} | {{ end }}
Managed by AlertManager
{{ end }}
</file>

<file path="monitoring/alertmanager.yml">
global:
  resolve_timeout: 5m
  slack_api_url: "https://hooks.slack.com/services/your/slack/webhook"
route:
  group_by: ["alertname", "job"]
  group_wait: 30s
  group_interval: 5m
  repeat_interval: 4h
  receiver: "slack-notifications"
  routes:
    - match:
        severity: critical
      receiver: "slack-critical"
      group_wait: 10s
      repeat_interval: 1h
receivers:
  - name: "slack-notifications"
    slack_configs:
      - channel: "#mcp-alerts"
        send_resolved: true
        title: '{{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        footer: '{{ template "slack.default.footer" . }}'
  - name: "slack-critical"
    slack_configs:
      - channel: "#mcp-critical"
        send_resolved: true
        title: '[CRITICAL] {{ template "slack.default.title" . }}'
        text: '{{ template "slack.default.text" . }}'
        footer: '{{ template "slack.default.footer" . }}'
templates:
  - "/etc/alertmanager/template/*.tmpl"
inhibit_rules:
  - source_match:
      severity: "critical"
    target_match:
      severity: "warning"
    equal: ["alertname", "job"]
</file>

<file path="monitoring/dashboard.py">
#!/usr/bin/env python3
from opentelemetry import metrics
from opentelemetry.exporter.prometheus import PrometheusMetricReader
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.http.metric_exporter import OTLPMetricExporter
from prometheus_client import start_http_server
import psutil
import time
from typing import Dict, Any
import logging
logger = logging.getLogger(__name__)
class MCPMonitoring:
    def __init__(self, 
                 prometheus_port: int = 9464,
                 otlp_endpoint: str = "http://localhost:4318/v1/metrics",
                 export_interval_ms: int = 5000):
        """Initialize the monitoring system"""
        self.prometheus_port = prometheus_port
        self.otlp_endpoint = otlp_endpoint
        self.export_interval_ms = export_interval_ms
        self.meter = None
        self.system_metrics = {}
        self.tool_metrics = {}
    def setup(self):
        """Set up the monitoring system with both Prometheus and OTLP exporters"""
        # Start Prometheus server
        start_http_server(self.prometheus_port)
        # Create metric readers
        prometheus_reader = PrometheusMetricReader()
        otlp_reader = PeriodicExportingMetricReader(
            OTLPMetricExporter(endpoint=self.otlp_endpoint),
            export_interval_ms=self.export_interval_ms
        )
        # Create and set meter provider
        provider = MeterProvider(metric_readers=[prometheus_reader, otlp_reader])
        metrics.set_meter_provider(provider)
        # Get meter for MCP metrics
        self.meter = metrics.get_meter("mcp.monitoring")
        # Initialize metrics
        self._setup_system_metrics()
        self._setup_tool_metrics()
        logger.info(f"Monitoring system started - Prometheus port: {self.prometheus_port}")
    def _setup_system_metrics(self):
        """Set up system-level metrics"""
        # CPU Usage
        self.system_metrics["cpu_usage"] = self.meter.create_observable_gauge(
            "mcp.system.cpu_usage",
            description="CPU usage percentage",
            unit="percent",
            callbacks=[self._get_cpu_usage]
        )
        # Memory Usage
        self.system_metrics["memory_usage"] = self.meter.create_observable_gauge(
            "mcp.system.memory_usage",
            description="Memory usage percentage",
            unit="percent",
            callbacks=[self._get_memory_usage]
        )
        # Disk Usage
        self.system_metrics["disk_usage"] = self.meter.create_observable_gauge(
            "mcp.system.disk_usage",
            description="Disk usage percentage",
            unit="percent",
            callbacks=[self._get_disk_usage]
        )
        # Process Count
        self.system_metrics["process_count"] = self.meter.create_observable_gauge(
            "mcp.system.process_count",
            description="Number of running MCP processes",
            callbacks=[self._get_process_count]
        )
    def _setup_tool_metrics(self):
        """Set up tool-specific metrics"""
        # Tool Execution Counter
        self.tool_metrics["execution_count"] = self.meter.create_counter(
            "mcp.tool.execution_count",
            description="Number of tool executions",
            unit="calls"
        )
        # Tool Execution Time
        self.tool_metrics["execution_time"] = self.meter.create_histogram(
            "mcp.tool.execution_time",
            description="Tool execution time",
            unit="ms"
        )
        # Tool Error Counter
        self.tool_metrics["error_count"] = self.meter.create_counter(
            "mcp.tool.error_count",
            description="Number of tool execution errors"
        )
        # Active Tools
        self.tool_metrics["active_tools"] = self.meter.create_observable_gauge(
            "mcp.tool.active_count",
            description="Number of currently active tools",
            callbacks=[self._get_active_tools]
        )
    def _get_cpu_usage(self) -> Dict[str, Any]:
        """Get CPU usage callback"""
        return {"": psutil.cpu_percent()}
    def _get_memory_usage(self) -> Dict[str, Any]:
        """Get memory usage callback"""
        return {"": psutil.virtual_memory().percent}
    def _get_disk_usage(self) -> Dict[str, Any]:
        """Get disk usage callback"""
        return {"": psutil.disk_usage("/").percent}
    def _get_process_count(self) -> Dict[str, Any]:
        """Get MCP process count callback"""
        count = len([p for p in psutil.process_iter(["name"]) 
                    if "mcp" in p.info["name"].lower()])
        return {"": count}
    def _get_active_tools(self) -> Dict[str, Any]:
        """Get active tools count callback"""
        # This should be implemented based on your tool tracking mechanism
        return {"": 0}  # Placeholder
    def record_tool_execution(self, tool_name: str, duration_ms: float, 
                            success: bool = True):
        """Record a tool execution"""
        # Record execution count
        self.tool_metrics["execution_count"].add(
            1,
            {"tool": tool_name}
        )
        # Record execution time
        self.tool_metrics["execution_time"].record(
            duration_ms,
            {"tool": tool_name}
        )
        # Record error if failed
        if not success:
            self.tool_metrics["error_count"].add(
                1,
                {"tool": tool_name}
            )
    def get_metric_data(self) -> Dict[str, Any]:
        """Get current metric data for all metrics"""
        return {
            "system": {
                "cpu_usage": psutil.cpu_percent(),
                "memory_usage": psutil.virtual_memory().percent,
                "disk_usage": psutil.disk_usage("/").percent,
                "process_count": len([p for p in psutil.process_iter(["name"]) 
                                   if "mcp" in p.info["name"].lower()])
            },
            "tools": {
                # This should be implemented based on your tool tracking
                "active_count": 0,
                # Add other tool metrics as needed
            }
        }
def create_monitoring(prometheus_port: int = 9464,
                     otlp_endpoint: str = "http://localhost:4318/v1/metrics",
                     export_interval_ms: int = 5000) -> MCPMonitoring:
    """Create and initialize a monitoring instance"""
    monitoring = MCPMonitoring(
        prometheus_port=prometheus_port,
        otlp_endpoint=otlp_endpoint,
        export_interval_ms=export_interval_ms
    )
    monitoring.setup()
    return monitoring
</file>

<file path="monitoring/grafana-dashboard.json">
{
  "annotations": {
    "list": []
  },
  "editable": true,
  "fiscalYearStartMonth": 0,
  "graphTooltip": 0,
  "links": [],
  "liveNow": false,
  "panels": [
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 0
      },
      "id": 1,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "editorMode": "code",
          "expr": "mcp_system_cpu_usage",
          "instant": false,
          "legendFormat": "CPU Usage",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "CPU Usage",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              },
              {
                "color": "red",
                "value": 80
              }
            ]
          },
          "unit": "percent"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 0
      },
      "id": 2,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "editorMode": "code",
          "expr": "mcp_system_memory_usage",
          "instant": false,
          "legendFormat": "Memory Usage",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Memory Usage",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 8
      },
      "id": 3,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "editorMode": "code",
          "expr": "rate(mcp_tool_execution_count[5m])",
          "instant": false,
          "legendFormat": "{{tool}}",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Tool Execution Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          },
          "unit": "ms"
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 8
      },
      "id": 4,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "editorMode": "code",
          "expr": "histogram_quantile(0.95, sum(rate(mcp_tool_execution_time_bucket[5m])) by (tool, le))",
          "instant": false,
          "legendFormat": "{{tool}}",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Tool Execution Time (p95)",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "palette-classic"
          },
          "custom": {
            "axisCenteredZero": false,
            "axisColorMode": "text",
            "axisLabel": "",
            "axisPlacement": "auto",
            "barAlignment": 0,
            "drawStyle": "line",
            "fillOpacity": 20,
            "gradientMode": "none",
            "hideFrom": {
              "legend": false,
              "tooltip": false,
              "viz": false
            },
            "lineInterpolation": "smooth",
            "lineWidth": 2,
            "pointSize": 5,
            "scaleDistribution": {
              "type": "linear"
            },
            "showPoints": "never",
            "spanNulls": true,
            "stacking": {
              "group": "A",
              "mode": "none"
            },
            "thresholdsStyle": {
              "mode": "off"
            }
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 0,
        "y": 16
      },
      "id": 5,
      "options": {
        "legend": {
          "calcs": [],
          "displayMode": "list",
          "placement": "bottom",
          "showLegend": true
        },
        "tooltip": {
          "mode": "single",
          "sort": "none"
        }
      },
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "editorMode": "code",
          "expr": "rate(mcp_tool_error_count[5m])",
          "instant": false,
          "legendFormat": "{{tool}}",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "Tool Error Rate",
      "type": "timeseries"
    },
    {
      "datasource": {
        "type": "prometheus",
        "uid": "prometheus"
      },
      "fieldConfig": {
        "defaults": {
          "color": {
            "mode": "thresholds"
          },
          "mappings": [],
          "thresholds": {
            "mode": "absolute",
            "steps": [
              {
                "color": "green",
                "value": null
              }
            ]
          }
        },
        "overrides": []
      },
      "gridPos": {
        "h": 8,
        "w": 12,
        "x": 12,
        "y": 16
      },
      "id": 6,
      "options": {
        "orientation": "auto",
        "reduceOptions": {
          "calcs": ["lastNotNull"],
          "fields": "",
          "values": false
        },
        "showThresholdLabels": false,
        "showThresholdMarkers": true
      },
      "pluginVersion": "10.2.0",
      "targets": [
        {
          "datasource": {
            "type": "prometheus",
            "uid": "prometheus"
          },
          "editorMode": "code",
          "expr": "mcp_system_process_count",
          "instant": false,
          "legendFormat": "Process Count",
          "range": true,
          "refId": "A"
        }
      ],
      "title": "MCP Process Count",
      "type": "gauge"
    }
  ],
  "refresh": "5s",
  "schemaVersion": 38,
  "style": "dark",
  "tags": ["mcp", "monitoring"],
  "templating": {
    "list": []
  },
  "time": {
    "from": "now-1h",
    "to": "now"
  },
  "timepicker": {},
  "timezone": "",
  "title": "MCP Monitoring Dashboard",
  "version": 1,
  "weekStart": ""
}
</file>

<file path="monitoring/grafana-datasource.yml">
apiVersion: 1
datasources:
  - name: Prometheus
    type: prometheus
    access: proxy
    url: http://prometheus:9090
    isDefault: true
    editable: true
    jsonData:
      timeInterval: "15s"
</file>

<file path="monitoring/otel-collector-config.yaml">
receivers:
  otlp:
    protocols:
      http:
        endpoint: 0.0.0.0:4318
processors:
  batch:
    timeout: 1s
    send_batch_size: 1024
exporters:
  prometheus:
    endpoint: "0.0.0.0:8889"
    namespace: "mcp"
    const_labels:
      service: "mcp-server"
  logging:
    loglevel: debug
service:
  pipelines:
    metrics:
      receivers: [otlp]
      processors: [batch]
      exporters: [prometheus, logging]
</file>

<file path="monitoring/prometheus-rules.yml">
groups:
  - name: mcp_alerts
    rules:
      # System Resource Alerts
      - alert: HighCPUUsage
        expr: mcp_system_cpu_usage > 80
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High CPU usage detected
          description: CPU usage is above 80% for more than 5 minutes
      - alert: HighMemoryUsage
        expr: mcp_system_memory_usage > 85
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High memory usage detected
          description: Memory usage is above 85% for more than 5 minutes
      - alert: HighDiskUsage
        expr: mcp_system_disk_usage > 90
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: High disk usage detected
          description: Disk usage is above 90% for more than 10 minutes
      # Tool Execution Alerts
      - alert: HighToolErrorRate
        expr: rate(mcp_tool_error_count[5m]) > 0.1
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High tool error rate detected
          description: Tool error rate is above 10% in the last 5 minutes
      - alert: SlowToolExecution
        expr: histogram_quantile(0.95, sum(rate(mcp_tool_execution_time_bucket[5m])) by (tool, le)) > 1000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: Slow tool execution detected
          description: 95th percentile of tool execution time is above 1 second
      - alert: HighToolExecutionRate
        expr: sum(rate(mcp_tool_execution_count[1m])) by (tool) > 100
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High tool execution rate detected
          description: Tool is being executed more than 100 times per minute
      # Process Alerts
      - alert: LowProcessCount
        expr: mcp_system_process_count < 1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: No MCP processes running
          description: No MCP processes are currently running
      - alert: HighProcessCount
        expr: mcp_system_process_count > 10
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High number of MCP processes
          description: More than 10 MCP processes are running
      # Resource Exhaustion Predictions
      - alert: MemoryExhaustionPrediction
        expr: predict_linear(mcp_system_memory_usage[1h], 3600) > 95
        for: 15m
        labels:
          severity: warning
        annotations:
          summary: Memory exhaustion predicted
          description: Memory usage is predicted to exceed 95% within the next hour
      - alert: DiskExhaustionPrediction
        expr: predict_linear(mcp_system_disk_usage[6h], 21600) > 95
        for: 30m
        labels:
          severity: warning
        annotations:
          summary: Disk exhaustion predicted
          description: Disk usage is predicted to exceed 95% within the next 6 hours
      # Service Health
      - alert: HighErrorRateByTool
        expr: sum(rate(mcp_tool_error_count[5m])) by (tool) / sum(rate(mcp_tool_execution_count[5m])) by (tool) > 0.05
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: High error rate for specific tool
          description: Error rate is above 5% for {{ $labels.tool }}
</file>

<file path="monitoring/prometheus.yml">
global:
  scrape_interval: 15s
  evaluation_interval: 15s
rule_files:
  - /etc/prometheus/prometheus-rules.yml
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - "alertmanager:9093"
scrape_configs:
  - job_name: "prometheus"
    static_configs:
      - targets: ["localhost:9090"]
  - job_name: "mcp"
    static_configs:
      - targets: ["host.docker.internal:9464"]
    metrics_path: "/metrics"
  - job_name: "otel-collector"
    static_configs:
      - targets: ["otel-collector:8889"]
    metrics_path: "/metrics"
</file>

<file path="scripts/build_languages.py">
"""Script to build tree-sitter language libraries."""
import os
import subprocess
from pathlib import Path
def build_languages():
    """Build tree-sitter language libraries."""
    # Get project root directory
    root_dir = Path(__file__).parent.parent
    # Create vendor directory if it doesn't exist
    vendor_dir = root_dir / 'vendor'
    vendor_dir.mkdir(exist_ok=True)
    # Clone tree-sitter-javascript if it doesn't exist
    js_dir = vendor_dir / 'tree-sitter-javascript'
    if not js_dir.exists():
        subprocess.run(['git', 'clone', 'https://github.com/tree-sitter/tree-sitter-javascript.git', str(js_dir)], check=True)
    # Build the language
    build_dir = js_dir / 'src'
    build_dir.mkdir(exist_ok=True)
    # Build the language library
    os.chdir(str(js_dir))
    subprocess.run(['cc', '-c', '-I.', '-o', 'src/parser.o', 'src/parser.c'], check=True)
    subprocess.run(['cc', '-c', '-I.', '-o', 'src/scanner.o', 'src/scanner.c'], check=True)
    subprocess.run(['cc', '-shared', '-o', 'src/tree-sitter-javascript.so', 'src/parser.o', 'src/scanner.o'], check=True)
if __name__ == '__main__':
    build_languages()
</file>

<file path="server/code_understanding/tests/test_data_generators_test.py">
"""
Tests for the test data generators.
"""
import pytest
from .test_data_generators import (
    TestDataGenerator,
    Language,
    CodeSample
)
def test_generate_simple_python():
    """Test generating simple Python code samples."""
    generator = TestDataGenerator()
    samples = generator.generate_samples(Language.PYTHON, "simple")
    assert len(samples) == 1
    sample = samples[0]
    assert sample.language == Language.PYTHON
    assert sample.complexity == "simple"
    assert "def" in sample.code
    assert len(sample.expected_symbols) >= 3
    assert len(sample.expected_relationships) >= 2
def test_generate_medium_javascript():
    """Test generating medium complexity JavaScript code samples."""
    generator = TestDataGenerator()
    samples = generator.generate_samples(Language.JAVASCRIPT, "medium")
    assert len(samples) == 1
    sample = samples[0]
    assert sample.language == Language.JAVASCRIPT
    assert sample.complexity == "medium"
    assert "class" in sample.code
    assert len(sample.expected_symbols) >= 5
    assert len(sample.expected_relationships) >= 3
def test_generate_complex_swift():
    """Test generating complex Swift code samples."""
    generator = TestDataGenerator()
    samples = generator.generate_samples(Language.SWIFT, "complex")
    assert len(samples) == 1
    sample = samples[0]
    assert sample.language == Language.SWIFT
    assert sample.complexity == "complex"
    assert "protocol" in sample.code
    assert "class" in sample.code
    assert len(sample.expected_symbols) >= 10
    assert len(sample.expected_relationships) >= 5
def test_generate_all_languages():
    """Test generating samples for all languages."""
    generator = TestDataGenerator()
    all_samples = generator.generate_all_samples()
    assert len(all_samples) == 3  # One for each language
    for language in Language:
        assert language in all_samples
        assert len(all_samples[language]) == 3  # One for each complexity level
def test_symbol_consistency():
    """Test that generated symbols match the code."""
    generator = TestDataGenerator()
    samples = generator.generate_all_samples()
    for language, language_samples in samples.items():
        for sample in language_samples:
            # Check that all expected symbols appear in the code
            for symbol in sample.expected_symbols:
                assert symbol["name"] in sample.code
            # Check that all expected relationships have valid sources and targets
            for rel in sample.expected_relationships:
                assert any(s["name"] == rel["source"] for s in sample.expected_symbols)
                assert any(s["name"] == rel["target"] for s in sample.expected_symbols)
def test_language_specific_features():
    """Test that each language's samples include language-specific features."""
    generator = TestDataGenerator()
    samples = generator.generate_all_samples()
    # Python-specific features
    python_samples = samples[Language.PYTHON]
    assert any("def" in s.code for s in python_samples)
    assert any("class" in s.code for s in python_samples)
    assert any("import" in s.code for s in python_samples)
    # JavaScript-specific features
    js_samples = samples[Language.JAVASCRIPT]
    assert any("function" in s.code for s in js_samples)
    assert any("class" in s.code for s in js_samples)
    assert any("import" in s.code for s in js_samples)
    # Swift-specific features
    swift_samples = samples[Language.SWIFT]
    assert any("func" in s.code for s in swift_samples)
    assert any("class" in s.code for s in swift_samples)
    assert any("protocol" in s.code for s in swift_samples)
def test_complexity_levels():
    """Test that complexity levels are properly reflected in the generated code."""
    generator = TestDataGenerator()
    samples = generator.generate_all_samples()
    for language, language_samples in samples.items():
        simple_samples = [s for s in language_samples if s.complexity == "simple"]
        medium_samples = [s for s in language_samples if s.complexity == "medium"]
        complex_samples = [s for s in language_samples if s.complexity == "complex"]
        # Simple samples should be basic functions
        assert all("def" in s.code or "function" in s.code or "func" in s.code 
                  for s in simple_samples)
        # Medium samples should include classes
        assert all("class" in s.code for s in medium_samples)
        # Complex samples should include multiple features
        assert all(any(feature in s.code for feature in ["import", "class", "def", "function", "protocol"])
                  for s in complex_samples)
def test_relationship_types():
    """Test that relationship types are appropriate for each language."""
    generator = TestDataGenerator()
    samples = generator.generate_all_samples()
    for language, language_samples in samples.items():
        for sample in language_samples:
            for rel in sample.expected_relationships:
                # All languages should have these basic relationships
                assert rel["type"] in ["defines", "calls", "imports"]
                # Language-specific relationships
                if language == Language.SWIFT:
                    assert any(rel["type"] == "conforms_to" for rel in sample.expected_relationships)
                elif language == Language.JAVASCRIPT:
                    assert any(rel["type"] == "exports" for rel in sample.expected_relationships)
                elif language == Language.PYTHON:
                    assert any(rel["type"] == "imports" for rel in sample.expected_relationships)
</file>

<file path="server/code_understanding/tests/test_data_generators.py">
"""
Test data generators for code understanding tests.
Provides generators for Python, JavaScript, and Swift code samples.
"""
import random
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from enum import Enum
class Language(Enum):
    """Supported programming languages."""
    PYTHON = "python"
    JAVASCRIPT = "javascript"
    SWIFT = "swift"
@dataclass
class CodeSample:
    """Represents a generated code sample."""
    language: Language
    code: str
    description: str
    expected_symbols: List[Dict[str, Any]]
    expected_relationships: List[Dict[str, Any]]
    complexity: str  # "simple", "medium", "complex"
class CodeGenerator:
    """Base class for code generators."""
    def __init__(self):
        self.var_names = ["x", "y", "z", "count", "result", "value", "data", "item"]
        self.func_names = ["process", "calculate", "transform", "validate", "handle"]
        self.class_names = ["Processor", "Calculator", "Transformer", "Validator"]
    def generate_var_name(self) -> str:
        """Generate a random variable name."""
        return random.choice(self.var_names)
    def generate_func_name(self) -> str:
        """Generate a random function name."""
        return random.choice(self.func_names)
    def generate_class_name(self) -> str:
        """Generate a random class name."""
        return random.choice(self.class_names)
class PythonGenerator(CodeGenerator):
    """Generator for Python code samples."""
    def generate_simple_function(self) -> CodeSample:
        """Generate a simple Python function."""
        func_name = self.generate_func_name()
        var_name = self.generate_var_name()
        code = f"""
def {func_name}({var_name}):
    result = {var_name} * 2
    return result
"""
        return CodeSample(
            language=Language.PYTHON,
            code=code,
            description="Simple function with parameter and return",
            expected_symbols=[
                {"name": func_name, "type": "function", "scope": "module"},
                {"name": var_name, "type": "parameter", "scope": func_name},
                {"name": "result", "type": "variable", "scope": func_name}
            ],
            expected_relationships=[
                {"type": "defines", "source": func_name, "target": var_name},
                {"type": "defines", "source": func_name, "target": "result"}
            ],
            complexity="simple"
        )
    def generate_class_with_methods(self) -> CodeSample:
        """Generate a Python class with methods."""
        class_name = self.generate_class_name()
        method_name = self.generate_func_name()
        var_name = self.generate_var_name()
        code = f"""
class {class_name}:
    def __init__(self, {var_name}):
        self.{var_name} = {var_name}
    def {method_name}(self):
        return self.{var_name} * 2
"""
        return CodeSample(
            language=Language.PYTHON,
            code=code,
            description="Class with constructor and method",
            expected_symbols=[
                {"name": class_name, "type": "class", "scope": "module"},
                {"name": "__init__", "type": "method", "scope": class_name},
                {"name": method_name, "type": "method", "scope": class_name},
                {"name": var_name, "type": "parameter", "scope": "__init__"},
                {"name": f"self.{var_name}", "type": "instance_variable", "scope": class_name}
            ],
            expected_relationships=[
                {"type": "defines", "source": class_name, "target": "__init__"},
                {"type": "defines", "source": class_name, "target": method_name},
                {"type": "defines", "source": "__init__", "target": var_name}
            ],
            complexity="medium"
        )
    def generate_complex_module(self) -> CodeSample:
        """Generate a complex Python module with imports and multiple components."""
        code = """
import os
from typing import List, Dict
class DataProcessor:
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.data: List[int] = []
    def process(self, input_data: List[int]) -> List[int]:
        result = []
        for item in input_data:
            if item > 0:
                result.append(item * 2)
        return result
def main():
    processor = DataProcessor({"threshold": 0})
    data = [1, 2, 3, -1, -2]
    result = processor.process(data)
    print(result)
if __name__ == "__main__":
    main()
"""
        return CodeSample(
            language=Language.PYTHON,
            code=code,
            description="Complex module with imports, class, and main function",
            expected_symbols=[
                {"name": "os", "type": "module", "scope": "module"},
                {"name": "List", "type": "type", "scope": "module"},
                {"name": "Dict", "type": "type", "scope": "module"},
                {"name": "DataProcessor", "type": "class", "scope": "module"},
                {"name": "main", "type": "function", "scope": "module"},
                {"name": "config", "type": "parameter", "scope": "__init__"},
                {"name": "data", "type": "instance_variable", "scope": "DataProcessor"},
                {"name": "process", "type": "method", "scope": "DataProcessor"},
                {"name": "input_data", "type": "parameter", "scope": "process"},
                {"name": "result", "type": "variable", "scope": "process"},
                {"name": "item", "type": "variable", "scope": "process"},
                {"name": "processor", "type": "variable", "scope": "main"},
                {"name": "data", "type": "variable", "scope": "main"},
                {"name": "result", "type": "variable", "scope": "main"}
            ],
            expected_relationships=[
                {"type": "imports", "source": "module", "target": "os"},
                {"type": "imports", "source": "module", "target": "List"},
                {"type": "imports", "source": "module", "target": "Dict"},
                {"type": "defines", "source": "module", "target": "DataProcessor"},
                {"type": "defines", "source": "module", "target": "main"},
                {"type": "defines", "source": "DataProcessor", "target": "process"},
                {"type": "calls", "source": "main", "target": "DataProcessor"},
                {"type": "calls", "source": "main", "target": "process"}
            ],
            complexity="complex"
        )
class JavaScriptGenerator(CodeGenerator):
    """Generator for JavaScript code samples."""
    def generate_simple_function(self) -> CodeSample:
        """Generate a simple JavaScript function."""
        func_name = self.generate_func_name()
        var_name = self.generate_var_name()
        code = f"""
function {func_name}({var_name}) {{
    const result = {var_name} * 2;
    return result;
}}
"""
        return CodeSample(
            language=Language.JAVASCRIPT,
            code=code,
            description="Simple function with parameter and return",
            expected_symbols=[
                {"name": func_name, "type": "function", "scope": "module"},
                {"name": var_name, "type": "parameter", "scope": func_name},
                {"name": "result", "type": "variable", "scope": func_name}
            ],
            expected_relationships=[
                {"type": "defines", "source": func_name, "target": var_name},
                {"type": "defines", "source": func_name, "target": "result"}
            ],
            complexity="simple"
        )
    def generate_class_with_methods(self) -> CodeSample:
        """Generate a JavaScript class with methods."""
        class_name = self.generate_class_name()
        method_name = self.generate_func_name()
        var_name = self.generate_var_name()
        code = f"""
class {class_name} {{
    constructor({var_name}) {{
        this.{var_name} = {var_name};
    }}
    {method_name}() {{
        return this.{var_name} * 2;
    }}
}}
"""
        return CodeSample(
            language=Language.JAVASCRIPT,
            code=code,
            description="Class with constructor and method",
            expected_symbols=[
                {"name": class_name, "type": "class", "scope": "module"},
                {"name": "constructor", "type": "method", "scope": class_name},
                {"name": method_name, "type": "method", "scope": class_name},
                {"name": var_name, "type": "parameter", "scope": "constructor"},
                {"name": f"this.{var_name}", "type": "instance_variable", "scope": class_name}
            ],
            expected_relationships=[
                {"type": "defines", "source": class_name, "target": "constructor"},
                {"type": "defines", "source": class_name, "target": method_name},
                {"type": "defines", "source": "constructor", "target": var_name}
            ],
            complexity="medium"
        )
    def generate_complex_module(self) -> CodeSample:
        """Generate a complex JavaScript module with imports and multiple components."""
        code = """
import { process } from './utils.js';
class DataProcessor {
    constructor(config) {
        this.config = config;
        this.data = [];
    }
    async process(inputData) {
        const result = [];
        for (const item of inputData) {
            if (item > 0) {
                result.push(await process(item));
            }
        }
        return result;
    }
}
export const main = async () => {
    const processor = new DataProcessor({ threshold: 0 });
    const data = [1, 2, 3, -1, -2];
    const result = await processor.process(data);
    console.log(result);
};
if (import.meta.main) {
    main();
}
"""
        return CodeSample(
            language=Language.JAVASCRIPT,
            code=code,
            description="Complex module with imports, class, and async functions",
            expected_symbols=[
                {"name": "process", "type": "import", "scope": "module"},
                {"name": "DataProcessor", "type": "class", "scope": "module"},
                {"name": "main", "type": "function", "scope": "module"},
                {"name": "config", "type": "parameter", "scope": "constructor"},
                {"name": "data", "type": "instance_variable", "scope": "DataProcessor"},
                {"name": "process", "type": "method", "scope": "DataProcessor"},
                {"name": "inputData", "type": "parameter", "scope": "process"},
                {"name": "result", "type": "variable", "scope": "process"},
                {"name": "item", "type": "variable", "scope": "process"},
                {"name": "processor", "type": "variable", "scope": "main"},
                {"name": "data", "type": "variable", "scope": "main"},
                {"name": "result", "type": "variable", "scope": "main"}
            ],
            expected_relationships=[
                {"type": "imports", "source": "module", "target": "process"},
                {"type": "defines", "source": "module", "target": "DataProcessor"},
                {"type": "defines", "source": "module", "target": "main"},
                {"type": "defines", "source": "DataProcessor", "target": "process"},
                {"type": "calls", "source": "main", "target": "DataProcessor"},
                {"type": "calls", "source": "main", "target": "process"}
            ],
            complexity="complex"
        )
class SwiftGenerator(CodeGenerator):
    """Generator for Swift code samples."""
    def generate_simple_function(self) -> CodeSample:
        """Generate a simple Swift function."""
        func_name = self.generate_func_name()
        var_name = self.generate_var_name()
        code = f"""
func {func_name}(_ {var_name}: Int) -> Int {{
    let result = {var_name} * 2
    return result
}}
"""
        return CodeSample(
            language=Language.SWIFT,
            code=code,
            description="Simple function with parameter and return",
            expected_symbols=[
                {"name": func_name, "type": "function", "scope": "module"},
                {"name": var_name, "type": "parameter", "scope": func_name},
                {"name": "result", "type": "variable", "scope": func_name}
            ],
            expected_relationships=[
                {"type": "defines", "source": func_name, "target": var_name},
                {"type": "defines", "source": func_name, "target": "result"}
            ],
            complexity="simple"
        )
    def generate_class_with_methods(self) -> CodeSample:
        """Generate a Swift class with methods."""
        class_name = self.generate_class_name()
        method_name = self.generate_func_name()
        var_name = self.generate_var_name()
        code = f"""
class {class_name} {{
    private var {var_name}: Int
    init({var_name}: Int) {{
        self.{var_name} = {var_name}
    }}
    func {method_name}() -> Int {{
        return {var_name} * 2
    }}
}}
"""
        return CodeSample(
            language=Language.SWIFT,
            code=code,
            description="Class with initializer and method",
            expected_symbols=[
                {"name": class_name, "type": "class", "scope": "module"},
                {"name": "init", "type": "initializer", "scope": class_name},
                {"name": method_name, "type": "method", "scope": class_name},
                {"name": var_name, "type": "parameter", "scope": "init"},
                {"name": f"self.{var_name}", "type": "instance_variable", "scope": class_name}
            ],
            expected_relationships=[
                {"type": "defines", "source": class_name, "target": "init"},
                {"type": "defines", "source": class_name, "target": method_name},
                {"type": "defines", "source": "init", "target": var_name}
            ],
            complexity="medium"
        )
    def generate_complex_module(self) -> CodeSample:
        """Generate a complex Swift module with imports and multiple components."""
        code = """
import Foundation
protocol DataProcessable {
    func process(_ data: [Int]) async throws -> [Int]
}
class DataProcessor: DataProcessable {
    private let config: [String: Any]
    private var data: [Int] = []
    init(config: [String: Any]) {
        self.config = config
    }
    func process(_ inputData: [Int]) async throws -> [Int] {
        var result: [Int] = []
        for item in inputData where item > 0 {
            result.append(try await processItem(item))
        }
        return result
    }
    private func processItem(_ item: Int) async throws -> Int {
        return item * 2
    }
}
@main
struct Main {
    static func main() async throws {
        let processor = DataProcessor(config: ["threshold": 0])
        let data = [1, 2, 3, -1, -2]
        let result = try await processor.process(data)
        print(result)
    }
}
"""
        return CodeSample(
            language=Language.SWIFT,
            code=code,
            description="Complex module with protocol, class, and async functions",
            expected_symbols=[
                {"name": "Foundation", "type": "module", "scope": "module"},
                {"name": "DataProcessable", "type": "protocol", "scope": "module"},
                {"name": "DataProcessor", "type": "class", "scope": "module"},
                {"name": "Main", "type": "struct", "scope": "module"},
                {"name": "config", "type": "instance_variable", "scope": "DataProcessor"},
                {"name": "data", "type": "instance_variable", "scope": "DataProcessor"},
                {"name": "process", "type": "method", "scope": "DataProcessor"},
                {"name": "processItem", "type": "method", "scope": "DataProcessor"},
                {"name": "inputData", "type": "parameter", "scope": "process"},
                {"name": "result", "type": "variable", "scope": "process"},
                {"name": "item", "type": "variable", "scope": "process"},
                {"name": "processor", "type": "variable", "scope": "main"},
                {"name": "data", "type": "variable", "scope": "main"},
                {"name": "result", "type": "variable", "scope": "main"}
            ],
            expected_relationships=[
                {"type": "imports", "source": "module", "target": "Foundation"},
                {"type": "defines", "source": "module", "target": "DataProcessable"},
                {"type": "defines", "source": "module", "target": "DataProcessor"},
                {"type": "defines", "source": "module", "target": "Main"},
                {"type": "conforms_to", "source": "DataProcessor", "target": "DataProcessable"},
                {"type": "defines", "source": "DataProcessor", "target": "process"},
                {"type": "defines", "source": "DataProcessor", "target": "processItem"},
                {"type": "calls", "source": "process", "target": "processItem"},
                {"type": "calls", "source": "main", "target": "DataProcessor"},
                {"type": "calls", "source": "main", "target": "process"}
            ],
            complexity="complex"
        )
class TestDataGenerator:
    """Main class for generating test data across all languages."""
    def __init__(self):
        self.python_generator = PythonGenerator()
        self.javascript_generator = JavaScriptGenerator()
        self.swift_generator = SwiftGenerator()
    def generate_samples(self, language: Language, complexity: Optional[str] = None) -> List[CodeSample]:
        """Generate test samples for a specific language and complexity level."""
        generator = self._get_generator(language)
        samples = []
        if complexity is None or complexity == "simple":
            samples.append(generator.generate_simple_function())
        if complexity is None or complexity == "medium":
            samples.append(generator.generate_class_with_methods())
        if complexity is None or complexity == "complex":
            samples.append(generator.generate_complex_module())
        return samples
    def _get_generator(self, language: Language) -> CodeGenerator:
        """Get the appropriate generator for a language."""
        generators = {
            Language.PYTHON: self.python_generator,
            Language.JAVASCRIPT: self.javascript_generator,
            Language.SWIFT: self.swift_generator
        }
        return generators[language]
    def generate_all_samples(self, complexity: Optional[str] = None) -> Dict[Language, List[CodeSample]]:
        """Generate test samples for all languages."""
        return {
            language: self.generate_samples(language, complexity)
            for language in Language
        }
</file>

<file path="server/code_understanding/__init__.py">
"""Code understanding module for analyzing source code."""
import os
import logging
from .parser import CodeParser
from .analyzer import CodeAnalyzer
from .extractor import SymbolExtractor
from .build_languages import build_languages
logger = logging.getLogger(__name__)
# Build language library on import
try:
    LANGUAGE_LIB_PATH = build_languages()
    logger.info(f"Language library built at {LANGUAGE_LIB_PATH}")
except Exception as e:
    logger.error(f"Failed to build language library: {e}")
    LANGUAGE_LIB_PATH = None
__all__ = ['CodeParser', 'CodeAnalyzer', 'SymbolExtractor']
</file>

<file path="server/code_understanding/analyzer.py">
"""Module for analyzing Python code."""
import ast
import logging
from typing import Dict, List, Optional, Set, Any, Union
import os
from .parser import CodeParser
# Import common types
from .common_types import MockNode, MockTree
logger = logging.getLogger(__name__)
class CodeAnalyzer:
    """Analyzer for Python code."""
    def __init__(self):
        """Initialize the analyzer."""
        self.parser = CodeParser()
        self.reset_state()
    def reset_state(self):
        self.imports: List[Dict[str, Any]] = []
        self.functions: List[Dict[str, Any]] = []
        self.classes: List[Dict[str, Any]] = []
        self.variables: List[Dict[str, Any]] = []
        # Add other state variables as needed
    def analyze_code(self, code: str, language: str = 'python') -> Dict[str, List[Dict[str, Any]]]:
        """Analyzes code string using the appropriate parser adapter."""
        tree = self.parser.parse(code, language=language)
        if not tree or not tree.root_node:
            logger.error(f"Parsing failed for language '{language}'. Cannot analyze.")
            return {'imports': [], 'functions': [], 'classes': [], 'variables': []}
        self.reset_state()
        logger.info(f"Root node type: {tree.type} for language {language}")
        self._analyze_node(tree, language)
        return {
            'imports': self.imports,
            'functions': self.functions,
            'classes': self.classes,
            'variables': self.variables
        }
    def _analyze_node(self, node: Union[MockNode, MockTree], parent_type: str = None, language: str = 'python', parent: Optional[MockNode] = None) -> None:
        """Analyze a single AST node and update the analysis results.
        Args:
            node: The AST node to analyze.
            parent_type: The type of the parent node, if any.
            language: The programming language being analyzed.
            parent: The parent node, if any.
        """
        if not node:
            return
        # If node is a MockTree, use its root_node
        if isinstance(node, MockTree):
            if node.root_node:
                self._analyze_node(node.root_node, parent_type, language, parent)
            return
        # Process imports and requires
        if node.type == 'import_statement':
            logger.debug(f"Processing JS import_statement: {node.text}")
            import_info = self._extract_js_es6_import(node)
            if import_info:
                self.imports.append(import_info)
        elif node.type == 'call_expression':
            # Check for require statements
            require_info = self._extract_js_require(node)
            if require_info:
                self.imports.append(require_info)
        elif node.type == 'variable_declarator':
            # Check for require statements in variable declarations
            require_info = self._extract_js_require(node)
            if require_info:
                self.imports.append(require_info)
                return  # Skip further processing of this node
            # If not a require, check if it's a function assignment
            name = None
            value = None
            is_function = False
            for child in node.children:
                if child.type == 'identifier':
                    name = child.text
                elif child.type in ('arrow_function', 'function_definition'):
                    value = child
                    is_function = True
                    break
            if value and name and is_function:
                if isinstance(value, MockNode) and value.type == 'arrow_function':
                    logger.debug(f"Processing arrow function assignment: name='{name}'")
                    func_info = self._extract_function(value)
                    if func_info['name']:
                        func_info['name'] = name  # Use the variable name as the function name
                        self.functions.append(func_info)
            elif name and not is_function:  # Only add as variable if it's not a function or require
                self.variables.append({
                    'name': name,
                    'type': 'variable',
                    'start_line': node.start_point[0] + 1 if node.start_point else 0,
                    'end_line': node.end_point[0] + 1 if node.end_point else 0
                })
        elif node.type == 'lexical_declaration':
            # Process variable declarations, including requires and arrow functions
            logger.debug(f"Processing lexical_declaration node: {node.text}")
            for child in node.children:
                if child.type == 'variable_declarator':
                    logger.debug(f"Processing variable_declarator child: {child.text}")
                    # First check if it's a require statement
                    require_info = self._extract_js_require(child)
                    if require_info:
                        logger.debug(f"Found require statement: {require_info}")
                        self.imports.append(require_info)
                        continue
                    # If not a require, process as normal variable declarator
                    self._analyze_node(child, node.type, language, node)
        # Process functions
        if node.type in ('function_definition', 'function_declaration'):
            # Only process top-level functions
            if not parent_type or parent_type == 'program':
                func_info = self._extract_function(node)
                if func_info['name']:
                    self.functions.append(func_info)
        elif node.type == 'function_declaration':
            # Handle function declarations
            name = ''
            for child in node.children:
                if child.type == 'identifier':
                    name = child.text
                    break
            logger.debug(f"Processing function_declaration node: name='{name}', effective_top_level=True (parent_type={parent.type if parent else 'None'})")
            func_info = self._extract_function(node)
            if func_info['name']:
                self.functions.append(func_info)
        elif node.type == 'function_definition':
            # Handle function definitions (including class methods)
            name = ''
            for child in node.children:
                if child.type == 'name':
                    name = child.text
                    break
            effective_top_level = parent is None or parent.type == 'program'
            if parent and parent.type == 'variable_declaration':
                effective_top_level = True
            logger.debug(f"Processing function_definition node: name='{name}', effective_top_level={effective_top_level} (parent_type={parent.type if parent else 'None'})")
            if effective_top_level:
                func_info = self._extract_function(node)
                if func_info['name']:
                    self.functions.append(func_info)
            else:
                logger.debug(f"Skipping function (not top-level, parent={parent.type if parent else 'None'})")
        elif node.type == 'class_declaration':
            # Handle class declarations
            class_info = self._extract_class(node)
            if class_info['name']:
                self.classes.append(class_info)
        # Process children in pre-order traversal
        for child in node.children:
            self._analyze_node(child, node.type, language, node)
    # --- JavaScript Helper Methods ---
    def _extract_js_es6_import(self, node: MockNode):
        """Extracts ES6 import details from an import_statement MockNode."""
        # ---- Add extra logging ----
        logger.debug(f"_extract_js_es6_import called for node: type={node.type}, text='{node.text}'")
        logger.debug(f"Node fields: {node.fields}")
        # ---- End extra logging ----
        module = node.fields.get('module')
        default_import = node.fields.get('default_name')
        named_imports = node.fields.get('named_names', [])
        start_line = node.start_point[0] + 1 if node.start_point else 0
        end_line = node.end_point[0] + 1 if node.end_point else 0
        if not module:
            logger.warning(f"JS import statement node missing 'module' field: {node.text}")
            return
        # Add default import if present
        if default_import:
            self.imports.append({
                'type': 'import',
                'name': default_import,
                'module': module,
                'is_default': True,
                'start_line': start_line,
                'end_line': end_line
            })
            logger.debug(f"Extracted JS default import: {default_import} from {module}")
        # Add named imports if present
        for named_import in named_imports:
            self.imports.append({
                'type': 'import',
                'name': named_import,
                'module': module,
                'is_default': False,
                'start_line': start_line,
                'end_line': end_line
            })
            logger.debug(f"Extracted JS named import: {named_import} from {module}")
        # Handle namespace imports if present
        namespace_import = node.fields.get('namespace_name')
        if namespace_import:
            self.imports.append({
                'type': 'import',
                'name': namespace_import,
                'module': module,
                'is_namespace': True,
                'start_line': start_line,
                'end_line': end_line
            })
            logger.debug(f"Extracted JS namespace import: {namespace_import} from {module}")
    def _extract_js_require(self, node: MockNode) -> Optional[Dict[str, Any]]:
        """Extract information about a JavaScript require statement.
        Args:
            node: The AST node representing a require statement.
        Returns:
            A dictionary containing information about the require statement, or None if not a require.
        """
        if not node:
            return None
        logger.debug(f"_extract_js_require called for node: type={node.type}, text='{node.text}'")
        # Handle variable declarator nodes
        if node.type == 'variable_declarator':
            # Check if this is a require statement
            if node.fields.get('type') == 'require':
                return {
                    'type': 'require',
                    'name': node.fields.get('name'),
                    'module': node.fields.get('module'),
                    'start_line': node.start_point[0] + 1 if node.start_point else 0,
                    'end_line': node.end_point[0] + 1 if node.end_point else 0
                }
            # Look for a call_expression child that might be a require
            name = None
            name_node = node.children[0] if node.children else None
            if name_node and name_node.type == 'identifier':
                name = name_node.text
                logger.debug(f"Found variable name: {name}")
            # Look for the require call in the initializer
            for child in node.children:
                if child.type == 'call_expression':
                    logger.debug(f"Found call_expression child: {child.text}")
                    # Check if this is a require call
                    callee = None
                    module = None
                    for call_child in child.children:
                        logger.debug(f"Processing call_child: type={call_child.type}, text='{call_child.text}'")
                        if call_child.type == 'identifier' and call_child.text == 'require':
                            callee = call_child
                        elif call_child.type == 'string':
                            module = call_child.text.strip('"\'')
                    if callee and module:
                        logger.debug(f"Found require call: callee={callee.text}, module={module}")
                        return {
                            'type': 'require',
                            'name': name or module.split('/')[-1],
                            'module': module,
                            'start_line': node.start_point[0] + 1 if node.start_point else 0,
                            'end_line': node.end_point[0] + 1 if node.end_point else 0
                        }
            return None
        # Handle direct call_expression nodes
        if node.type != 'call_expression':
            return None
        logger.debug(f"Processing call_expression node: {node.text}")
        # Check if this is a require call
        callee = None
        module = None
        for child in node.children:
            logger.debug(f"Processing child: type={child.type}, text='{child.text}'")
            if child.type == 'identifier' and child.text == 'require':
                callee = child
            elif child.type == 'string':
                module = child.text.strip('"\'')
        if not callee or not module:
            return None
        # For direct require calls without assignment, use the last part of the path as the name
        name = module.split('/')[-1]
        logger.debug(f"Found require call: callee={callee.text}, module={module}")
        return {
            'type': 'require',
            'name': name,
            'module': module,
            'start_line': node.start_point[0] + 1 if node.start_point else 0,
            'end_line': node.end_point[0] + 1 if node.end_point else 0
        }
    # --- Common Helper Methods (unchanged from previous state) ---
    def _extract_function(self, node: MockNode) -> Dict[str, Any]:
        """Extract information about a function or method definition.
        Args:
            node: The AST node representing the function or method definition.
        Returns:
            dict: A dictionary containing the function name, start and end line numbers,
                  parameters, decorators, and async status.
        """
        if not node:
            return {
                'name': '',
                'start_line': 0,
                'end_line': 0,
                'parameters': [],
                'decorators': [],
                'is_async': False,
                'is_static': False
            }
        # Extract function name
        name = ''
        if node.type == 'method_definition':
            # For method definitions, get name from the name field
            name_node = node.fields.get('name')
            if name_node and hasattr(name_node, 'text'):
                name = name_node.text
            # Check if method is static
            is_static = any(child.type == 'static' for child in node.children)
        else:
            # For regular function definitions, get name from children
            for child in node.children:
                if child.type == 'name':
                    name = child.text
                    break
        # Extract decorators
        decorators = []
        for child in node.children:
            if child.type == 'decorator':
                decorators.append(child.text)
        # Extract parameters
        params_node = None
        for child in node.children:
            if child.type == 'parameters':
                params_node = child
                break
        parameters = self._extract_parameters(params_node) if params_node else []
        # Check if function is async
        is_async = any(child.type == 'async' for child in node.children)
        # Get line numbers - end_point[0] is already 0-based
        start_line = node.start_point[0] + 1 if node.start_point else 0
        end_line = node.end_point[0] if node.end_point else 0
        # Log method extraction details
        logger.debug(f"Extracting {'method' if node.type == 'method_definition' else 'function'} '{name}'")
        if node.type == 'method_definition':
            logger.debug(f"  Method is static: {is_static}")
        return {
            'name': name,
            'start_line': start_line,
            'end_line': end_line,
            'parameters': parameters,
            'decorators': decorators,
            'is_async': is_async,
            'is_static': is_static if node.type == 'method_definition' else False
        }
    def _extract_class(self, node: MockNode) -> Dict[str, Any]:
        """Extract information about a class definition.
        Args:
            node: The AST node representing the class definition.
        Returns:
            dict: A dictionary containing the class name, base classes, methods, and attributes.
        """
        if not node:
            return {
                'name': '',
                'base_classes': [],
                'methods': [],
                'attributes': []
            }
        # Extract class name
        name = ''
        for child in node.children:
            if child.type == 'name':
                name = child.text
                break
        # Extract base classes
        base_classes = []
        for child in node.children:
            if child.type == 'extends_clause':
                for base in child.children:
                    if base.type == 'name':
                        base_classes.append(base.text)
        # Extract methods and attributes
        methods = []
        attributes = []
        # Get the class body - check both body and class_body fields
        body_node = node.fields.get('body') or node.fields.get('class_body')
        if body_node:
            for child in body_node.children:
                if child.type in ('function_definition', 'method_definition'):
                    method_info = self._extract_function(child)
                    if method_info['name']:
                        methods.append(method_info)
                elif child.type == 'field_definition':
                    # Handle class fields/attributes
                    for field_child in child.children:
                        if field_child.type == 'name':
                            attributes.append({
                                'name': field_child.text,
                                'is_static': any(c.type == 'static' for c in child.children)
                            })
        # Log class extraction details
        logger.debug(f"Extracting class '{name}'")
        logger.debug(f"  Base classes: {base_classes}")
        logger.debug(f"  Methods: {[m['name'] for m in methods]}")
        logger.debug(f"  Attributes: {[a['name'] for a in attributes]}")
        return {
            'name': name,
            'base_classes': base_classes,
            'methods': methods,
            'attributes': attributes
        }
    def _extract_parameters(self, node: MockNode) -> List[Dict[str, Any]]:
        """Extract information about function parameters.
        Args:
            node: The parameters node.
        Returns:
            List of dictionaries containing parameter information.
        """
        if not node:
            return []
        parameters = []
        for child in node.children:
            param_info = {
                'name': '',
                'type': None,
                'default': None,
                'start_line': child.start_point[0] + 1 if child.start_point else 0,
                'end_line': child.end_point[0] + 1 if child.end_point else 0
            }
            if child.type == 'identifier':
                param_info['name'] = child.text
                param_info['type'] = 'parameter'
            elif child.type == 'typed_parameter':
                name_node = next((c for c in child.children if c.type == 'name'), None)
                type_node = next((c for c in child.children if c.type == 'type'), None)
                if name_node:
                    param_info['name'] = name_node.text
                if type_node:
                    param_info['type'] = type_node.text
            elif child.type == 'list_splat_pattern':
                name_node = next((c for c in child.children if c.type == 'name'), None)
                if name_node:
                    param_info['name'] = f"*{name_node.text}"
                    param_info['type'] = 'parameter'
            if param_info['name']:
                parameters.append(param_info)
        return parameters
    def _extract_functions(self, root):
        """Extract function definitions from a node.
        Args:
            root: Root node to extract functions from
        Returns:
            List of dictionaries containing function information
        """
        functions = []
        for node in root.children:
            if node.type == 'function_definition':
                functions.append({
                    'name': node.fields["name"].text,
                    'start_line': node.start_point[0] + 1,
                    'end_line': node.end_point[0] + 1,
                    'parameters': []
                })
        return functions
    def _infer_type(self, node: Any) -> str:
        """Infer type from value node."""
        if node is None:
            return 'unknown'
        type_map = {
            'string': 'str',
            'integer': 'int',
            'float': 'float',
            'true': 'bool',
            'false': 'bool',
            'none': 'None',
            'list': 'list',
            'dictionary': 'dict',
            'tuple': 'tuple'
        }
        return type_map.get(node.type, 'unknown')
    def analyze_file(self, file_path: str) -> Dict[str, List[Dict[str, Any]]]:
        """Analyze a Python file and extract information about its contents.
        Args:
            file_path: Path to the Python file to analyze.
        Returns:
            Dictionary containing analysis results.
        Raises:
            FileNotFoundError: If the file does not exist.
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File not found: {file_path}")
        try:
            with open(file_path, 'r') as f:
                code = f.read()
            return self.analyze_code(code)
        except Exception as e:
            logger.error(f"Failed to analyze file {file_path}: {e}")
            raise
    def analyze_directory(self, directory_path):
        """Analyze all Python files in a directory.
        Args:
            directory_path (str): Path to the directory to analyze.
        Returns:
            list: A list of dictionaries, each containing analysis results for a file:
                - file (str): The file path
                - imports (list): List of import information
                - functions (list): List of function information
                - classes (list): List of class information
                - variables (list): List of variable information
        Raises:
            FileNotFoundError: If the directory does not exist.
        """
        if not os.path.exists(directory_path):
            raise FileNotFoundError(f"Directory not found: {directory_path}")
        if not os.path.isdir(directory_path):
            raise NotADirectoryError(f"Path is not a directory: {directory_path}")
        results = []
        for root, _, files in os.walk(directory_path):
            for file in files:
                if file.endswith('.py'):
                    file_path = os.path.join(root, file)
                    try:
                        analysis = self.analyze_file(file_path)
                        analysis['file'] = file_path
                        results.append(analysis)
                    except Exception as e:
                        logger.error(f"Error analyzing file {file_path}: {str(e)}")
        return results
    def _extract_imports(self, root):
        """Extract import statements from a node.
        Args:
            root: Root node to extract imports from
        Returns:
            List of dictionaries containing import information
        """
        imports = []
        for node in root.children:
            if node.type == 'import':
                imports.append({
                    'type': 'import',
                    'name': node.text,
                    'start_line': node.start_point[0] + 1,
                    'end_line': node.end_point[0] + 1
                })
        return imports
    def _extract_classes(self, root):
        """Extract class definitions from a node.
        Args:
            root: Root node to extract classes from
        Returns:
            List of dictionaries containing class information
        """
        classes = []
        for node in root.children:
            if node.type == 'class_definition':
                classes.append({
                    'name': node.fields["name"].text,
                    'start_line': node.start_point[0] + 1,
                    'end_line': node.end_point[0] + 1,
                    'methods': [],
                    'bases': []
                })
        return classes
    def _extract_variables(self, root):
        """Extract variable assignments from a node.
        Args:
            root: Root node to extract variables from
        Returns:
            List of dictionaries containing variable information
        """
        variables = []
        for node in root.children:
            if node.type == 'assignment':
                right_text = node.fields["right"].text
                inferred_type = "str" if ((right_text.startswith("'") and right_text.endswith("'")) or (right_text.startswith('"') and right_text.endswith('"'))) else "unknown"
                variables.append({
                    'name': node.fields["left"].text,
                    'start_line': node.start_point[0] + 1,
                    'end_line': node.end_point[0] + 1,
                    'type': inferred_type
                })
        return variables
</file>

<file path="server/code_understanding/build_languages.py">
"""Script to build tree-sitter language libraries."""
import os
import logging
from tree_sitter import Language
logger = logging.getLogger(__name__)
def build_languages():
    """Build tree-sitter language libraries."""
    try:
        # Get the absolute path to the vendor directory
        current_dir = os.path.dirname(os.path.abspath(__file__))
        vendor_dir = os.path.join(os.path.dirname(os.path.dirname(current_dir)), 'vendor')
        # Python language path
        python_path = os.path.join(vendor_dir, 'tree-sitter-python')
        # Create build directory if it doesn't exist
        build_dir = os.path.join(current_dir, 'build')
        os.makedirs(build_dir, exist_ok=True)
        # Build language library
        library_path = os.path.join(build_dir, 'languages.so')
        Language.build_library(
            library_path,
            [python_path]
        )
        logger.info(f"Successfully built language library at {library_path}")
        return library_path
    except Exception as e:
        logger.error(f"Failed to build language library: {e}")
        raise
if __name__ == '__main__':
    logging.basicConfig(level=logging.INFO)
    build_languages()
</file>

<file path="server/code_understanding/common_types.py">
"""Common data structures for code understanding."""
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Any, Iterator
import logging
logger = logging.getLogger(__name__)
# Moved from parser.py to break circular import
@dataclass
class MockNode:
    """Mock AST node for testing and unified representation."""
    type: str
    text: str = ""
    children: List["MockNode"] = field(default_factory=list)
    start_point: Tuple[int, int] = (0, 0)
    end_point: Tuple[int, int] = (0, 0)
    parent: Optional['MockNode'] = None # Optional parent link
    fields: Dict[str, Any] = field(default_factory=dict) # For named children/attributes
    def __post_init__(self):
        """Initialize optional fields."""
        # Ensure children/fields are mutable defaults
        if self.children is None:
            self.children = []
        if self.fields is None:
            self.fields = {}
    def children_by_field_name(self, field_name: str) -> List["MockNode"]:
        """Get children associated with a specific field name."""
        # This might need adjustment based on how fields vs children are used
        field_value = self.fields.get(field_name)
        if isinstance(field_value, list):
            return field_value
        elif isinstance(field_value, MockNode):
            return [field_value]
        return []
        # # Alternative: check children based on a hypothetical 'field' attr?
        # return [child for child in self.children if getattr(child, 'field', None) == field_name]
    def child_by_field_name(self, field_name: str) -> Optional["MockNode"]:
        """Get a single child node associated with a field name."""
        field_value = self.fields.get(field_name)
        if isinstance(field_value, MockNode):
             return field_value
        elif isinstance(field_value, list) and field_value:
             # Return first element if it's a list?
             # Or should this only return non-list fields? Decide based on usage.
             if isinstance(field_value[0], MockNode):
                  return field_value[0]
        return None
    def walk(self) -> Iterator["MockNode"]:
        """Walk through the node and its children (depth-first)."""
        yield self
        for child in self.children:
            yield from child.walk()
# Moved from parser.py to break circular import
class MockTree:
    """Mock tree structure holding the root MockNode."""
    def __init__(self, root: Optional[MockNode] = None):
        """Initialize mock tree.
        Args:
            root: Root MockNode
        """
        self.root_node = root
        self.type = 'mock_tree' # Keep a type identifier if useful
    def walk(self) -> Iterator[MockNode]:
        """Walk through all nodes in the tree (depth-first)."""
        if self.root_node:
            return self.root_node.walk()
        else:
            return iter([]) # Return empty iterator if no root
</file>

<file path="server/code_understanding/context_mapper.py">
"""Module for mapping and tracking code contexts and relationships."""
from typing import Dict, List, Set, Any, Optional, Tuple
from pathlib import Path
from .semantic_analyzer import SemanticAnalyzer, Type, Scope
from .module_resolver import ModuleResolver
class ContextMapper:
    """Maps and tracks relationships between code contexts."""
    def __init__(self, root_dir: str):
        """Initialize the context mapper.
        Args:
            root_dir: Root directory of the project
        """
        self.root_dir = Path(root_dir)
        self.semantic_analyzer = SemanticAnalyzer()
        self.module_resolver = ModuleResolver(root_dir)
        self.context_map: Dict[str, Dict[str, Any]] = {}
        self.relationship_map: Dict[str, List[Dict[str, Any]]] = {}
    def analyze_file(self, file_path: str, content: str) -> Dict[str, Any]:
        """Analyze a file and build its context map.
        Args:
            file_path: Path to the file
            content: File contents
        Returns:
            Dictionary containing analysis results
        """
        # Get semantic analysis
        semantic_result = self.semantic_analyzer.analyze_file(file_path, content)
        # Get module dependencies
        module_deps = self.module_resolver.get_module_dependencies(file_path)
        # Build context map
        context_info = self._build_context_info(file_path, semantic_result, module_deps)
        # Store results
        self.context_map[file_path] = context_info
        # Update relationships
        self._update_relationships(file_path, context_info)
        return context_info
    def _build_context_info(self, file_path: str, semantic_result: Dict[str, Any],
                          module_deps: Dict[str, List[str]]) -> Dict[str, Any]:
        """Build context information for a file.
        Args:
            file_path: Path to the file
            semantic_result: Results from semantic analysis
            module_deps: Module dependencies
        Returns:
            Dictionary containing context information
        """
        return {
            'file_path': file_path,
            'types': semantic_result['types'],
            'contexts': semantic_result['contexts'],
            'dependencies': module_deps,
            'relationships': self._build_relationships(semantic_result, module_deps)
        }
    def _build_relationships(self, semantic_result: Dict[str, Any],
                           module_deps: Dict[str, List[str]]) -> List[Dict[str, Any]]:
        """Build relationships between code elements.
        Args:
            semantic_result: Results from semantic analysis
            module_deps: Module dependencies
        Returns:
            List of relationships
        """
        relationships = []
        # Add type relationships
        for name, type_info in semantic_result['types'].items():
            relationships.append({
                'type': 'type_definition',
                'from': name,
                'to': str(type_info),
                'context': 'type'
            })
        # Add function relationships
        for name, context in semantic_result['contexts'].items():
            if context['type'] == 'function':
                relationships.append({
                    'type': 'function_definition',
                    'from': name,
                    'to': str(context['return_type']),
                    'context': 'function'
                })
                # Add parameter relationships
                for param in context['parameters']:
                    relationships.append({
                        'type': 'parameter',
                        'from': name,
                        'to': param['name'],
                        'context': 'function'
                    })
        # Add class relationships
        for name, context in semantic_result['contexts'].items():
            if context['type'] == 'class':
                relationships.append({
                    'type': 'class_definition',
                    'from': name,
                    'to': 'object',
                    'context': 'class'
                })
                # Add method relationships
                for method_name, method_info in context['methods'].items():
                    relationships.append({
                        'type': 'method',
                        'from': name,
                        'to': method_name,
                        'context': 'class'
                    })
                # Add property relationships
                for prop_name, prop_info in context['properties'].items():
                    relationships.append({
                        'type': 'property',
                        'from': name,
                        'to': prop_name,
                        'context': 'class'
                    })
        # Add module relationships
        for dep in module_deps['direct']:
            relationships.append({
                'type': 'module_dependency',
                'from': 'current',
                'to': dep,
                'context': 'module'
            })
        return relationships
    def _update_relationships(self, file_path: str, context_info: Dict[str, Any]):
        """Update the relationship map with new relationships.
        Args:
            file_path: Path to the file
            context_info: Context information
        """
        self.relationship_map[file_path] = context_info['relationships']
    def get_context(self, file_path: str, symbol: str) -> Optional[Dict[str, Any]]:
        """Get context information for a symbol.
        Args:
            file_path: Path to the file
            symbol: Symbol to look up
        Returns:
            Context information or None if not found
        """
        if file_path in self.context_map:
            context_info = self.context_map[file_path]
            # Check types
            if symbol in context_info['types']:
                return {
                    'type': 'type',
                    'symbol': symbol,
                    'type_info': str(context_info['types'][symbol])
                }
            # Check contexts
            if symbol in context_info['contexts']:
                return {
                    'type': context_info['contexts'][symbol]['type'],
                    'symbol': symbol,
                    'context': context_info['contexts'][symbol]
                }
        return None
    def get_relationships(self, file_path: str, symbol: Optional[str] = None) -> List[Dict[str, Any]]:
        """Get relationships for a file or symbol.
        Args:
            file_path: Path to the file
            symbol: Optional symbol to filter relationships
        Returns:
            List of relationships
        """
        if file_path not in self.relationship_map:
            return []
        relationships = self.relationship_map[file_path]
        if symbol:
            return [r for r in relationships if r['from'] == symbol or r['to'] == symbol]
        return relationships
    def get_symbol_usage(self, file_path: str, symbol: str) -> List[Dict[str, Any]]:
        """Get usage information for a symbol.
        Args:
            file_path: Path to the file
            symbol: Symbol to look up
        Returns:
            List of usage information
        """
        if file_path not in self.context_map:
            return []
        context_info = self.context_map[file_path]
        usages = []
        # Check type usage
        if symbol in context_info['types']:
            usages.append({
                'type': 'type_usage',
                'symbol': symbol,
                'context': 'type'
            })
        # Check function usage
        if symbol in context_info['contexts']:
            context = context_info['contexts'][symbol]
            if context['type'] == 'function':
                usages.append({
                    'type': 'function_usage',
                    'symbol': symbol,
                    'context': 'function'
                })
        # Check class usage
        if symbol in context_info['contexts']:
            context = context_info['contexts'][symbol]
            if context['type'] == 'class':
                usages.append({
                    'type': 'class_usage',
                    'symbol': symbol,
                    'context': 'class'
                })
        # Check variable usage
        for name, type_info in context_info['types'].items():
            if str(type_info) == symbol:
                usages.append({
                    'type': 'variable_usage',
                    'symbol': name,
                    'context': 'variable'
                })
        return usages
    def get_dependency_graph(self) -> Dict[str, Any]:
        """Get the complete dependency graph.
        Returns:
            Dictionary containing nodes and edges of the dependency graph
        """
        graph = {
            'nodes': [],
            'edges': []
        }
        # Add nodes for all files
        for file_path, context_info in self.context_map.items():
            graph['nodes'].append({
                'id': file_path,
                'type': 'file',
                'symbols': list(context_info['types'].keys()) + list(context_info['contexts'].keys())
            })
        # Add edges for dependencies
        for file_path, context_info in self.context_map.items():
            for dep in context_info['dependencies']['direct']:
                graph['edges'].append({
                    'from': file_path,
                    'to': dep,
                    'type': 'module_dependency'
                })
        return graph
    def get_symbol_graph(self, file_path: str) -> Dict[str, Any]:
        """Get the symbol relationship graph for a file.
        Args:
            file_path: Path to the file
        Returns:
            Dictionary containing nodes and edges of the symbol graph
        """
        if file_path not in self.context_map:
            return {'nodes': [], 'edges': []}
        context_info = self.context_map[file_path]
        graph = {
            'nodes': [],
            'edges': []
        }
        # Add nodes for all symbols
        for name, type_info in context_info['types'].items():
            graph['nodes'].append({
                'id': name,
                'type': 'type',
                'type_info': str(type_info)
            })
        for name, context in context_info['contexts'].items():
            graph['nodes'].append({
                'id': name,
                'type': context['type'],
                'context': context
            })
        # Add edges for relationships
        for rel in context_info['relationships']:
            graph['edges'].append({
                'from': rel['from'],
                'to': rel['to'],
                'type': rel['type']
            })
        return graph
</file>

<file path="server/code_understanding/extractor.py">
"""Symbol extractor for code understanding."""
from typing import Dict, List, Optional, Any, Tuple
import logging
logger = logging.getLogger(__name__)
class SymbolExtractor:
    """Extracts symbols from syntax trees."""
    def __init__(self):
        """Initialize the symbol extractor."""
        self.symbols = {
            'imports': [],
            'functions': [],
            'classes': [],
            'variables': []
        }
        self.references = {
            'calls': [],
            'attributes': [],
            'variables': []
        }
        self.current_scope = None
        self.current_file = None
        self.file_contexts = {}
    def extract_symbols(self, tree: Any, file_path: str = None, file_contexts: Dict[str, Any] = None) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, List[Dict[str, Any]]]]:
        """Extract symbols from a syntax tree.
        Args:
            tree: Syntax tree to analyze
            file_path: Path to the current file
            file_contexts: Dictionary of file contexts
        Returns:
            Tuple of (symbols, references)
        """
        try:
            self.symbols = {
                'imports': [],
                'functions': [],
                'classes': [],
                'variables': []
            }
            self.references = {
                'calls': [],
                'attributes': [],
                'variables': []
            }
            self.current_scope = 'global'
            self.current_file = file_path
            self.file_contexts = file_contexts or {}
            # Process root node if it exists
            if hasattr(tree, 'root_node') and tree.root_node:
                self._process_node(tree.root_node)
            elif hasattr(tree, 'type'):
                self._process_node(tree)
            return self.symbols, self.references
        except Exception as e:
            logger.error(f"Failed to extract symbols: {e}")
            return {}, {}
    def extract_references(self, tree: Any) -> Dict[str, List[Dict[str, Any]]]:
        """Extract references from a syntax tree.
        Args:
            tree: Syntax tree to analyze
        Returns:
            Dictionary of references
        """
        try:
            self.references = {
                'calls': [],
                'attributes': [],
                'variables': []
            }
            self.current_scope = 'global'
            self._process_node(tree)
            return self.references
        except Exception as e:
            logger.error(f"Failed to extract references: {e}")
            return {}
    def _process_node(self, node: Any, parent_scope: Optional[str] = None):
        """Process a syntax tree node and extract symbols.
        Args:
            node: Syntax tree node
            parent_scope: Parent scope name
        """
        try:
            if not node:
                return
            # Store old scope to restore later
            old_scope = self.current_scope
            # Update current scope if parent_scope is provided
            if parent_scope:
                self.current_scope = parent_scope
            elif not self.current_scope:
                self.current_scope = 'global'
            # Process node based on type
            if hasattr(node, 'type'):
                if node.type == 'import':
                    self._process_import(node)
                elif node.type == 'function_definition':
                    self._process_function(node)
                    # Process function body with function name as scope
                    body_node = node.child_by_field_name('body')
                    if body_node:
                        name_node = node.child_by_field_name('name')
                        if name_node:
                            self._process_node(body_node, name_node.text)
                elif node.type == 'class_definition':
                    self._process_class(node)
                    # Process class body with class name as scope
                    body_node = node.child_by_field_name('body')
                    if body_node:
                        name_node = node.child_by_field_name('name')
                        if name_node:
                            self._process_node(body_node, name_node.text)
                elif node.type == 'identifier':
                    self._process_identifier(node)
                elif node.type == 'assignment':
                    self._process_assignment(node)
            # Process children if not already processed
            if hasattr(node, 'children'):
                for child in node.children:
                    self._process_node(child, self.current_scope)
            # Restore old scope
            self.current_scope = old_scope
        except Exception as e:
            logger.error(f"Failed to process node: {e}")
    def _process_import(self, node: Any):
        """Process an import statement node.
        Args:
            node: Import statement node
        """
        try:
            text = node.text.strip()
            if text.startswith('import '):
                # Simple import
                modules = text[7:].split(',')  # Skip 'import ' and split on comma
                for module in modules:
                    module = module.strip()
                    # Handle aliases
                    if ' as ' in module:
                        module, alias = module.split(' as ')
                        module = module.strip()
                        alias = alias.strip()
                        self.symbols['imports'].append({
                            'module': module,
                            'alias': alias,
                            'start_line': node.start_point[0],
                            'end_line': node.end_point[0]
                        })
                    else:
                        self.symbols['imports'].append({
                            'module': module,
                            'start_line': node.start_point[0],
                            'end_line': node.end_point[0]
                        })
            elif text.startswith('from '):
                # From import
                parts = text.split(' import ')
                if len(parts) == 2:
                    module = parts[0][5:].strip()  # Skip 'from '
                    names = [n.strip() for n in parts[1].split(',')]
                    for name in names:
                        # Handle aliases
                        if ' as ' in name:
                            name, alias = name.split(' as ')
                            name = name.strip()
                            alias = alias.strip()
                            self.symbols['imports'].append({
                                'module': module,
                                'symbol': name,
                                'alias': alias,
                                'start_line': node.start_point[0],
                                'end_line': node.end_point[0]
                            })
                        else:
                            self.symbols['imports'].append({
                                'module': module,
                                'symbol': name.strip(),
                                'start_line': node.start_point[0],
                                'end_line': node.end_point[0]
                            })
        except Exception as e:
            logger.error(f"Failed to process import: {e}")
    def _process_function(self, node: Any):
        """Process a function definition node.
        Args:
            node: Function definition node
        """
        try:
            name_node = node.child_by_field_name('name')
            if name_node:
                func_info = {
                    'name': name_node.text,
                    'start_line': node.start_point[0],
                    'end_line': node.end_point[0],
                    'parameters': []
                }
                # Process parameters
                params_node = node.child_by_field_name('parameters')
                if params_node:
                    for param in params_node.children:
                        if param.type == 'identifier':
                            func_info['parameters'].append({
                                'name': param.text,
                                'start_line': param.start_point[0],
                                'end_line': param.end_point[0]
                            })
                self.symbols['functions'].append(func_info)
        except Exception as e:
            logger.error(f"Failed to process function: {e}")
    def _process_class(self, node: Any):
        """Process a class definition node.
        Args:
            node: Class definition node
        """
        try:
            name_node = node.child_by_field_name('name')
            if name_node:
                class_info = {
                    'name': name_node.text,
                    'start_line': node.start_point[0],
                    'end_line': node.end_point[0],
                    'bases': [],
                    'methods': []
                }
                # Process base classes
                bases_node = node.child_by_field_name('bases')
                if bases_node:
                    for base in bases_node.children:
                        if base.type == 'identifier':
                            # Check if base class is imported
                            base_name = base.text
                            base_file = None
                            # First check if it's an imported symbol
                            for imp in self.symbols['imports']:
                                if imp.get('symbol') == base_name:
                                    # Found direct symbol import
                                    module_name = imp.get('module')
                                    # Try to find the actual file path for the imported module
                                    for file_path in self.file_contexts:
                                        if file_path.endswith(module_name + '.py'):
                                            # Check if the base class exists in this file
                                            for other_class in self.file_contexts[file_path].symbols.get('classes', []):
                                                if other_class.get('name') == base_name:
                                                    base_file = file_path
                                                    break
                                            if base_file:
                                                break
                                    if not base_file:
                                        base_file = module_name  # If not found, use module name
                                    break
                                elif imp.get('module') == base_name:
                                    # Found module import
                                    base_file = imp.get('module')
                                    break
                            # If not found in imports, look in other files
                            if not base_file:
                                for file_path, file_context in self.file_contexts.items():
                                    if file_path != self.current_file:  # Don't look in current file
                                        for other_class in file_context.symbols.get('classes', []):
                                            if other_class.get('name') == base_name:
                                                base_file = file_path
                                                break
                                        if base_file:
                                            break
                            # If still not found, assume it's in current file
                            if not base_file:
                                base_file = self.current_file
                            class_info['bases'].append({
                                'name': base_name,
                                'file_path': base_file,
                                'start_line': base.start_point[0],
                                'end_line': base.end_point[0]
                            })
                # Process methods
                body_node = node.child_by_field_name('body')
                if body_node:
                    for child in body_node.children:
                        if child.type == 'function_definition':
                            name_node = child.child_by_field_name('name')
                            if name_node:
                                class_info['methods'].append({
                                    'name': name_node.text,
                                    'start_line': child.start_point[0],
                                    'end_line': child.end_point[0]
                                })
                self.symbols['classes'].append(class_info)
        except Exception as e:
            logger.error(f"Failed to process class: {e}")
    def _process_identifier(self, node: Any):
        """Process an identifier node.
        Args:
            node: Identifier node
        """
        try:
            # Add to references if in a function or method scope
            if self.current_scope != 'global':
                self.references['variables'].append({
                    'name': node.text,
                    'scope': self.current_scope,
                    'start_line': node.start_point[0],
                    'end_line': node.end_point[0]
                })
        except Exception as e:
            logger.error(f"Failed to process identifier: {e}")
    def _process_assignment(self, node: Any):
        """Process an assignment node.
        Args:
            node: Assignment node
        """
        try:
            left_node = node.child_by_field_name('left')
            if left_node:
                self.symbols['variables'].append({
                    'name': left_node.text,
                    'scope': self.current_scope,
                    'start_line': node.start_point[0],
                    'end_line': node.end_point[0]
                })
        except Exception as e:
            logger.error(f"Failed to process assignment: {e}")
    def get_symbols(self) -> Dict[str, List[Dict[str, Any]]]:
        """Get the extracted symbols.
        Returns:
            Dictionary of symbols
        """
        return self.symbols
    def get_references(self) -> Dict[str, List[Dict[str, Any]]]:
        """Get the extracted references.
        Returns:
            Dictionary of references
        """
        return self.references
</file>

<file path="server/code_understanding/graph.py">
"""Module for graph data structures."""
import logging
from dataclasses import dataclass, field
from enum import Enum
from typing import Dict, List, Optional, Set, Any, Union
logger = logging.getLogger(__name__)
class NodeType(Enum):
    """Node types."""
    MODULE = 'module'
    FUNCTION = 'function'
    CLASS = 'class'
    METHOD = 'method'
    PARAMETER = 'parameter'
    VARIABLE = 'variable'
    ATTRIBUTE = 'attribute'
    SYMBOL = 'symbol'
class RelationType(Enum):
    """Edge types."""
    IMPORTS = 'imports'
    CONTAINS = 'contains'
    CALLS = 'calls'
    REFERENCES = 'references'
    INHERITS = 'inherits'
    HAS_ATTRIBUTE = 'has_attribute'
@dataclass
class Node:
    """A node in the graph."""
    id: str
    name: str
    type: NodeType
    properties: Dict[str, Any]
    def __post_init__(self):
        """Initialize additional attributes after dataclass initialization."""
        self.file_path = self.properties.get('file_path', '')
        self.start_line = self.properties.get('start_line', 0)
        self.end_line = self.properties.get('end_line', 0)
@dataclass
class Edge:
    """An edge in the graph."""
    from_node: str
    to_node: str
    type: str
    properties: Dict[str, Any] = field(default_factory=dict)
class Graph:
    """A graph data structure."""
    def __init__(self):
        """Initialize the graph."""
        self.nodes: Dict[str, Node] = {}
        self.edges: List[Edge] = []
    def add_node(self, name: str, type: Union[str, NodeType], file_path: str, start_line: int = 0, end_line: int = 0, properties: Optional[Dict[str, Any]] = None) -> Node:
        """Add a node to the graph.
        Args:
            name: Node name
            type: Node type
            file_path: File path
            start_line: Start line number
            end_line: End line number
            properties: Optional node properties
        Returns:
            Added node
        """
        # Convert NodeType enum to string if needed
        node_type = type.value if isinstance(type, NodeType) else type
        node_id = f"{file_path}:{node_type}:{name}"
        if node_id not in self.nodes:
            self.nodes[node_id] = Node(
                id=node_id,
                name=name,
                type=node_type,
                properties=properties or {}
            )
        return self.nodes[node_id]
    def add_edge(self, from_node: str, to_node: str, type: Union[str, RelationType], properties: Optional[Dict[str, Any]] = None) -> None:
        """Add an edge to the graph.
        Args:
            from_node: Source node ID
            to_node: Target node ID
            type: Edge type
            properties: Optional edge properties
        """
        if not properties:
            properties = {}
        # Convert RelationType enum to string if needed
        edge_type = type.value if isinstance(type, RelationType) else type
        edge = Edge(from_node=from_node, to_node=to_node, type=edge_type, properties=properties)
        self.edges.append(edge)
    def create_edge(self, from_node: Node, to_node: Node, type: RelationType, properties: Optional[Dict[str, Any]] = None) -> None:
        """Create an edge between two nodes.
        Args:
            from_node: Source node
            to_node: Target node
            type: Edge type
            properties: Optional edge properties
        """
        if not properties:
            properties = {}
        if from_node.id not in self.nodes or to_node.id not in self.nodes:
            raise ValueError("Both nodes must exist in the graph")
        edge = Edge(from_node=from_node.id, to_node=to_node.id, type=type.value, properties=properties)
        self.edges.append(edge)
    def get_node(self, node_id: str) -> Optional[Node]:
        """Get a node by ID.
        Args:
            node_id: Node ID
        Returns:
            Node if found, None otherwise
        """
        return self.nodes.get(node_id)
    def get_edges(self, source_id: Optional[str] = None, target_id: Optional[str] = None, rel_type: Optional[Union[str, RelationType]] = None) -> List[Edge]:
        """Get edges matching the given criteria.
        Args:
            source_id: Optional source node ID to filter by
            target_id: Optional target node ID to filter by
            rel_type: Optional relationship type to filter by
        Returns:
            List of matching edges
        """
        result = []
        # Convert RelationType enum to string if needed
        edge_type = rel_type.value if isinstance(rel_type, RelationType) else rel_type
        for edge in self.edges:
            matches = True
            if source_id is not None and edge.from_node != source_id:
                matches = False
            if target_id is not None and edge.to_node != target_id:
                matches = False
            if edge_type is not None and edge.type != edge_type:
                matches = False
            if matches:
                result.append(edge)
        return result
    def get_nodes_by_type(self, node_type: Union[str, NodeType]) -> List[Node]:
        """Get nodes of a specific type.
        Args:
            node_type: Node type
        Returns:
            List of matching nodes
        """
        # Convert NodeType enum to string if needed
        type_str = node_type.value if isinstance(node_type, NodeType) else node_type
        return [node for node in self.nodes.values() if node.type == type_str]
    def get_nodes_by_file(self, file_path: str) -> List[Node]:
        """Get nodes from a specific file.
        Args:
            file_path: File path
        Returns:
            List of matching nodes
        """
        return [node for node in self.nodes.values() if node.file_path == file_path]
    def clear(self) -> None:
        """Clear all nodes and edges."""
        self.nodes.clear()
        self.edges.clear()
    def find_or_create_node(self, name: str, type: NodeType, properties: Optional[Dict[str, Any]] = None) -> Node:
        """Find an existing node or create a new one.
        Args:
            name: Node name
            type: Node type
            properties: Optional node properties
        Returns:
            Node object
        """
        # Create a unique ID for the node
        file_path = properties.get('file_path', '') if properties else ''
        # If name already includes file path (e.g. "file.py:ClassName"), extract it
        if ':' in name:
            file_path, name = name.split(':', 1)
            if not properties:
                properties = {}
            properties['file_path'] = file_path
        # First try to find an existing node with the same name and file path
        for node in self.nodes.values():
            if node.type == type.value:
                # Try exact match first
                if node.name == name and node.properties.get('file_path') == file_path:
                    return node
                # Try with file path in name
                if node.name == f"{file_path}:{name}":
                    return node
                # Try just the name part if it matches
                if ':' in node.name:
                    node_file_path, node_name = node.name.split(':', 1)
                    if node_name == name and node_file_path == file_path:
                        return node
        # If not found, create a new node
        node_id = f"{file_path}:{type.value}:{name}"
        if not properties:
            properties = {}
        if file_path and 'file_path' not in properties:
            properties['file_path'] = file_path
        # For class nodes, include file path in name for better cross-file matching
        full_name = f"{file_path}:{name}" if file_path and type == NodeType.CLASS else name
        self.nodes[node_id] = Node(
            id=node_id,
            name=full_name,
            type=type.value,
            properties=properties
        )
        return self.nodes[node_id]
    def find_node(self, name: str) -> Optional[Node]:
        """Find a node by its name.
        Args:
            name: The name of the node to find
        Returns:
            The node if found, None otherwise
        """
        for node in self.nodes.values():
            if node.name == name:
                return node
        return None
</file>

<file path="server/code_understanding/language_adapters.py">
'''Language-specific parser adapters for JavaScript and Swift.'''
import ast
import re
from typing import Optional, Dict, Any, List
from tree_sitter import Language, Parser, Tree, Node
import logging # Add logging
import subprocess # For building grammar
import os # For path checks
from pathlib import Path # For path handling
# Import common Mock structure
from .common_types import MockTree, MockNode
logger = logging.getLogger(__name__)
# Define Tree-sitter query strings (or load from .scm files)
# --- Revised JS_IMPORT_QUERY V6 (Minimal) ---
JS_IMPORT_QUERY = """
(import_statement source: (string) @source)
"""
# --- End Revised Query V6 ---
# Query for require() call - run on the call_expression node
JS_REQUIRE_QUERY = """
(call_expression
  function: (identifier) @require_func
  arguments: (arguments (string) @source)
  (#match? @require_func "^require$") 
)
"""
# Add other queries (export, symbol) if needed for conversion logic
class JavaScriptParserAdapter:
    """JavaScript parser adapter using tree-sitter for robust parsing."""
    def __init__(self):
        """Initialize the JavaScript parser adapter."""
        self.parser = Parser()
        self.language = None
        # Store compiled queries if needed for conversion
        self.import_query = None
        self.require_query = None
        # Add others like symbol_query if needed
        self._load_language_and_queries()
    def _load_language_and_queries(self):
        """Load the tree-sitter JS language and compile queries."""
        try:
            # Robust grammar loading/building logic from javascript_parser.py
            vendor_path = Path(__file__).parent.parent.parent / 'vendor' / 'tree-sitter-javascript'
            # Standard location for compiled library might be just 'languages.so' 
            # or a specific name in a build/dist dir. Adjust as needed.
            # Let's try a common pattern: build/<lang_name>.so
            build_dir = Path(__file__).parent / 'build'
            language_lib = build_dir / 'javascript.so' 
            # Ensure build directory exists
            build_dir.mkdir(exist_ok=True)
            # Clone the repository if grammar source doesn't exist
            if not (vendor_path / 'src' / 'parser.c').exists():
                if vendor_path.exists(): # Clean up if incomplete clone exists
                     try: subprocess.run(['rm', '-rf', str(vendor_path)], check=True)
                     except: pass # Ignore errors
                logger.info("Cloning tree-sitter-javascript repository...")
                # Use https instead of ssh for broader compatibility
                subprocess.run(['git', 'clone', 'https://github.com/tree-sitter/tree-sitter-javascript.git', str(vendor_path)], check=True, capture_output=True)
            # Build the language library if it doesn't exist or is older than source
            needs_build = True
            if language_lib.exists() and vendor_path.exists():
                src_mtime = max(p.stat().st_mtime for p in vendor_path.glob('src/*.[ch]'))
                if language_lib.stat().st_mtime >= src_mtime:
                    needs_build = False
            if needs_build:
                logger.info(f"Building tree-sitter-javascript library to {language_lib}...")
                try:
                    # Use the Language.build_library method for simplicity
                    Language.build_library(
                        str(language_lib),
                        [str(vendor_path)] # Must be list of directories
                    )
                    logger.info("Successfully built tree-sitter-javascript library.")
                except Exception as e:
                    logger.error(f"Failed to build JS grammar using build_library: {e}")
                    # Optional: Add fallback to manual compile steps if needed
                    raise # Reraise to signal failure
            # Load the language
            self.language = Language(str(language_lib), 'javascript')
            self.parser.set_language(self.language)
            logger.info("JavaScript language loaded successfully.")
            # Compile queries needed for conversion
            self.import_query = self.language.query(JS_IMPORT_QUERY)
            self.require_query = self.language.query(JS_REQUIRE_QUERY) # Compile require query
            # Compile other queries if needed
            logger.info("JavaScript queries compiled.")
        except Exception as e:
            logger.exception(f"Error loading JavaScript language or queries: {e}")
            self.language = None # Ensure language is None if loading failed
            # Do not raise here; CodeParser should handle the lack of a language
    def parse(self, code: str) -> Optional[MockTree]:
        """Parse JavaScript code into a MockTree using tree-sitter."""
        if not self.language:
            logger.error("JavaScript language not loaded for adapter. Cannot parse.")
            raise RuntimeError("JavaScript language not loaded for adapter.")
        # Validate input before the main try block
        if isinstance(code, bytes):
            try:
                 code = code.decode('utf-8')
            except UnicodeDecodeError as e:
                 logger.error(f"Failed to decode input bytes: {e}")
                 raise ValueError("Input code is not valid UTF-8") from e
        # Raise ValueError for empty or whitespace-only code
        if not code.strip():
            raise ValueError("Input code cannot be empty or whitespace only.")
        # Now, proceed with parsing inside the try block
        try:
            tree = self.parser.parse(bytes(code, 'utf8'))
            root_ts_node = tree.root_node
            # ---- Remove Temporary Debugging ----
            # logger.debug(f"Tree-sitter S-expression:\n{root_ts_node.sexp()}")
            # ---- End Remove Temporary Debugging ----
            # Optional: Check for errors, but proceed anyway for partial analysis
            if root_ts_node.has_error:
                logger.warning("JS Parsing resulted in errors. Analysis might be incomplete.")
                # Find first error node for logging
                error_node = next((n for n in root_ts_node.descendants if n.has_error), root_ts_node)
                logger.warning(f"First error node: type={error_node.type}, pos={error_node.start_point}")
            # Convert the tree-sitter tree to our MockTree structure
            mock_root = self._tree_sitter_to_mock_node(root_ts_node)
            if mock_root:
                 return MockTree(root=mock_root)
            else:
                 logger.error("Conversion from Tree-sitter to MockNode failed for JS root.")
                 return None
        except Exception as e:
            logger.exception(f"Error parsing JS code: {e}")
            return None
    def _tree_sitter_to_mock_node(self, ts_node: Node) -> Optional[MockNode]:
         """Recursively convert a tree-sitter node to a MockNode."""
         if not ts_node:
             return None
         # --- Add Entry Logging ---
         logger.debug(f"_tree_sitter_to_mock_node processing: type={ts_node.type}, text='{ts_node.text.decode("utf-8")[:50]}...'")
         # --- End Entry Logging ---
         node_type = ts_node.type
         # --- Program Node Handling --- 
         if node_type == 'program':
              mock_type = 'program'
              start_point = ts_node.start_point
              end_point = ts_node.end_point
              children = []
              fields = {}
              for child_node in ts_node.children:
                   child_mock = self._tree_sitter_to_mock_node(child_node) # Recursive call first
                   if child_mock:
                        children.append(child_mock)
              return MockNode(type=mock_type, text=mock_type, start_point=start_point, end_point=end_point, children=children, fields=fields)
         # --- Specific Node Type Conversions --- 
         elif node_type == 'import_statement':
             return self._convert_import_statement(ts_node)
         # Let _convert_variable_declarator handle require check internally
         elif node_type == 'variable_declaration' or node_type == 'lexical_declaration':
             return self._convert_variable_declaration(ts_node)
         elif node_type == 'variable_declarator':
             return self._convert_variable_declarator(ts_node)
         elif node_type == 'function_declaration':
             converted_node = self._convert_function_definition(ts_node)
             # --- Add return value logging ---
             if converted_node:
                 logger.debug(f"_convert_function_definition returned: type={converted_node.type}, name={converted_node.fields.get('name')}")
             else:
                 logger.debug(f"_convert_function_definition returned None")
             # --- End logging ---
             return converted_node
         elif node_type == 'class_declaration':
             return self._convert_class_definition(ts_node)
         elif node_type == 'method_definition':
              return self._convert_function_definition(ts_node)
         elif node_type == 'export_statement':
             return self._convert_export_statement(ts_node)
         # Arrow functions are usually values, handled in _convert_variable_declarator
         # If encountered elsewhere, convert generically? Or return None?
         # elif node_type == 'arrow_function':
         #     # Let _convert_variable_declarator handle these primarily
         #     # If needed for other contexts, call _convert_arrow_function here
         #     pass # Or return self._convert_arrow_function(ts_node) ?
         else:
             # Default: Convert generically ONLY if not handled above
             # Add logging before generic conversion
             return self._convert_node_generically(ts_node)
    # --- Refactored Helper Methods --- 
    def _convert_import_statement(self, ts_node: Node) -> Optional[MockNode]:
         """Converts a tree-sitter import_statement node using minimal query and Python AST traversal."""
         if ts_node.type != 'import_statement': return None
         mock_type = 'import_statement'
         start_point=ts_node.start_point
         end_point=ts_node.end_point
         children = [] 
         fields = {}
         source = None
         default_import_name = None
         named_imports = []
         namespace_import_name = None 
         # Run the minimal query just to get the source reliably
         captures = self.import_query.captures(ts_node)
         logger.debug(f"JS Import Captures V6 ({ts_node.start_point}): {[(c[1], c[0].type) for c in captures]}")
         for captured_node, capture_name in captures:
             if capture_name == 'source': 
                 source = captured_node.text.decode('utf-8').strip('\'"')
                 fields['module'] = source
                 break # Source found
         if source is None:
             logger.warning(f"Could not extract source from import statement: {ts_node.text.decode()}")
             return None
         # Now, manually find the import_clause node among children
         clause_node = next((c for c in ts_node.children if c.type == 'import_clause'), None)
         is_side_effect_only = True # Assume side-effect until clause processed
         if clause_node:
             logger.debug(f"Processing clause node: type={clause_node.type}")
             is_side_effect_only = False
             # Check children of the clause node
             for child in clause_node.children:
                  if child.type == 'identifier': # Default import
                       default_import_name = child.text.decode('utf-8')
                       fields['default_name'] = default_import_name
                  elif child.type == 'named_imports': # Named imports
                       for named_child in child.children:
                            if named_child.type == 'import_specifier':
                                 for spec_child in named_child.children:
                                      if spec_child.type == 'identifier':
                                           named_imports.append(spec_child.text.decode('utf-8'))
                       if named_imports:
                            fields['named_names'] = named_imports
                  elif child.type == 'namespace_import': # Namespace import
                       for ns_child in child.children:
                            if ns_child.type == 'identifier':
                                 namespace_import_name = ns_child.text.decode('utf-8')
                                 fields['namespace_name'] = namespace_import_name
         # Create the MockNode with all fields
         return MockNode(
             type=mock_type,
             text=ts_node.text.decode('utf-8'),
             start_point=start_point,
             end_point=end_point,
             children=children,
             fields=fields
         )
    def _create_require_mock_node(self, declarator_node: Node, captures: list) -> Optional[MockNode]:
          """Creates a require_statement MockNode from captures and the declarator node."""
          mock_type = 'require_statement'
          # Get position from the original declaration (parent of declarator)
          parent_node = declarator_node.parent 
          start_point = parent_node.start_point if parent_node else declarator_node.start_point
          end_point = parent_node.end_point if parent_node else declarator_node.end_point
          children = []
          fields = {}
          # Get variable name directly from the declarator node
          name_node_ts = declarator_node.child_by_field_name('name')
          req_var_name = name_node_ts.text.decode('utf-8') if name_node_ts else "<unknown>"
          fields['name'] = req_var_name 
          req_source = "<unknown>"
          # Get source from captures
          for captured_node, capture_name in captures:
               if capture_name == 'source': 
                    req_source = captured_node.text.decode('utf-8').strip('\'"')
                    fields['module'] = req_source
                    break # Should only be one source
          node_text = req_source 
          if fields.get('module') and fields.get('name') != "<unknown>":
               children.append(MockNode(type='identifier', text=fields['name']))
               children.append(MockNode(type='string', text=fields['module']))
               return MockNode(type=mock_type, text=node_text, start_point=start_point, end_point=end_point, children=children, fields=fields)
          else:
               logger.warning(f"Could not extract require parts. Name: {req_var_name}, Source: {req_source}, Node: {declarator_node.text.decode()}")
               return None
    def _convert_function_definition(self, ts_node: Node) -> Optional[MockNode]:
        """Converts function_declaration or method_definition."""
        if ts_node.type not in ['function_declaration', 'method_definition']: return None
        mock_type = 'function_definition'
        start_point=ts_node.start_point
        end_point=ts_node.end_point
        children = []
        fields = {}
        # Initialize node_text with a default
        node_text = "<anonymous_func>" 
        func_name = None
        name_node_ts = ts_node.child_by_field_name('name')
        params_node_ts = ts_node.child_by_field_name('parameters')
        body_node_ts = ts_node.child_by_field_name('body')
        if name_node_ts: 
            # Extract the name string directly
            func_name = name_node_ts.text.decode('utf-8')
            # Set node_text to the function name if found
            node_text = func_name 
            # Store the name string in fields
            fields['name'] = func_name 
            # Optionally add the identifier node as a child if needed elsewhere
            # name_mock = self._convert_node_generically(name_node_ts)
            # if name_mock: children.append(name_mock)
        if ts_node.type == 'method_definition' and 'static' in [c.type for c in ts_node.children]:
            fields['is_static'] = True
        if params_node_ts: 
            params_mock = self._convert_node_generically(params_node_ts)
            if params_mock: children.append(params_mock)
        if body_node_ts: 
             body_mock = self._convert_node_generically(body_node_ts)
             if body_mock: children.append(body_mock)
        return MockNode(type=mock_type, text=node_text, start_point=start_point, end_point=end_point, children=children, fields=fields)
    def _convert_arrow_function(self, ts_node: Node, assigned_name: Optional[str]=None) -> Optional[MockNode]:
         """Converts an arrow_function node."""
         if ts_node.type != 'arrow_function': return None
         mock_type = 'function_definition'
         start_point=ts_node.start_point
         end_point=ts_node.end_point
         children = []
         fields = {}
         # Use assigned_name directly for node_text and fields['name']
         node_text = assigned_name or "<anonymous_arrow>"
         if assigned_name: 
             fields['name'] = assigned_name 
             # Optionally add identifier node as child
             # children.append(MockNode(type='identifier', text=assigned_name))
         params_node_ts = ts_node.child_by_field_name('parameters')
         body_node_ts = ts_node.child_by_field_name('body')
         if params_node_ts: 
            params_mock = self._convert_node_generically(params_node_ts)
            if params_mock: children.append(params_mock)
         if body_node_ts: 
             body_mock = self._convert_node_generically(body_node_ts)
             if body_mock: children.append(body_mock)
         return MockNode(type=mock_type, text=node_text, start_point=start_point, end_point=end_point, children=children, fields=fields)
    def _convert_class_definition(self, ts_node: Node) -> Optional[MockNode]:
         """Converts a class_declaration node."""
         if ts_node.type != 'class_declaration': return None
         mock_type = 'class_definition'
         start_point=ts_node.start_point
         end_point=ts_node.end_point
         children = []
         fields = {}
         node_text = "<anonymous_class>"
         name_node_ts = ts_node.child_by_field_name('name')
         body_node_ts = ts_node.child_by_field_name('body') 
         if name_node_ts: 
             fields['name'] = self._convert_node_generically(name_node_ts)
             if fields['name']: node_text = fields['name'].text
         if body_node_ts: 
             body_mock = self._convert_node_generically(body_node_ts)
             if body_mock: children.append(body_mock)
         return MockNode(type=mock_type, text=node_text, start_point=start_point, end_point=end_point, children=children, fields=fields)
    def _convert_variable_declaration(self, ts_node: Node) -> Optional[MockNode]:
         """Converts variable_declaration or lexical_declaration.
            Returns a wrapper node containing converted declarators.
         """
         if ts_node.type not in ['variable_declaration', 'lexical_declaration']: return None
         mock_type = 'variable_declaration' # Unified type
         start_point=ts_node.start_point; end_point=ts_node.end_point
         children = [] # Store converted declarator nodes
         for child_node in ts_node.children:
              if child_node.type == 'variable_declarator':
                   decl_mock = self._convert_variable_declarator(child_node)
                   if decl_mock: children.append(decl_mock)
         if not children: return None # Skip empty declarations (e.g., just 'var;')
         # Return a single 'variable_declaration' node wrapping the declarators
         return MockNode(type=mock_type, text=ts_node.type, start_point=start_point, end_point=end_point, children=children)
    def _convert_variable_declarator(self, ts_node: Node) -> Optional[MockNode]:
        """Converts a variable_declarator node. 
           Checks if value is require() or arrow func.
           Returns require_statement, function_definition, or variable_declarator MockNode.
        """
        if ts_node.type != 'variable_declarator': return None
        name_node_ts = ts_node.child_by_field_name('name')
        value_node_ts = ts_node.child_by_field_name('value')
        assigned_name = name_node_ts.text.decode('utf-8') if name_node_ts else None
        # --- Check for require --- 
        if value_node_ts and value_node_ts.type == 'call_expression':
            # Check if this is a require call
            callee = value_node_ts.child_by_field_name('function')
            args = value_node_ts.child_by_field_name('arguments')
            if callee and callee.type == 'identifier' and callee.text.decode('utf-8') == 'require':
                # Find the module string in the arguments
                module = None
                if args:
                    for arg in args.children:
                        if arg.type == 'string':
                            module = arg.text.decode('utf-8').strip('"\'')
                            break
                if module:
                    # Create a require_statement node
                    mock_type = 'variable_declarator'
                    start_point = ts_node.start_point
                    end_point = ts_node.end_point
                    children = []
                    fields = {
                        'name': assigned_name,
                        'module': module,
                        'type': 'require'
                    }
                    if assigned_name:
                        children.append(MockNode(type='identifier', text=assigned_name))
                    children.append(MockNode(type='string', text=module))
                    return MockNode(
                        type=mock_type,
                        text=f"require('{module}')",
                        start_point=start_point,
                        end_point=end_point,
                        children=children,
                        fields=fields
                    )
        # --- Check for arrow function --- 
        if value_node_ts and value_node_ts.type == 'arrow_function':
             return self._convert_arrow_function(value_node_ts, assigned_name=assigned_name)
        # --- Otherwise, handle as a regular variable declarator --- 
        mock_type = 'variable_declarator' 
        start_point=ts_node.start_point; end_point=ts_node.end_point
        children = [] ; fields = {}
        node_text = assigned_name or "<declarator>"
        if name_node_ts:
             name_mock = self._convert_node_generically(name_node_ts)
             if name_mock: fields['name'] = name_mock
        if value_node_ts:
            value_mock = self._convert_node_generically(value_node_ts)
            if value_mock: fields['value'] = value_mock
        return MockNode(type=mock_type, text=node_text, start_point=start_point, end_point=end_point, children=children, fields=fields)
    def _convert_export_statement(self, ts_node: Node) -> Optional[MockNode]:
         """Converts an export_statement node.
            Returns the converted *exported item* (marked) or a special node for named lists.
         """
         if ts_node.type != 'export_statement': return None
         is_default = any(c.type == 'default' for c in ts_node.children)
         export_clause = next((c for c in ts_node.children if c.type == 'export_clause'), None)
         exported_node_ts = ts_node.named_children[0] if ts_node.named_child_count > 0 and not export_clause else None
         # Handle `export { name1, name2 };`
         if export_clause: 
             exported_names = []
             source_node = export_clause.child_by_field_name('source') # Handle export * from 'module'
             source = source_node.text.decode('utf-8').strip('"\'') if source_node else None
             for specifier in export_clause.children:
                 if specifier.type == 'export_specifier':
                      name_node = specifier.child_by_field_name('name')
                      alias_node = specifier.child_by_field_name('alias')
                      name = name_node.text.decode('utf-8') if name_node else None
                      alias = alias_node.text.decode('utf-8') if alias_node else None
                      if name: exported_names.append({'name': name, 'alias': alias})
             if not exported_names and not source: return None # Skip empty/unknown exports
             mock_type = 'export_named_list' 
             node_text = ", ".join([e['alias'] or e['name'] for e in exported_names]) + (f" from {source}" if source else "")
             fields = {'names': exported_names, 'source': source, 'is_default': is_default}
             children = [MockNode(type='identifier', text=e['alias'] or e['name']) for e in exported_names]
             return MockNode(type=mock_type, text=node_text, start_point=ts_node.start_point, end_point=ts_node.end_point, children=children, fields=fields)
         # Handle `export default ...`, `export function/class/const ...`
         elif exported_node_ts:
             export_mock = self._tree_sitter_to_mock_node(exported_node_ts)
             if export_mock:
                  # Mark the converted node itself as exported
                  export_mock.fields['is_exported'] = True
                  if is_default: export_mock.fields['is_default_export'] = True
                  return export_mock # Return the converted exported item
             else:
                  logger.warning(f"Failed to convert node inside export statement: {exported_node_ts.type}")
                  return None # Failed to convert exported item
         else: 
              logger.warning(f"Unhandled export statement structure: {ts_node.text.decode()}")
              return self._convert_node_generically(ts_node)
    def _convert_node_generically(self, ts_node: Node) -> Optional[MockNode]:
        """Generic fallback conversion for unhandled nodes, focusing on children."""
        if not ts_node: return None
        node_type = ts_node.type
        # Skip trivial types more broadly
        if not ts_node.is_named or node_type in ['comment', ';', '{ ', '}', '(', ')', ',', ':', '[', ']']: 
            return None
        # Use mapped type if available
        type_map = { 'program': 'module', 'property_identifier': 'identifier', 'formal_parameters': 'parameters', 'statement_block': 'body'} # Basic map
        mock_type = type_map.get(node_type, node_type)
        node_text = ts_node.text.decode('utf-8')
        start_point = ts_node.start_point
        end_point = ts_node.end_point
        children = []
        fields = {} # Keep fields empty for generic nodes
        # Recursively convert *named* children only for generic case to reduce noise
        for child_ts_node in ts_node.named_children:
            mock_child = self._tree_sitter_to_mock_node(child_ts_node) # Call main recursive func
            if mock_child:
                children.append(mock_child)
        # Create node even if children list is empty, if it's a potentially meaningful type
        # Refine the list of types to potentially skip if they have no children
        skip_if_empty = ['expression_statement', 'parenthesized_expression'] 
        if children or mock_type not in skip_if_empty:
            return MockNode(
                type=mock_type, 
                text=node_text, 
                start_point=start_point, 
                end_point=end_point, 
                children=children,
                fields=fields
            )
        else:
            # Log skipped nodes if needed for debugging
            # logger.debug(f"Skipping generic node conversion for type {mock_type} with no children: {node_text[:50]}...")
            return None
class SwiftParserAdapter:
    def parse(self, code: str) -> Optional[MockTree]:
        if not code or code.strip() == '':
            raise ValueError("Input code cannot be empty or whitespace only.")
        # For now, bypass validation and return a mock tree with a 'source_file' root node
        logger.warning("Swift parsing not implemented, returning basic mock tree.")
        root = MockNode(type='source_file', text='source_file')
        return MockTree(root)
</file>

<file path="server/code_understanding/mock_parser.py">
"""Mock parser for testing purposes."""
from typing import Any, Optional, List, Dict, Tuple
import ast
import logging
# Import common types
from .common_types import MockNode, MockTree
logger = logging.getLogger(__name__)
class MockParser:
    """Mock parser for testing."""
    def parse(self, code: str) -> Optional[MockTree]:
        """Parse code and return a mock tree."""
        print(f"--- ENTERING MockParser.parse ---")
        try:
            if isinstance(code, bytes):
                code = code.decode('utf-8')
            tree = ast.parse(code)
            logger.info(f"MockParser: AST parsed. Body nodes: {[type(n).__name__ for n in tree.body]}")
            root = MockNode('module', children=[])
            def process_node(node: ast.AST) -> Optional[MockNode]:
                logger.info(f"MockParser: Entered process_node for type: {type(node).__name__}")
                mock_node = None
                if isinstance(node, ast.Import):
                    logger.info(f"MockParser: Processing ast.Import")
                    mock_node = self._convert_ast_node(node)
                elif isinstance(node, ast.ImportFrom):
                    logger.info(f"MockParser: Processing ast.ImportFrom")
                    mock_node = self._convert_ast_node(node)
                elif isinstance(node, ast.FunctionDef):
                    logger.info(f"MockParser: Processing ast.FunctionDef")
                    mock_node = self._convert_ast_node(node)
                elif isinstance(node, ast.ClassDef):
                    logger.info(f"MockParser: Processing ast.ClassDef")
                    mock_node = self._convert_ast_node(node)
                elif isinstance(node, ast.Call):
                    logger.info(f"MockParser: Processing ast.Call")
                    mock_node = self._convert_ast_node(node)
                elif isinstance(node, ast.Attribute):
                    logger.info(f"MockParser: Processing ast.Attribute")
                    mock_node = self._convert_ast_node(node)
                elif isinstance(node, ast.Name):
                    logger.info(f"MockParser: Processing ast.Name")
                    mock_node = self._convert_ast_node(node)
                elif isinstance(node, ast.Return):
                    logger.info(f"MockParser: Processing ast.Return")
                    mock_node = self._convert_ast_node(node)
                elif isinstance(node, ast.Assign):
                    logger.info(f"MockParser: Processing ast.Assign")
                    mock_node = self._convert_ast_node(node)
                elif isinstance(node, ast.Expr):
                    logger.info(f"MockParser: Processing ast.Expr, descending into value")
                    return process_node(node.value)
                else:
                    logger.warning(f"MockParser: Unhandled node type {type(node).__name__} in process_node, processing children.")
                    for child in ast.iter_child_nodes(node):
                        child_node = process_node(child)
                        if child_node:
                            logger.info(f"MockParser: Appending child {child_node.type} from unhandled parent {type(node).__name__} to root")
                            root.children.append(child_node)
                    mock_node = None
                if mock_node:
                    logger.info(f"MockParser: Exiting process_node, returning MockNode of type {mock_node.type}")
                else:
                    logger.info(f"MockParser: Exiting process_node, returning None")
                return mock_node
            logger.info(f"MockParser: Starting loop over tree.body")
            for i, node in enumerate(tree.body):
                logger.info(f"MockParser: Processing body node {i} of type {type(node).__name__}")
                node_result = process_node(node)
                if node_result:
                    logger.info(f"MockParser: Appending node {node_result.type} from body to root")
                    root.children.append(node_result)
                else:
                    logger.info(f"MockParser: process_node returned None for body node {i} ({type(node).__name__})")
            logger.info(f"MockParser: Final root node children types: {[child.type for child in root.children]}")
            print(f"--- EXITING MockParser.parse NORMALLY ---")
            return MockTree(root)
        except Exception as e:
            print(f"--- EXITING MockParser.parse WITH ERROR: {e} ---")
            logger.exception(f"MockParser: Failed to parse code")
            return None
    def extract_symbols(self, tree: MockTree) -> Tuple[Dict[str, List[Dict[str, Any]]], List[Dict[str, Any]]]:
        """Extract symbols and references from the AST.
        Args:
            tree: The AST to extract symbols from
        Returns:
            A tuple of (symbols, references) where:
            - symbols is a dictionary mapping symbol types to lists of symbol information
            - references is a list of reference information dictionaries
        """
        symbols = {
            'imports': [],
            'functions': [],
            'classes': [],
            'variables': []
        }
        references = []
        current_scope = None
        def process_node(node: MockNode) -> None:
            nonlocal current_scope
            if node.type == 'import_statement':
                for child in node.children:
                    symbols['imports'].append({
                        'type': 'import',
                        'module': child.text,
                        'start_line': node.start_point[0],
                        'end_line': node.end_point[0]
                    })
            elif node.type == 'import_from_statement':
                module = next((child.text for child in node.children if child.type == 'dotted_name'), '')
                for child in node.children:
                    if child.type == 'identifier':
                        symbols['imports'].append({
                            'type': 'import',
                            'module': module,
                            'symbol': child.text,
                            'start_line': node.start_point[0],
                            'end_line': node.end_point[0]
                        })
            elif node.type == 'function_definition':
                name = next((child.text for child in node.children if child.type == 'name'), '')
                params = []
                for child in node.children:
                    if child.type == 'parameters':
                        for param in child.children:
                            if param.type == 'identifier':
                                params.append({
                                    'name': param.text,
                                    'start_line': param.start_point[0],
                                    'end_line': param.end_point[0]
                                })
                symbols['functions'].append({
                    'type': 'function',
                    'name': name,
                    'parameters': params,
                    'start_line': node.start_point[0] + 1,
                    'end_line': node.end_point[0]
                })
                old_scope = current_scope
                current_scope = name
                for child in node.children:
                    if child.type == 'body':
                        for child_node in child.children:
                            process_node(child_node)
                current_scope = old_scope
            elif node.type == 'class_definition':
                name = node.text
                methods = []
                bases = []
                for child in node.children:
                    if child.type == 'bases':
                        bases.extend([base.text for base in child.children if base.type == 'identifier'])
                    elif child.type == 'body':
                        for method in child.children:
                            if method.type == 'function_definition':
                                methods.append({
                                    'name': method.text,
                                    'start_line': method.start_point[0] + 1,
                                    'end_line': method.end_point[0]
                                })
                symbols['classes'].append({
                    'type': 'class',
                    'name': name,
                    'bases': bases,
                    'methods': methods,
                    'start_line': node.start_point[0] + 1,
                    'end_line': node.end_point[0]
                })
                old_scope = current_scope
                current_scope = name
                for child in node.children:
                    if child.type == 'body':
                        for child_node in child.children:
                            process_node(child_node)
                current_scope = old_scope
            elif node.type == 'call':
                references.append({
                    'type': 'call',
                    'name': node.text,
                    'scope': current_scope,
                    'start_line': node.start_point[0],
                    'end_line': node.end_point[0]
                })
            elif node.type == 'attribute':
                references.append({
                    'type': 'attribute',
                    'name': node.text.split('.')[-1],
                    'scope': node.text.split('.')[0],
                    'start_line': node.start_point[0],
                    'end_line': node.end_point[0]
                })
            for child in node.children:
                process_node(child)
        process_node(tree.root_node)
        return symbols, references
    def _convert_ast_to_mock_tree(self, node: ast.AST) -> MockTree:
        """Convert AST node to mock tree."""
        if isinstance(node, ast.Module):
            children = []
            for child in node.body:
                if isinstance(child, ast.Import):
                    children.append(self._convert_ast_node(child))
                elif isinstance(child, ast.ImportFrom):
                    children.append(self._convert_ast_node(child))
                elif isinstance(child, ast.FunctionDef):
                    children.append(self._convert_ast_node(child))
                elif isinstance(child, ast.ClassDef):
                    children.append(self._convert_ast_node(child))
                else:
                    children.append(self._convert_ast_node(child))
            root = MockNode('module', children=children)
        else:
            root = self._convert_ast_node(node)
        return MockTree(root)
    def _convert_ast_node(self, node: ast.AST) -> MockNode:
        """Convert AST node to mock node."""
        if isinstance(node, ast.Import):
            names = [name.name for name in node.names]
            return MockNode(
                type='import_statement', 
                text=f"import {', '.join(names)}", 
                start_point=(node.lineno - 1, 0), 
                end_point=(node.lineno - 1, len(f"import {', '.join(names)}")), 
                children=[MockNode('identifier', text=name) for name in names]
            )
        elif isinstance(node, ast.ImportFrom):
            module = node.module or ''
            level = node.level # Get the relative import level
            # Prepend dots for relative imports
            relative_prefix = '.' * level
            full_module_name = f"{relative_prefix}{module}"
            names = [name.name for name in node.names]
            # Create the module node with the full name including dots
            module_node = MockNode('dotted_name', text=full_module_name)
            return MockNode(
                type='import_from_statement', 
                text=f"from {full_module_name} import {', '.join(names)}", # Use full name in text too
                start_point=(node.lineno - 1, 0), 
                end_point=(node.lineno - 1, len(f"from {full_module_name} import {', '.join(names)}")), 
                children=[
                    module_node,
                    *[MockNode('identifier', text=name) for name in names]
                ]
            )
        elif isinstance(node, ast.FunctionDef):
            params = []
            for arg in node.args.args:
                params.append(MockNode('identifier', text=arg.arg, start_point=(node.lineno, 0), end_point=(node.lineno, len(arg.arg))))
            body_nodes = []
            for child in node.body:
                if isinstance(child, ast.Return):
                    if isinstance(child.value, ast.Call):
                        body_nodes.append(self._convert_ast_node(child.value))
                    else:
                        body_nodes.append(self._convert_ast_node(child))
                else:
                    body_nodes.append(self._convert_ast_node(child))
            return MockNode('function_definition', text=node.name, start_point=(node.lineno, 0), end_point=(node.end_lineno, 0), children=[
                MockNode('name', text=node.name),
                MockNode('parameters', children=params),
                MockNode('body', children=body_nodes)
            ])
        elif isinstance(node, ast.ClassDef):
            name_node = MockNode(type='identifier', text=node.name)
            bases = []
            for base in node.bases:
                if isinstance(base, ast.Name):
                    bases.append(MockNode(type='identifier', text=base.id))
                elif isinstance(base, ast.Attribute):
                    bases.append(MockNode(type='identifier', text=f"{base.value.id}.{base.attr}"))
            bases_node = MockNode(type='bases', children=bases)
            body = []
            for child in node.body:
                if isinstance(child, ast.FunctionDef):
                    body.append(self._convert_ast_node(child))
                elif isinstance(child, ast.Assign):
                    for target in child.targets:
                        if isinstance(target, ast.Name):
                            body.append(MockNode('attribute', text=target.id))
            body_node = MockNode(type='body', children=body)
            return MockNode(
                type='class_definition',
                text=node.name,
                fields={
                    'name': name_node,
                    'bases': bases_node,
                    'body': body_node,
                    'type': 'class',
                    'start_line': node.lineno,
                    'end_line': node.end_lineno or node.lineno,
                    'bases_list': [base.id for base in node.bases if isinstance(base, ast.Name)]
                },
                start_point=(node.lineno - 1, node.col_offset),
                end_point=(node.end_lineno or node.lineno, node.end_col_offset or 0),
                children=[name_node, bases_node, body_node]
            )
        elif isinstance(node, ast.Call):
            if isinstance(node.func, ast.Name):
                return MockNode('call', text=node.func.id, start_point=(node.lineno, 0), end_point=(node.lineno, len(node.func.id)))
            elif isinstance(node.func, ast.Attribute):
                return MockNode('call', text=f"{node.func.value.id}.{node.func.attr}", start_point=(node.lineno, 0), end_point=(node.lineno, len(f"{node.func.value.id}.{node.func.attr}")))
            else:
                return MockNode('call', text=ast.unparse(node.func), start_point=(node.lineno, 0), end_point=(node.lineno, len(ast.unparse(node.func))))
        elif isinstance(node, ast.Attribute):
            if isinstance(node.value, ast.Name):
                return MockNode('attribute', text=f"{node.value.id}.{node.attr}", start_point=(node.lineno, 0), end_point=(node.lineno, len(f"{node.value.id}.{node.attr}")))
            else:
                return MockNode('attribute', text=f"{ast.unparse(node.value)}.{node.attr}", start_point=(node.lineno, 0), end_point=(node.lineno, len(f"{ast.unparse(node.value)}.{node.attr}")))
        elif isinstance(node, ast.Name):
            return MockNode('identifier', text=node.id, start_point=(node.lineno, node.col_offset), end_point=(node.lineno, node.col_offset + len(node.id)))
        elif isinstance(node, ast.Return):
            if isinstance(node.value, ast.Call):
                return self._convert_ast_node(node.value)
            else:
                value_text = ast.unparse(node.value) if node.value else ''
                return MockNode('return', text=value_text, start_point=(node.lineno, 0), end_point=(node.lineno, len(value_text)))
        else:
            try:
                node_text = ast.unparse(node)
            except Exception:
                 node_text = f"[Unparse failed for {type(node).__name__}]"
            logger.warning(f"MockParser: Creating 'unknown' node for unhandled AST type: {type(node).__name__}")
            return MockNode('unknown', text=node_text)
</file>

<file path="server/code_understanding/module_resolver.py">
"""Module for resolving JavaScript module paths and dependencies."""
import os
from pathlib import Path
from typing import Optional, List, Set, Dict, Any
import re
class ModuleResolver:
    """Resolves JavaScript module paths and dependencies."""
    def __init__(self, root_dir: str):
        """Initialize the module resolver.
        Args:
            root_dir: Root directory of the project
        """
        self.root_dir = Path(root_dir)
        self.module_cache: Dict[str, Dict[str, Any]] = {}
        self.path_cache: Dict[str, Path] = {}
    def resolve_import(self, import_path: str, from_file: str) -> Optional[Path]:
        """Resolve an import path to its actual file location.
        Args:
            import_path: Import path to resolve
            from_file: File containing the import
        Returns:
            Resolved file path or None if not found
        """
        # Handle package imports (e.g., 'react', 'lodash')
        if not import_path.startswith('.'):
            return self._resolve_package_import(import_path)
        # Handle relative imports
        from_path = Path(from_file)
        if not from_path.is_absolute():
            from_path = self.root_dir / from_path
        # Try different extensions
        extensions = ['.js', '.jsx', '.ts', '.tsx']
        base_path = from_path.parent / import_path
        # Try exact path first
        if base_path.exists():
            return base_path
        # Try with extensions
        for ext in extensions:
            path = base_path.with_suffix(ext)
            if path.exists():
                return path
        # Try index files
        for ext in extensions:
            path = base_path / f'index{ext}'
            if path.exists():
                return path
        return None
    def _resolve_package_import(self, package_name: str) -> Optional[Path]:
        """Resolve a package import to its location.
        Args:
            package_name: Name of the package to resolve
        Returns:
            Package location or None if not found
        """
        # Check node_modules in project root
        node_modules = self.root_dir / 'node_modules'
        if node_modules.exists():
            package_dir = node_modules / package_name
            if package_dir.exists():
                return package_dir
        # Check package.json for local packages
        package_json = self.root_dir / 'package.json'
        if package_json.exists():
            try:
                import json
                with open(package_json) as f:
                    pkg = json.load(f)
                    if package_name in pkg.get('dependencies', {}):
                        return node_modules / package_name
            except Exception:
                pass
        return None
    def get_module_dependencies(self, file_path: str) -> Dict[str, List[str]]:
        """Get all dependencies for a module.
        Args:
            file_path: Path to the module
        Returns:
            Dictionary containing direct and transitive dependencies
        """
        if file_path in self.module_cache:
            return self.module_cache[file_path]
        dependencies = {
            'direct': [],
            'transitive': set()
        }
        # Read file content
        try:
            with open(file_path) as f:
                content = f.read()
        except Exception:
            return dependencies
        # Find all imports
        import_patterns = [
            r'import\s+.*?from\s+[\'"]([^\'"]+)[\'"]',  # ES6 imports
            r'require\s*\(\s*[\'"]([^\'"]+)[\'"]',      # CommonJS requires
            r'import\s*\(\s*[\'"]([^\'"]+)[\'"]'        # Dynamic imports
        ]
        for pattern in import_patterns:
            for match in re.finditer(pattern, content):
                import_path = match.group(1)
                resolved_path = self.resolve_import(import_path, file_path)
                if resolved_path:
                    rel_path = str(resolved_path.relative_to(self.root_dir))
                    dependencies['direct'].append(rel_path)
                    # Get transitive dependencies
                    if rel_path not in self.module_cache:
                        trans_deps = self.get_module_dependencies(rel_path)
                        dependencies['transitive'].update(trans_deps['direct'])
                        dependencies['transitive'].update(trans_deps['transitive'])
        # Cache the results
        self.module_cache[file_path] = {
            'direct': dependencies['direct'],
            'transitive': list(dependencies['transitive'])
        }
        return self.module_cache[file_path]
    def get_module_graph(self) -> Dict[str, Any]:
        """Generate a complete module dependency graph.
        Returns:
            Dictionary containing nodes and edges of the module graph
        """
        graph = {
            'nodes': [],
            'edges': []
        }
        # Find all JavaScript files
        js_files = []
        for ext in ['.js', '.jsx', '.ts', '.tsx']:
            js_files.extend(self.root_dir.rglob(f'*{ext}'))
        # Add nodes for all files
        for file_path in js_files:
            rel_path = str(file_path.relative_to(self.root_dir))
            graph['nodes'].append({
                'id': rel_path,
                'type': 'module',
                'dependencies': self.get_module_dependencies(rel_path)
            })
        # Add edges for dependencies
        for node in graph['nodes']:
            for dep in node['dependencies']['direct']:
                graph['edges'].append({
                    'from': node['id'],
                    'to': dep,
                    'type': 'import'
                })
        return graph
    def find_circular_dependencies(self) -> List[List[str]]:
        """Find circular dependencies in the module graph.
        Returns:
            List of circular dependency chains
        """
        graph = self.get_module_graph()
        cycles = []
        visited = set()
        path = []
        def dfs(node: str, current_path: List[str]):
            if node in current_path:
                cycle_start = current_path.index(node)
                cycles.append(current_path[cycle_start:])
                return
            if node in visited:
                return
            visited.add(node)
            current_path.append(node)
            for edge in graph['edges']:
                if edge['from'] == node:
                    dfs(edge['to'], current_path.copy())
        for node in graph['nodes']:
            if node['id'] not in visited:
                dfs(node['id'], [])
        return cycles
    def get_module_stats(self) -> Dict[str, Any]:
        """Get statistics about the module system.
        Returns:
            Dictionary containing various module statistics
        """
        graph = self.get_module_graph()
        stats = {
            'total_modules': len(graph['nodes']),
            'total_dependencies': len(graph['edges']),
            'circular_dependencies': len(self.find_circular_dependencies()),
            'module_types': {},
            'dependency_counts': []
        }
        # Count module types
        for node in graph['nodes']:
            ext = Path(node['id']).suffix
            stats['module_types'][ext] = stats['module_types'].get(ext, 0) + 1
        # Calculate dependency counts
        dep_counts = {}
        for node in graph['nodes']:
            count = len(node['dependencies']['direct'])
            dep_counts[count] = dep_counts.get(count, 0) + 1
        stats['dependency_counts'] = [
            {'dependencies': count, 'modules': num}
            for count, num in sorted(dep_counts.items())
        ]
        return stats
</file>

<file path="server/code_understanding/parser.py">
"""Module for parsing Python code."""
import ast
import logging
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple, Union, Any, Iterator
from pathlib import Path
from .language_adapters import JavaScriptParserAdapter, SwiftParserAdapter
from .common_types import MockNode, MockTree
logger = logging.getLogger(__name__)
# Names to ignore when processing symbols
IGNORED_NAMES = {'self', 'cls', 'None', 'True', 'False'}
class CodeParser:
    """Parser for Python code."""
    def __init__(self):
        """Initialize the parser and load language support."""
        # Initialize ALL attributes first
        self.py_language = None
        self.py_parser = None
        self.adapters: Dict[str, Any] = {}
        self.mock_parser = None # Initialize mock_parser early
        # Now load parsers and adapters
        self._try_load_py_parser()
        self._try_load_adapters()
        # Ensure mock is set up if Python parser failed during _try_load_py_parser
        # This check might now be redundant if _try_load_py_parser handles it,
        # but it's safe to leave for robustness or remove if confident.
        if not self.py_parser or not self.py_language:
             self._setup_mock_parser_fallback()
    def _try_load_py_parser(self):
        """Try to load the Tree-sitter Python parser."""
        try:
            from tree_sitter import Parser, Language
            self.py_parser = Parser()
            # Assuming build_languages put python.so in the right place
            try:
                # Adjust path as needed, e.g., using build/python.so or vendor
                py_lang_path = Path(__file__).parent / 'build' / 'python.so' 
                if py_lang_path.exists():
                    self.py_language = Language(str(py_lang_path), 'python')
                    self.py_parser.set_language(self.py_language)
                    logger.info("Tree-sitter Python parser loaded successfully.")
                else:
                    logger.warning("Python tree-sitter grammar not found at expected location.")
                    self._setup_mock_parser_fallback() # Setup mock if python fails
            except Exception as e:
                logger.warning(f"Failed to load Python Tree-sitter grammar: {e}")
                self._setup_mock_parser_fallback()
        except ImportError:
            logger.warning("tree_sitter library not found.")
            self._setup_mock_parser_fallback()
        except Exception as e:
            logger.warning(f"Error initializing Python parser: {e}")
            self._setup_mock_parser_fallback()
    def _try_load_adapters(self):
        """Try to load adapters for other languages."""
        try:
            js_adapter = JavaScriptParserAdapter()
            if js_adapter.language: # Check if language loaded successfully
                 self.adapters['javascript'] = js_adapter
                 logger.info("JavaScriptParserAdapter loaded successfully.")
            else:
                 logger.warning("JavaScriptParserAdapter initialized but language failed to load.")
        except Exception as e:
            logger.error(f"Failed to initialize JavaScriptParserAdapter: {e}")
        try:
            # Add Swift or other adapters similarly
            # swift_adapter = SwiftParserAdapter()
            # self.adapters['swift'] = swift_adapter
            # logger.info("SwiftParserAdapter loaded.")
            pass
        except Exception as e:
            logger.error(f"Failed to initialize other language adapters: {e}")
    def _setup_mock_parser_fallback(self):
        """Use a mock parser for testing or when tree-sitter fails."""
        if not self.mock_parser:
             try:
                 from .mock_parser import MockParser
                 self.mock_parser = MockParser()
                 logger.info("Initialized MockParser fallback.")
             except ImportError:
                  logger.error("Failed to import MockParser for fallback.")
                  self.mock_parser = None # Ensure it's None
    def parse(self, code: str, language: str = 'python') -> Optional[MockTree]:
        """Parse code using the appropriate parser based on language.
        Args:
            code: Source code string.
            language: The programming language (e.g., 'python', 'javascript').
        Returns:
            MockTree: A unified abstract syntax tree representation, or None on failure.
        """
        selected_parser = None
        is_adapter = False
        if language == 'python':
            if self.py_parser and self.py_language:
                 selected_parser = self.py_parser
            elif self.mock_parser: # Use mock parser if Python tree-sitter failed
                 logger.warning("Using MockParser for Python due to Tree-sitter load failure.")
                 selected_parser = self.mock_parser
                 is_adapter = True # MockParser has a parse() -> MockTree method
            else:
                 logger.error("No Python parser (Tree-sitter or Mock) available.")
                 return None
        elif language in self.adapters:
            selected_parser = self.adapters[language]
            is_adapter = True # Adapters (and MockParser) return MockTree directly
            logger.info(f"Using {type(selected_parser).__name__} for language: {language}")
        else:
             # Optional: Fallback to mock parser for unsupported languages if desired?
             # if self.mock_parser:
             #    logger.warning(f"Language '{language}' not supported by specific adapters, attempting MockParser fallback.")
             #    selected_parser = self.mock_parser
             #    is_adapter = True
             # else:
             logger.error(f"No parser adapter found for language: {language}")
             return None
        if not selected_parser:
             logger.error(f"Parser selection failed for language: {language}")
             return None
        # Pre-parsing validation block
        try:
            # Validate syntax using Python's ast (ONLY for Python)
            if language == 'python':
                ast.parse(code)
            # No pre-validation for other languages currently
        except SyntaxError as e:
            logger.error(f"Syntax error detected by pre-parser validation for {language}: {e}")
            raise ValueError(f"Invalid {language} code: {e}") # Reraise for analyzer
        except Exception as e:
            logger.warning(f"Pre-parse validation step failed for {language}: {e}")
            # Proceeding, assuming the actual parser might handle it
        # Actual parsing block
        try:
            if is_adapter:
                # Ensure the selected parser (adapter) is actually called
                logger.debug(f"Calling {type(selected_parser).__name__}.parse() for {language}")
                return selected_parser.parse(code)
            else: # Assume Tree-sitter parser for Python
                 # This block should only execute if language == 'python' and py_parser is valid
                 logger.debug(f"Calling tree-sitter parse() for {language}")
                 if isinstance(code, str):
                     code_bytes = bytes(code, 'utf8')
                 else:
                      code_bytes = code # Assume bytes if not str
                 tree = selected_parser.parse(code_bytes)
                 # Convert tree-sitter tree to MockTree
                 mock_root = self._tree_sitter_to_mock_tree(tree)
                 return MockTree(root=mock_root)
        except Exception as e:
            logger.exception(f"Parsing failed with {type(selected_parser).__name__} for {language}: {e}")
            return None # Return None on parsing failure
    def _tree_sitter_to_mock_tree(self, tree: Any) -> Any:
         """Convert tree-sitter tree to mock node structure (root)."""
         # This needs careful implementation to map TS nodes to MockNodes
         # It was previously returning the root node directly, ensure it returns MockNode
         # Placeholder - reuse the recursive helper concept
         return self._convert_ts_node_recursive(tree.root_node)
    def _convert_ts_node_recursive(self, node):
        """Recursive helper to convert tree-sitter node to MockNode."""
        if not node:
            return None
        text = node.text.decode('utf8') if hasattr(node.text, 'decode') else str(node.text)
        # Basic type mapping - should be language specific? Adapter handles this now.
        # Keep a generic mapping here?
        mock_type = node.type
        mock_node = MockNode(
            type=mock_type,
            text=text,
            start_point=node.start_point,
            end_point=node.end_point,
            children=[],
            fields={}
        )
        # Process children recursively
        for field_name, child_node in node.children_by_field_name().items():
             # How Tree-sitter handles fields vs children needs clarification
             # Assuming children_by_field_name gives named children
             mock_child = self._convert_ts_node_recursive(child_node)
             if mock_child:
                  mock_node.fields[field_name] = mock_child
                  # Should they also be in children list? Depends on MockNode usage.
                  # mock_node.children.append(mock_child) 
        for child_node in node.children: # Iterate unnamed children? Check tree-sitter API
             mock_child = self._convert_ts_node_recursive(child_node)
             if mock_child:
                 # Avoid duplicating named children if they are also in .children
                 if field_name not in mock_node.fields or mock_node.fields[field_name] != mock_child:
                      mock_node.children.append(mock_child)
        return mock_node
    def _mock_parse(self, code: str) -> MockTree:
        """Create a mock syntax tree for testing.
        Args:
            code: Python source code
        Returns:
            MockTree object
        """
        tree = ast.parse(code)
        root = self._ast_to_mock_node(tree)
        return MockTree(root)
    def _ast_to_mock_node(self, node: ast.AST) -> Union[MockNode, List[MockNode]]:
        """Convert AST node to mock node.
        Args:
            node: AST node
        Returns:
            MockNode object or list of MockNode objects
        """
        if isinstance(node, ast.Module):
            children = []
            for child in node.body:
                child_node = self._ast_to_mock_node(child)
                if isinstance(child_node, list):
                    children.extend(child_node)
                else:
                    children.append(child_node)
            return MockNode(type='module', children=children)
        elif isinstance(node, ast.FunctionDef):
            name_node = MockNode(type='identifier', text=node.name)
            return MockNode(
                type='function_definition',
                text=node.name,
                fields={'name': name_node},
                start_point=(node.lineno - 1, node.col_offset),
                end_point=(node.end_lineno or node.lineno, node.end_col_offset or 0)
            )
        elif isinstance(node, ast.ClassDef):
            name_node = MockNode(type='identifier', text=node.name)
            return MockNode(
                type='class_definition',
                text=node.name,
                fields={'name': name_node},
                start_point=(node.lineno - 1, node.col_offset),
                end_point=(node.end_lineno or node.lineno, node.end_col_offset or 0)
            )
        elif isinstance(node, ast.Assign):
            if len(node.targets) == 1 and isinstance(node.targets[0], ast.Name):
                left_node = MockNode(type='identifier', text=node.targets[0].id)
                if isinstance(node.value, ast.Constant) and isinstance(node.value.value, str):
                    right_text = repr(node.value.value)
                else:
                    right_text = repr(node.value)
                right_node = MockNode(type='string', text=right_text)
                return MockNode(
                    type='assignment',
                    text=f"{node.targets[0].id} = {right_text}",
                    fields={'left': left_node, 'right': right_node},
                    start_point=(node.lineno - 1, node.col_offset),
                    end_point=(node.end_lineno or node.lineno, node.end_col_offset or 0)
                )
        elif isinstance(node, ast.Import):
            return MockNode(
                type='import',
                text=f"import {', '.join(name.name for name in node.names)}",
                start_point=(node.lineno - 1, node.col_offset),
                end_point=(node.end_lineno or node.lineno, node.end_col_offset or 0)
            )
        elif isinstance(node, ast.ImportFrom):
            imports = []
            for name in node.names:
                imports.append(MockNode(
                    type='import',
                    text=f"from {node.module} import {name.name}",
                    start_point=(node.lineno - 1, node.col_offset),
                    end_point=(node.end_lineno or node.lineno, node.end_col_offset or 0)
                ))
            return imports
        return MockNode(type='unknown')
    def _value_to_mock_node(self, node: ast.AST) -> MockNode:
        """Convert value node to mock node.
        Args:
            node: AST node
        Returns:
            MockNode object
        """
        if isinstance(node, ast.Constant):
            if isinstance(node.value, str):
                node_type = 'string'
            elif isinstance(node.value, int):
                node_type = 'integer'
            elif isinstance(node.value, float):
                node_type = 'float'
            elif isinstance(node.value, bool):
                node_type = 'true' if node.value else 'false'
            elif node.value is None:
                node_type = 'none'
            else:
                node_type = 'unknown'
        elif isinstance(node, ast.List):
            node_type = 'list'
        elif isinstance(node, ast.Dict):
            node_type = 'dictionary'
        elif isinstance(node, ast.Tuple):
            node_type = 'tuple'
        else:
            node_type = 'unknown'
        return MockNode(
            type=node_type,
            text=ast.unparse(node) if hasattr(ast, 'unparse') else str(node),
            start_point=(node.lineno - 1, node.col_offset),
            end_point=(node.end_lineno - 1, node.end_col_offset)
        )
    def extract_symbols(self, tree: Union[Any, MockTree]) -> Tuple[Dict[str, List[Dict[str, Any]]], Dict[str, List[Dict[str, Any]]]]:
        """Extract symbols and references from a syntax tree.
        Args:
            tree: Syntax tree
        Returns:
            Tuple of (symbols, references)
        """
        symbols = {
            'imports': [],
            'functions': [],
            'classes': [],
            'variables': []
        }
        references = {
            'imports': [],
            'calls': [],
            'attributes': [],
            'variables': []
        }
        if isinstance(tree, MockTree):
            self._extract_from_mock_tree(tree.root_node, symbols, references)
        else:
            self._extract_from_tree_sitter(tree, symbols, references)
        return symbols, references
    def _extract_from_mock_tree(self, node: MockNode, symbols: Dict[str, List[Dict[str, Any]]], references: Dict[str, List[Dict[str, Any]]]):
        """Extract symbols and references from a mock tree.
        Args:
            node: The root node
            symbols: Dictionary to store symbol information
            references: Dictionary to store reference information
        """
        if not node:
            return
        try:
            # Process imports
            if node.type == 'import_statement':
                # Get the module name and alias from the children
                module_name = None
                alias_name = None
                for child in node.children:
                    if child.type == 'identifier':
                        if not module_name:
                            module_name = child.text
                        else:
                            alias_name = child.text
                if module_name:
                    symbols['imports'].append({
                        'module': module_name,
                        'symbol': '',
                        'alias': alias_name,
                        'start_line': node.start_point[0],
                        'end_line': node.end_point[0]
                    })
            elif node.type == 'import_from_statement':
                # Get the module name and symbols from the children
                module_name = None
                for child in node.children:
                    if child.type == 'dotted_name':
                        module_name = child.text
                        break
                if module_name:
                    for child in node.children:
                        if child.type == 'identifier':
                            symbols['imports'].append({
                                'module': module_name,
                                'symbol': child.text,
                                'alias': None,
                                'start_line': node.start_point[0],
                                'end_line': node.end_point[0]
                            })
            # Process functions and methods
            elif node.type == 'function_definition':
                # Get the function name from the identifier
                func_name = None
                for child in node.children:
                    if child.type == 'identifier':
                        func_name = child.text
                        break
                if func_name:
                    # Get parameters
                    parameters = []
                    for child in node.children:
                        if child.type == 'parameters':
                            for param_node in child.children:
                                if param_node.type == 'identifier':
                                    parameters.append({
                                        'name': param_node.text,
                                        'start_line': param_node.start_point[0],
                                        'end_line': param_node.end_point[0]
                                    })
                    # Check if this is a method (inside a class)
                    parent = node.parent
                    while parent:
                        if parent.type == 'class_definition':
                            # Skip processing here since it will be handled in class_definition
                            break
                        parent = parent.parent
                    else:
                        # Only add as a function if not inside a class
                        symbols['functions'].append({
                            'name': func_name,
                            'parameters': parameters,
                            'start_line': node.start_point[0],
                            'end_line': node.end_point[0]
                        })
            # Process classes
            elif node.type == 'class_definition':
                # Get the class name from the name field
                name_node = node.fields.get('name')
                class_name = name_node.text if name_node else None
                if class_name:
                    # Get base classes from the bases field
                    bases = []
                    bases_node = node.fields.get('bases')
                    if bases_node:
                        for base_node in bases_node.children:
                            if base_node.type == 'identifier':
                                bases.append(base_node.text)
                    # Create class info
                    class_info = {
                        'name': class_name,
                        'bases': bases,
                        'methods': [],
                        'start_line': node.start_point[0],
                        'end_line': node.end_point[0]
                    }
                    symbols['classes'].append(class_info)
                    # Process all methods in the class
                    body_node = node.fields.get('body')
                    if body_node:
                        for method_node in body_node.children:
                            if method_node.type == 'function_definition':
                                method_name_node = method_node.fields.get('name')
                                method_name = method_name_node.text if method_name_node else None
                                if method_name:
                                    parameters = []
                                    params_node = method_node.fields.get('parameters')
                                    if params_node:
                                        for param_node in params_node.children:
                                            if param_node.type == 'identifier':
                                                parameters.append({
                                                    'name': param_node.text,
                                                    'start_line': param_node.start_point[0],
                                                    'end_line': param_node.end_point[0]
                                                })
                                    class_info['methods'].append({
                                        'name': method_name,
                                        'parameters': parameters,
                                        'start_line': method_node.start_point[0],
                                        'end_line': method_node.end_point[0]
                                    })
            # Process function calls
            elif node.type == 'call':
                # Get the function name from the identifier
                func_name = None
                for child in node.children:
                    if child.type == 'identifier':
                        func_name = child.text
                        break
                if func_name:
                    references['calls'].append({
                        'name': func_name,
                        'start_line': node.start_point[0],
                        'end_line': node.end_point[0]
                    })
            # Process variable references
            elif node.type == 'assignment':
                left_node = node.fields.get('left')
                right_node = node.fields.get('right')
                if left_node:
                    var_name = left_node.text
                    value_type = right_node.type if right_node else 'unknown'
                    if value_type == 'string':
                        value_type = 'str'
                    symbols['variables'].append({
                        'name': var_name,
                        'type': value_type,
                        'start_line': node.start_point[0],
                        'end_line': node.end_point[0]
                    })
            elif node.type == 'identifier':
                if node.text and node.text not in IGNORED_NAMES:
                    # Skip type hints
                    if node.text not in ('str', 'int', 'float', 'bool', 'list', 'dict', 'tuple', 'set', 'Optional', 'List', 'Dict', 'Tuple', 'Set'):
                        references['variables'].append({
                            'name': node.text,
                            'start_line': node.start_point[0],
                            'end_line': node.end_point[0]
                        })
            # Process children
            for child in node.children:
                self._extract_from_mock_tree(child, symbols, references)
        except Exception as e:
            logger.warning(f"Error extracting symbols from node {node.type}: {e}")
            raise
    def _extract_from_tree_sitter(self, tree: Any, symbols: Dict[str, List[dict]], references: Dict[str, List[dict]]) -> None:
        """Extract symbols from tree-sitter tree.
        Args:
            tree: Tree-sitter tree
            symbols: Dictionary to store symbols
            references: Dictionary to store references
        """
        # Similar to _extract_from_mock_tree but using tree-sitter nodes
        # This will be implemented when tree-sitter is properly integrated
        pass
    def get_root_node(self, tree: Union[Any, MockTree]) -> Union[Any, MockNode]:
        """Get the root node of a tree.
        Args:
            tree: Syntax tree
        Returns:
            Root node of the tree
        Raises:
            ValueError: If tree is None
        """
        if tree is None:
            return None
        return tree.root_node
    def node_to_dict(self, node: Union[Any, MockNode]) -> Dict[str, Any]:
        """Convert a node to a dictionary representation."""
        if node is None:
            return {}
        result = {
            'type': node.type,
            'text': node.text.decode('utf-8') if isinstance(node.text, bytes) else str(node.text),
            'start_point': node.start_point,
            'end_point': node.end_point,
            'children': []
        }
        for child in node.children:
            result['children'].append(self.node_to_dict(child))
        return result
</file>

<file path="server/code_understanding/relationship_extractor.py">
"""Module for extracting relationships between JavaScript code elements."""
from typing import Dict, List, Set, Any, Optional
from pathlib import Path
from .javascript_parser import JavaScriptParser
from .module_resolver import ModuleResolver
class JavaScriptRelationshipExtractor:
    """Extracts relationships between JavaScript code elements."""
    def __init__(self, root_dir: str):
        """Initialize the relationship extractor.
        Args:
            root_dir: Root directory of the project
        """
        self.root_dir = Path(root_dir)
        self.parser = JavaScriptParser()
        self.module_resolver = ModuleResolver(root_dir)
        self.import_mapping: Dict[str, Dict[str, str]] = {}
        self.export_mapping: Dict[str, Dict[str, str]] = {}
        self.symbol_table: Dict[str, Dict[str, Any]] = {}
    def analyze_file(self, file_path: str, content: str) -> Dict[str, Any]:
        """Analyze a JavaScript file and extract relationships.
        Args:
            file_path: Path to the file
            content: File contents
        Returns:
            Dictionary containing analysis results
        """
        # Parse the file
        tree = self.parser.parse(content)
        if not tree:
            return {
                'error': 'Failed to parse file',
                'imports': {},
                'exports': {},
                'symbols': {},
                'relationships': []
            }
        # Extract imports and exports
        imports = self.parser.get_imports(tree)
        exports = self.parser.get_exports(tree)
        requires = self.parser.get_requires(tree)
        # Process imports and exports
        self._process_imports_exports(file_path, imports, exports, requires)
        # Extract symbols
        symbols = self.parser.get_symbols(tree)
        self.symbol_table[file_path] = {
            s['name']: s['type'] for s in symbols
        }
        # Build relationships
        relationships = self._build_relationships(tree, file_path)
        return {
            'imports': self.import_mapping.get(file_path, {}),
            'exports': self.export_mapping.get(file_path, {}),
            'symbols': self.symbol_table.get(file_path, {}),
            'relationships': relationships
        }
    def _process_imports_exports(self, file_path: str, imports: List[Dict[str, Any]],
                               exports: List[Dict[str, Any]], requires: List[Dict[str, Any]]):
        """Process imports and exports in the file.
        Args:
            file_path: Path to the file
            imports: List of import information
            exports: List of export information
            requires: List of require information
        """
        # Process imports
        self.import_mapping[file_path] = {}
        for imp in imports:
            source = imp.get('source')
            if source:
                resolved_path = self.module_resolver.resolve_import(source, file_path)
                if resolved_path:
                    self.import_mapping[file_path][source] = str(resolved_path)
        # Process exports
        self.export_mapping[file_path] = {}
        for exp in exports:
            if 'source' in exp:
                source = exp['source']
                resolved_path = self.module_resolver.resolve_import(source, file_path)
                if resolved_path:
                    self.export_mapping[file_path][source] = str(resolved_path)
            elif 'name' in exp:
                self.export_mapping[file_path][exp['name']] = file_path
        # Process requires
        for req in requires:
            source = req.get('source')
            if source:
                resolved_path = self.module_resolver.resolve_import(source, file_path)
                if resolved_path:
                    self.import_mapping[file_path][source] = str(resolved_path)
    def _build_relationships(self, tree: Any, file_path: str) -> List[Dict[str, Any]]:
        """Build relationships between code elements.
        Args:
            tree: AST root node
            file_path: Path to the file
        Returns:
            List of relationships
        """
        relationships = []
        # Add import relationships
        for source, resolved_path in self.import_mapping.get(file_path, {}).items():
            relationships.append({
                'type': 'import',
                'from': file_path,
                'to': resolved_path,
                'source': source
            })
        # Add export relationships
        for name, resolved_path in self.export_mapping.get(file_path, {}).items():
            relationships.append({
                'type': 'export',
                'from': file_path,
                'to': resolved_path,
                'name': name
            })
        # Add symbol relationships
        for name, type_info in self.symbol_table.get(file_path, {}).items():
            relationships.append({
                'type': 'symbol',
                'from': file_path,
                'to': name,
                'symbol_type': type_info
            })
        return relationships
    def get_cross_file_references(self, file_path: str) -> Dict[str, List[str]]:
        """Get cross-file references for a file.
        Args:
            file_path: Path to the file
        Returns:
            Dictionary containing outgoing and incoming references
        """
        return {
            'outgoing': list(self.import_mapping.get(file_path, {}).values()),
            'incoming': [
                f for f, imports in self.import_mapping.items()
                if file_path in imports.values()
            ]
        }
    def get_module_graph(self) -> Dict[str, Any]:
        """Get the module dependency graph.
        Returns:
            Dictionary containing nodes and edges of the graph
        """
        graph = {
            'nodes': [],
            'edges': []
        }
        # Add nodes
        for file_path in set(self.import_mapping.keys()) | set(self.export_mapping.keys()):
            graph['nodes'].append({
                'id': file_path,
                'type': 'module'
            })
        # Add edges
        for file_path, imports in self.import_mapping.items():
            for source, resolved_path in imports.items():
                graph['edges'].append({
                    'from': file_path,
                    'to': resolved_path,
                    'type': 'import'
                })
        return graph
</file>

<file path="server/code_understanding/relationships.py">
"""Module for building code relationships."""
import os
import logging
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Set, Any, Union
import ast
from .parser import CodeParser
from .extractor import SymbolExtractor
from .graph import Graph, Node, NodeType, RelationType
logger = logging.getLogger(__name__)
# Constants
IGNORED_NAMES = {'self', 'cls'}
@dataclass
class FileContext:
    """Context for a file being analyzed."""
    def __init__(self, path: str, code: Optional[str] = None, tree: Optional[Any] = None, symbols: Optional[Dict[str, Any]] = None, references: Optional[Dict[str, Any]] = None) -> None:
        """Initialize file context.
        Args:
            path: Path to the file
            code: Original code string
            tree: AST tree
            symbols: Dictionary of symbols extracted from the file
            references: Dictionary of references extracted from the file
        """
        self.path = path
        self.code = code
        self.tree = tree
        self.symbols = symbols or {
            'imports': [],
            'functions': [],
            'classes': [],
            'variables': []
        }
        self.references = references or {
            'imports': [],
            'calls': [],
            'attributes': [],
            'variables': []
        }
class RelationshipBuilder:
    """Builds relationship graphs from code analysis results."""
    def __init__(self):
        """Initialize the relationship builder."""
        self.graph = Graph()
        self.current_file_node = None
        self.parser = None
        self.file_contexts = {}
        self.ignored_names = {'self', 'cls', 'super', 'object', 'type', 'None', 'True', 'False'}
        self.extractor = SymbolExtractor()
    def analyze_file(self, file_path: str, code: Optional[str] = None) -> None:
        """Analyze a single file and build its relationships.
        Args:
            file_path: Path to the file to analyze
            code: Optional code string to analyze instead of reading from file
        """
        try:
            # Read code from file if not provided
            if code is None:
                if not os.path.exists(file_path):
                    raise FileNotFoundError(f"File not found: {file_path}")
                with open(file_path, 'r') as f:
                    code = f.read()
            # Parse the code
            tree = self.parser.parse(code)
            if not tree:
                return
            # Extract symbols
            symbols, references = self.parser.extract_symbols(tree)
            # Create file context
            context = FileContext(
                path=file_path,
                code=code,
                tree=tree,
                symbols=symbols
            )
            # Store file context
            self.file_contexts[file_path] = context
            # Create file node
            file_node = self.graph.find_or_create_node(
                name=file_path,
                type=NodeType.MODULE,
                properties={'file_path': file_path}
            )
            # Set current file node
            self.current_file_node = file_node
            # Process relationships
            self._process_imports(context)
            self._process_classes(context)
            self._process_functions(context)
            self._process_references(context, references)
        except Exception as e:
            logger.error(f"Error analyzing file {file_path}: {str(e)}")
            raise
    def analyze_directory(self, directory: str) -> None:
        """Analyze all Python files in a directory.
        Args:
            directory: Directory path
        Raises:
            FileNotFoundError: If the directory doesn't exist
        """
        if not os.path.exists(directory):
            raise FileNotFoundError(f"Directory not found: {directory}")
        try:
            for root, _, files in os.walk(directory):
                for file in files:
                    if file.endswith('.py'):
                        file_path = os.path.join(root, file)
                        self.analyze_file(file_path)
        except Exception as e:
            logger.error(f"Failed to analyze directory {directory}: {str(e)}")
            raise
    def get_relationships(self) -> Graph:
        """Get the relationship graph.
        Returns:
            Graph object containing relationships
        """
        return self.graph
    def clear(self) -> None:
        """Clear all analysis data."""
        self.graph.clear()
        self.file_contexts.clear()
    def _process_imports(self, context: Union[FileContext, List[Dict[str, Any]]]) -> None:
        """Process import statements.
        Args:
            context: Either a FileContext or a list of import information dictionaries
        """
        if isinstance(context, FileContext):
            if not context.symbols or 'imports' not in context.symbols:
                return
            imports = context.symbols['imports']
        else:
            imports = context
        if not isinstance(imports, list) or not imports:
            return
        for imp in imports:
            if not isinstance(imp, dict):
                continue
            # Create module node
            module = imp.get('module')
            if not module:
                continue
            module_node = self.graph.find_or_create_node(
                name=module,
                type=NodeType.MODULE,
                properties={
                    'file_path': module,
                    'start_line': imp.get('start_line', 0),
                    'end_line': imp.get('end_line', 0)
                }
            )
            # Add edge from current file to module
            if self.current_file_node:
                self.graph.create_edge(
                    from_node=self.current_file_node,
                    to_node=module_node,
                    type=RelationType.IMPORTS,
                    properties={
                        'start_line': imp.get('start_line', 0),
                        'end_line': imp.get('end_line', 0)
                    }
                )
            # Process imported symbols
            if 'symbol' in imp:
                symbol = imp['symbol']
                alias = imp.get('alias', symbol)
                symbol_node = self.graph.find_or_create_node(
                    name=f"{module}:{symbol}",
                    type=NodeType.SYMBOL,
                    properties={
                        'file_path': module,
                        'start_line': imp.get('start_line', 0),
                        'end_line': imp.get('end_line', 0)
                    }
                )
                # Add edge from module to symbol
                self.graph.create_edge(
                    from_node=module_node,
                    to_node=symbol_node,
                    type=RelationType.CONTAINS,
                    properties={
                        'start_line': imp.get('start_line', 0),
                        'end_line': imp.get('end_line', 0)
                    }
                )
                # Add edge from current file to symbol
                if self.current_file_node:
                    self.graph.create_edge(
                        from_node=self.current_file_node,
                        to_node=symbol_node,
                        type=RelationType.IMPORTS,
                        properties={
                            'start_line': imp.get('start_line', 0),
                            'end_line': imp.get('end_line', 0),
                            'alias': alias
                        }
                    )
    def _process_classes(self, context: FileContext) -> None:
        """Process class definitions.
        Args:
            context: FileContext containing class information
        """
        if not context.symbols or 'classes' not in context.symbols:
            return
        for class_info in context.symbols['classes']:
            if not isinstance(class_info, dict) or 'name' not in class_info:
                logger.debug(f"Invalid class info in {context.path}: {class_info}")
                continue
            logger.debug(f"Processing class {class_info['name']} in {context.path}")
            # Create class node
            class_node = self.graph.find_or_create_node(
                name=f"{context.path}:{class_info['name']}",
                type=NodeType.CLASS,
                properties={
                    'file_path': context.path,
                    'start_line': class_info.get('start_line', 0),
                    'end_line': class_info.get('end_line', 0)
                }
            )
            # Create contains edge from file to class
            if self.current_file_node:
                self.graph.create_edge(
                    from_node=self.current_file_node,
                    to_node=class_node,
                    type=RelationType.CONTAINS,
                    properties={
                        'start_line': class_info.get('start_line', 0),
                        'end_line': class_info.get('end_line', 0)
                    }
                )
            # Process inheritance
            if 'bases' in class_info and isinstance(class_info['bases'], list):
                for base_name in class_info['bases']:
                    # First try to find the base class in the same file
                    base_node = self.graph.find_node(f"{context.path}:{base_name}")
                    if not base_node:
                        # If not found, try to find it in any file
                        base_nodes = self.graph.get_nodes_by_type(NodeType.CLASS.value)
                        for node in base_nodes:
                            if node.name.endswith(f":{base_name}"):
                                base_node = node
                                break
                    if base_node:
                        self.graph.create_edge(
                            from_node=class_node,
                            to_node=base_node,
                            type=RelationType.INHERITS,
                            properties={
                                'start_line': class_info.get('start_line', 0),
                                'end_line': class_info.get('end_line', 0)
                            }
                        )
            # Process methods
            if 'methods' in class_info and isinstance(class_info['methods'], list):
                for method_info in class_info['methods']:
                    if not isinstance(method_info, dict) or 'name' not in method_info:
                        continue
                    method_node = self.graph.find_or_create_node(
                        name=f"{context.path}:{class_info['name']}.{method_info['name']}",
                        type=NodeType.METHOD,
                        properties={
                            'file_path': context.path,
                            'start_line': method_info.get('start_line', 0),
                            'end_line': method_info.get('end_line', 0)
                        }
                    )
                    self.graph.create_edge(
                        from_node=class_node,
                        to_node=method_node,
                        type=RelationType.CONTAINS,
                        properties={
                            'start_line': method_info.get('start_line', 0),
                            'end_line': method_info.get('end_line', 0)
                        }
                    )
    def _process_inheritance(self, context: FileContext) -> None:
        """Process class inheritance relationships.
        Args:
            context: The file context to process
        """
        if not context.symbols or 'classes' not in context.symbols:
            return
        for class_info in context.symbols['classes']:
            if not isinstance(class_info, dict) or 'name' not in class_info:
                continue
            class_name = class_info['name']
            bases = class_info.get('bases', [])
            # Process each base class
            for base in bases:
                if not isinstance(base, str):
                    continue
                # Try to find the base class in the current file first
                base_file = context.path
                base_node = self.graph.find_node(f"{base_file}:{base}")
                # If not found in current file, try to find it in imported modules
                if not base_node and 'imports' in context.symbols:
                    for imp in context.symbols['imports']:
                        if imp.get('type') == 'import' and imp.get('module'):
                            module = imp['module']
                            # Try to find the base class in the imported module
                            base_node = self.graph.find_node(f"{module}:{base}")
                            if base_node:
                                base_file = module
                                break
                if base_node:
                    # Create inheritance edge
                    class_node = self.graph.find_node(f"{context.path}:{class_name}")
                    if class_node:
                        self.graph.create_edge(
                            from_node=class_node,
                            to_node=base_node,
                            type=RelationType.INHERITS,
                            properties={
                                'file_path': context.path,
                                'start_line': class_info.get('start_line', 0),
                                'end_line': class_info.get('end_line', 0)
                            }
                        )
    def _process_functions(self, context: FileContext) -> None:
        """Process function definitions.
        Args:
            context: FileContext containing function information
        """
        if not context.symbols or 'functions' not in context.symbols:
            return
        for func_info in context.symbols['functions']:
            if not isinstance(func_info, dict) or 'name' not in func_info:
                logger.debug(f"Invalid function info in {context.path}: {func_info}")
                continue
            logger.debug(f"Processing function {func_info['name']} in {context.path}")
            # Create function node
            func_node = self.graph.find_or_create_node(
                name=f"{context.path}:{func_info['name']}",
                type=NodeType.FUNCTION,
                properties={
                    'file_path': context.path,
                    'start_line': func_info.get('start_line', 0),
                    'end_line': func_info.get('end_line', 0)
                }
            )
            # Create contains edge from file to function
            if self.current_file_node:
                self.graph.create_edge(
                    from_node=self.current_file_node,
                    to_node=func_node,
                    type=RelationType.CONTAINS,
                    properties={
                        'start_line': func_info.get('start_line', 0),
                        'end_line': func_info.get('end_line', 0)
                    }
                )
            # Process function parameters
            if 'parameters' in func_info and isinstance(func_info['parameters'], list):
                for param_info in func_info['parameters']:
                    if not isinstance(param_info, dict) or 'name' not in param_info:
                        continue
                    # Create parameter node
                    param_node = self.graph.find_or_create_node(
                        name=f"{context.path}:{func_info['name']}.{param_info['name']}",
                        type=NodeType.PARAMETER,
                        properties={
                            'file_path': context.path,
                            'start_line': param_info.get('start_line', 0),
                            'end_line': param_info.get('end_line', 0)
                        }
                    )
                    # Create contains edge from function to parameter
                    self.graph.create_edge(
                        from_node=func_node,
                        to_node=param_node,
                        type=RelationType.CONTAINS,
                        properties={
                            'start_line': param_info.get('start_line', 0),
                            'end_line': param_info.get('end_line', 0)
                        }
                    )
    def _process_references(self, context: Union[FileContext, List[Dict[str, Any]]], references: Optional[List[Dict[str, Any]]] = None) -> None:
        """Process code references.
        Args:
            context: Either a FileContext or a list of reference information dictionaries
            references: Optional list of references if context is a FileContext
        """
        if isinstance(context, FileContext):
            if not references:
                return
            refs = references
            file_path = context.path
        else:
            refs = context
            if not self.current_file_node:
                return
            file_path = self.current_file_node.properties['file_path']
        if not isinstance(refs, list) or not refs:
            return
        # Process function calls and attributes
        for ref in refs:
            if not isinstance(ref, dict) or 'type' not in ref:
                continue
            if ref['type'] == 'call':
                if 'name' not in ref or 'scope' not in ref:
                    continue
                # Find the calling function/method
                scope_name = ref['scope']
                caller_node = self.graph.find_node(f"{file_path}:{scope_name}")
                if not caller_node:
                    continue
                # Find or create the called function/method
                called_name = ref['name']
                called_node = None
                # First try to find the function in the same file
                called_node = self.graph.find_node(f"{file_path}:{called_name}")
                # If not found, look for it in imported modules
                if not called_node and isinstance(context, FileContext):
                    for imp in context.symbols.get('imports', []):
                        if 'symbol' in imp and imp['symbol'] == called_name:
                            module = imp['module']
                            # Try to find the function in the imported module
                            called_node = self.graph.find_node(f"{module}:{called_name}")
                            if called_node:
                                break
                # If still not found, look for it in any file
                if not called_node:
                    # Search for the function in any file
                    function_nodes = self.graph.get_nodes_by_type(NodeType.FUNCTION.value)
                    for node in function_nodes:
                        if node.name.endswith(f":{called_name}"):
                            called_node = node
                            break
                # If still not found, create a new node
                if not called_node:
                    called_node = self.graph.find_or_create_node(
                        name=f"{file_path}:{called_name}",
                        type=NodeType.FUNCTION,
                        properties={'file_path': file_path}
                    )
                # Create call edge
                self.graph.create_edge(
                    from_node=caller_node,
                    to_node=called_node,
                    type=RelationType.CALLS,
                    properties={
                        'start_line': ref.get('start_line', 0),
                        'end_line': ref.get('end_line', 0)
                    }
                )
            elif ref['type'] == 'attribute':
                if 'name' not in ref or 'scope' not in ref:
                    continue
                # Find the class/method that owns the attribute
                scope_name = ref['scope']
                scope_node = self.graph.find_node(f"{file_path}:{scope_name}")
                if not scope_node:
                    continue
                # Create attribute node
                attr_name = ref['name']
                attr_node = self.graph.find_or_create_node(
                    name=f"{file_path}:{scope_name}.{attr_name}",
                    type=NodeType.ATTRIBUTE,
                    properties={
                        'file_path': file_path,
                        'start_line': ref.get('start_line', 0),
                        'end_line': ref.get('end_line', 0)
                    }
                )
                # Create attribute edge
                self.graph.create_edge(
                    from_node=scope_node,
                    to_node=attr_node,
                    type=RelationType.HAS_ATTRIBUTE,
                    properties={
                        'start_line': ref.get('start_line', 0),
                        'end_line': ref.get('end_line', 0)
                    }
                )
</file>

<file path="server/code_understanding/semantic_analyzer.py">
"""Module for semantic analysis of JavaScript code."""
from typing import Dict, List, Set, Any, Optional, Union
from pathlib import Path
import re
from .language_adapters import JavaScriptParserAdapter
class Type:
    """Represents a JavaScript type."""
    def __init__(self, name: str, is_array: bool = False, is_optional: bool = False):
        self.name = name
        self.is_array = is_array
        self.is_optional = is_optional
    def __str__(self) -> str:
        result = self.name
        if self.is_array:
            result += '[]'
        if self.is_optional:
            result += '?'
        return result
class Scope:
    """Represents a lexical scope in JavaScript."""
    def __init__(self, parent: Optional['Scope'] = None):
        self.parent = parent
        self.variables: Dict[str, Type] = {}
        self.functions: Dict[str, Type] = {}
        self.classes: Dict[str, Dict[str, Type]] = {}
    def lookup(self, name: str) -> Optional[Type]:
        """Look up a variable in the current scope and parent scopes."""
        if name in self.variables:
            return self.variables[name]
        if self.parent:
            return self.parent.lookup(name)
        return None
    def lookup_function(self, name: str) -> Optional[Type]:
        """Look up a function in the current scope and parent scopes."""
        if name in self.functions:
            return self.functions[name]
        if self.parent:
            return self.parent.lookup_function(name)
        return None
    def lookup_class(self, name: str) -> Optional[Dict[str, Type]]:
        """Look up a class in the current scope and parent scopes."""
        if name in self.classes:
            return self.classes[name]
        if self.parent:
            return self.parent.lookup_class(name)
        return None
class SemanticAnalyzer:
    """Performs semantic analysis on JavaScript code."""
    def __init__(self):
        """Initialize the semantic analyzer."""
        self.parser = JavaScriptParserAdapter()
        self.global_scope = Scope()
        self.context_map: Dict[str, Dict[str, Any]] = {}
        # Initialize built-in types
        self._init_builtin_types()
    def _init_builtin_types(self):
        """Initialize built-in JavaScript types."""
        builtin_types = {
            'number': Type('number'),
            'string': Type('string'),
            'boolean': Type('boolean'),
            'object': Type('object'),
            'array': Type('array', is_array=True),
            'function': Type('function'),
            'undefined': Type('undefined'),
            'null': Type('null'),
            'symbol': Type('symbol'),
            'bigint': Type('bigint'),
            'Promise': Type('Promise'),
            'Date': Type('Date'),
            'RegExp': Type('RegExp'),
            'Error': Type('Error'),
            'Map': Type('Map'),
            'Set': Type('Set'),
            'WeakMap': Type('WeakMap'),
            'WeakSet': Type('WeakSet')
        }
        for name, type_info in builtin_types.items():
            self.global_scope.variables[name] = type_info
    def analyze_file(self, file_path: str, content: str) -> Dict[str, Any]:
        """Analyze a JavaScript file for semantic information.
        Args:
            file_path: Path to the JavaScript file
            content: File contents
        Returns:
            Dictionary containing semantic analysis results
        """
        # Parse the file
        try:
            ast = self.parser.parse(content)
        except Exception as e:
            return {
                'error': str(e),
                'types': {},
                'contexts': {}
            }
        # Create file scope
        file_scope = Scope(self.global_scope)
        # Analyze the AST
        types = self._analyze_ast(ast, file_scope)
        # Build context map
        contexts = self._build_context_map(ast, file_scope)
        # Store results
        self.context_map[file_path] = {
            'types': types,
            'contexts': contexts,
            'scope': file_scope
        }
        return {
            'types': types,
            'contexts': contexts
        }
    def _analyze_ast(self, ast: Any, scope: Scope) -> Dict[str, Type]:
        """Analyze the AST and infer types.
        Args:
            ast: Abstract syntax tree
            scope: Current scope
        Returns:
            Dictionary of variable names to their inferred types
        """
        types = {}
        for node in self._traverse_ast(ast):
            if node.get('type') == 'variable_declaration':
                # Handle variable declarations
                for decl in node.get('declarations', []):
                    name = decl.get('id', {}).get('name')
                    if name:
                        type_info = self._infer_type(decl.get('init'), scope)
                        scope.variables[name] = type_info
                        types[name] = type_info
            elif node.get('type') == 'function_declaration':
                # Handle function declarations
                name = node.get('id', {}).get('name')
                if name:
                    type_info = self._infer_function_type(node, scope)
                    scope.functions[name] = type_info
                    types[name] = type_info
            elif node.get('type') == 'class_declaration':
                # Handle class declarations
                name = node.get('id', {}).get('name')
                if name:
                    class_info = self._analyze_class(node, scope)
                    scope.classes[name] = class_info
                    types[name] = Type(name)  # Class type
        return types
    def _infer_type(self, node: Any, scope: Scope) -> Type:
        """Infer the type of a node.
        Args:
            node: AST node
            scope: Current scope
        Returns:
            Inferred type
        """
        if not node:
            return Type('undefined')
        node_type = node.get('type')
        if node_type == 'numeric_literal':
            return Type('number')
        elif node_type == 'string_literal':
            return Type('string')
        elif node_type == 'boolean_literal':
            return Type('boolean')
        elif node_type == 'array_expression':
            element_type = Type('any')
            if node.get('elements'):
                element_type = self._infer_type(node['elements'][0], scope)
            return Type(element_type.name, is_array=True)
        elif node_type == 'object_expression':
            return Type('object')
        elif node_type == 'identifier':
            # Look up variable type
            var_type = scope.lookup(node.get('name'))
            return var_type or Type('any')
        elif node_type == 'call_expression':
            # Infer type from function call
            func_type = self._infer_type(node.get('callee'), scope)
            return func_type
        elif node_type == 'member_expression':
            # Handle property access
            obj_type = self._infer_type(node.get('object'), scope)
            prop_name = node.get('property', {}).get('name')
            if prop_name and obj_type.name in scope.classes:
                class_info = scope.classes[obj_type.name]
                return class_info.get(prop_name, Type('any'))
            return Type('any')
        return Type('any')
    def _infer_function_type(self, node: Any, scope: Scope) -> Type:
        """Infer the type of a function.
        Args:
            node: Function declaration node
            scope: Current scope
        Returns:
            Function type
        """
        # Create new scope for function parameters
        func_scope = Scope(scope)
        # Analyze parameters
        params = node.get('params', [])
        param_types = []
        for param in params:
            if param.get('type') == 'identifier':
                name = param.get('name')
                type_info = Type('any')  # Default to any
                func_scope.variables[name] = type_info
                param_types.append(type_info)
        # Analyze return type
        body = node.get('body', {})
        return_type = Type('void')
        if body.get('type') == 'block_statement':
            for stmt in body.get('body', []):
                if stmt.get('type') == 'return_statement':
                    return_type = self._infer_type(stmt.get('argument'), func_scope)
                    break
        return Type('function', is_array=False)
    def _analyze_class(self, node: Any, scope: Scope) -> Dict[str, Type]:
        """Analyze a class declaration.
        Args:
            node: Class declaration node
            scope: Current scope
        Returns:
            Dictionary of class members to their types
        """
        class_info = {}
        # Analyze class body
        body = node.get('body', {})
        if body.get('type') == 'class_body':
            for member in body.get('body', []):
                if member.get('type') == 'method_definition':
                    name = member.get('key', {}).get('name')
                    if name:
                        class_info[name] = self._infer_function_type(member, scope)
                elif member.get('type') == 'property_definition':
                    name = member.get('key', {}).get('name')
                    if name:
                        class_info[name] = self._infer_type(member.get('value'), scope)
        return class_info
    def _build_context_map(self, ast: Any, scope: Scope) -> Dict[str, Any]:
        """Build a map of code contexts.
        Args:
            ast: Abstract syntax tree
            scope: Current scope
        Returns:
            Dictionary mapping contexts to their information
        """
        contexts = {}
        for node in self._traverse_ast(ast):
            if node.get('type') == 'function_declaration':
                # Function context
                name = node.get('id', {}).get('name')
                if name:
                    contexts[name] = {
                        'type': 'function',
                        'scope': self._get_scope_info(scope),
                        'parameters': self._get_parameters_info(node),
                        'return_type': self._infer_function_type(node, scope)
                    }
            elif node.get('type') == 'class_declaration':
                # Class context
                name = node.get('id', {}).get('name')
                if name:
                    contexts[name] = {
                        'type': 'class',
                        'scope': self._get_scope_info(scope),
                        'methods': self._get_class_methods_info(node),
                        'properties': self._get_class_properties_info(node)
                    }
            elif node.get('type') == 'variable_declaration':
                # Variable context
                for decl in node.get('declarations', []):
                    name = decl.get('id', {}).get('name')
                    if name:
                        contexts[name] = {
                            'type': 'variable',
                            'scope': self._get_scope_info(scope),
                            'value_type': self._infer_type(decl.get('init'), scope)
                        }
        return contexts
    def _get_scope_info(self, scope: Scope) -> Dict[str, Any]:
        """Get information about a scope.
        Args:
            scope: Scope to analyze
        Returns:
            Dictionary containing scope information
        """
        return {
            'variables': {name: str(type_info) for name, type_info in scope.variables.items()},
            'functions': {name: str(type_info) for name, type_info in scope.functions.items()},
            'classes': {name: {prop: str(type_info) for prop, type_info in class_info.items()}
                      for name, class_info in scope.classes.items()}
        }
    def _get_parameters_info(self, node: Any) -> List[Dict[str, Any]]:
        """Get information about function parameters.
        Args:
            node: Function declaration node
        Returns:
            List of parameter information dictionaries
        """
        params = []
        for param in node.get('params', []):
            if param.get('type') == 'identifier':
                params.append({
                    'name': param.get('name'),
                    'type': str(Type('any'))  # Default to any
                })
        return params
    def _get_class_methods_info(self, node: Any) -> Dict[str, Any]:
        """Get information about class methods.
        Args:
            node: Class declaration node
        Returns:
            Dictionary mapping method names to their information
        """
        methods = {}
        body = node.get('body', {})
        if body.get('type') == 'class_body':
            for member in body.get('body', []):
                if member.get('type') == 'method_definition':
                    name = member.get('key', {}).get('name')
                    if name:
                        methods[name] = {
                            'type': 'method',
                            'return_type': str(Type('any'))  # Default to any
                        }
        return methods
    def _get_class_properties_info(self, node: Any) -> Dict[str, Any]:
        """Get information about class properties.
        Args:
            node: Class declaration node
        Returns:
            Dictionary mapping property names to their information
        """
        properties = {}
        body = node.get('body', {})
        if body.get('type') == 'class_body':
            for member in body.get('body', []):
                if member.get('type') == 'property_definition':
                    name = member.get('key', {}).get('name')
                    if name:
                        properties[name] = {
                            'type': 'property',
                            'value_type': str(Type('any'))  # Default to any
                        }
        return properties
    def _traverse_ast(self, node: Any) -> List[Any]:
        """Traverse the AST and yield nodes.
        Args:
            node: AST node
        Returns:
            List of nodes
        """
        nodes = [node]
        for child in node.get('children', []):
            nodes.extend(self._traverse_ast(child))
        return nodes
</file>

<file path="server/code_understanding/symbols.py">
"""Symbol extractor module for extracting symbols from syntax trees."""
from typing import Dict, List, Any, Optional, Set
import logging
from .parser import MockNode as Node, MockTree as Tree
logger = logging.getLogger(__name__)
class SymbolExtractor:
    """Extractor class that extracts symbols from syntax trees."""
    def __init__(self):
        """Initialize the symbol extractor."""
        self.current_scope = None
        self.symbols = {}
        self.references = {}
    def extract_symbols(self, tree: Any) -> Dict[str, List[Dict[str, Any]]]:
        """Extract symbols from the tree.
        Args:
            tree: Syntax tree
        Returns:
            Dictionary of symbols
        """
        symbols = {
            'functions': [],
            'classes': [],
            'variables': []
        }
        if tree.root_node.type == 'program':
            for node in tree.root_node.children:
                if node.type == 'function_declaration':
                    name_node = node.fields.get('name')
                    if name_node:
                        symbols['functions'].append({
                            'name': name_node.text,
                            'start_line': node.start_point[0],
                            'end_line': node.end_point[0]
                        })
                elif node.type == 'class_declaration':
                    name_node = node.fields.get('name')
                    if name_node:
                        symbols['classes'].append({
                            'name': name_node.text,
                            'start_line': node.start_point[0],
                            'end_line': node.end_point[0]
                        })
                elif node.type == 'variable_declaration':
                    declarator = node.fields.get('declarator')
                    if declarator:
                        name_node = declarator.fields.get('name')
                        if name_node:
                            symbols['variables'].append({
                                'name': name_node.text,
                                'start_line': node.start_point[0],
                                'end_line': node.end_point[0]
                            })
        return symbols
    def _process_node(self, node: Node) -> None:
        """Process a node in the syntax tree.
        Args:
            node: Node to process
        """
        try:
            if not node:
                return
            # Update current scope
            if node.type in ('function_definition', 'class_definition'):
                name_node = node.child_by_field_name('name')
                if name_node:
                    scope_name = self._get_node_text(name_node)
                    old_scope = self.current_scope
                    self.current_scope = f"{old_scope}.{scope_name}" if old_scope else scope_name
                    # Process the node
                    if node.type == 'function_definition':
                        self._process_function(node)
                    else:
                        self._process_class(node)
                    # Process body with updated scope
                    for child in node.children_by_field_name('body'):
                        self._process_node(child)
                    self.current_scope = old_scope
                    return
            # Process other node types
            if node.type == 'import':
                self._process_import(node)
            elif node.type == 'identifier':
                self._process_identifier(node)
            elif node.type == 'assignment':
                self._process_assignment(node)
            # Process children
            for child in node.children:
                self._process_node(child)
        except Exception as e:
            logger.error(f"Failed to process node: {e}")
    def _process_identifier(self, node: Node) -> None:
        """Process an identifier node.
        Args:
            node: Identifier node
        """
        try:
            name = self._get_node_text(node)
            if name:
                # Add reference with current scope
                self._add_reference(name, {
                    'scope': self.current_scope or 'global',
                    'start': node.start_point,
                    'end': node.end_point
                })
                # If not already a symbol and not a parameter, add it as a variable
                if name not in self.symbols and not self._is_parameter(name):
                    self.symbols[name] = {
                        'type': 'variable',
                        'scope': self.current_scope or 'global',
                        'start': node.start_point,
                        'end': node.end_point
                    }
        except Exception as e:
            logger.error(f"Failed to process identifier: {e}")
    def _process_import(self, node: Node) -> None:
        """Process an import statement node.
        Args:
            node: Import statement node
        """
        try:
            # Extract imported names from the text
            text = self._get_node_text(node)
            if text.startswith('import '):
                # Simple import: "import os, sys"
                module_names = [name.strip() for name in text[7:].split(',')]
                for module_name in module_names:
                    module_name = module_name.strip()
                    if module_name:
                        # Extract just the module name without any 'as' alias
                        if ' as ' in module_name:
                            module_name = module_name.split(' as ')[0].strip()
                        self.symbols[module_name] = {
                            'type': 'import',
                            'scope': self.current_scope or 'global',
                            'start': node.start_point,
                            'end': node.end_point
                        }
            elif text.startswith('from '):
                # From import: "from typing import List, Optional"
                parts = text.split(' import ')
                if len(parts) == 2:
                    module = parts[0].replace('from ', '').strip()
                    # Split by comma and handle each import
                    imported_names = [name.strip() for name in parts[1].split(',')]
                    for name in imported_names:
                        name = name.strip()
                        if name:
                            # Extract just the name without any 'as' alias
                            if ' as ' in name:
                                name = name.split(' as ')[0].strip()
                            self.symbols[name] = {
                                'type': 'import',
                                'scope': self.current_scope or 'global',
                                'start': node.start_point,
                                'end': node.end_point,
                                'module': module
                            }
                            # Add the module itself as a symbol
                            if module not in self.symbols:
                                self.symbols[module] = {
                                    'type': 'import',
                                    'scope': self.current_scope or 'global',
                                    'start': node.start_point,
                                    'end': node.end_point
                                }
        except Exception as e:
            logger.error(f"Failed to process import: {e}")
    def _process_function(self, node: Node) -> None:
        """Process a function definition node.
        Args:
            node: Function definition node
        """
        try:
            name_node = node.child_by_field_name('name')
            if name_node:
                func_name = self._get_node_text(name_node)
                params = []
                # Process parameters
                params_node = node.child_by_field_name('parameters')
                if params_node:
                    for param in params_node.children:
                        if param.type == 'identifier':
                            param_text = self._get_node_text(param)
                            if ':' in param_text:
                                param_name = param_text.split(':')[0].strip()
                                param_type = param_text.split(':')[1].strip()
                            else:
                                param_name = param_text
                                param_type = 'Any'
                            params.append(param_name)
                            # Add parameter as a symbol in function scope
                            self.symbols[param_name] = {
                                'type': param_type,
                                'scope': f"{self.current_scope}.{func_name}" if self.current_scope else func_name,
                                'start': param.start_point,
                                'end': param.end_point
                            }
                # Add function to symbols
                self.symbols[func_name] = {
                    'type': 'function',
                    'scope': self.current_scope or 'global',
                    'start': node.start_point,
                    'end': node.end_point,
                    'params': params
                }
                # Process function body with updated scope
                old_scope = self.current_scope
                self.current_scope = f"{old_scope}.{func_name}" if old_scope else func_name
                for child in node.children_by_field_name('body'):
                    self._process_node(child)
                self.current_scope = old_scope
        except Exception as e:
            logger.error(f"Failed to process function: {e}")
    def _process_class(self, node: Node) -> None:
        """Process a class definition node.
        Args:
            node: Class definition node
        """
        try:
            name_node = node.child_by_field_name('name')
            if name_node:
                class_name = self._get_node_text(name_node)
                bases = self._get_class_bases(node)
                self._add_symbol(class_name, {
                    'type': 'class',
                    'scope': self.current_scope,
                    'bases': bases,
                    'start': node.start_point,
                    'end': node.end_point
                })
        except Exception as e:
            logger.error(f"Failed to process class: {e}")
    def _process_assignment(self, node: Node) -> None:
        """Process an assignment node.
        Args:
            node: Assignment node
        """
        try:
            left = node.child_by_field_name('left')
            right = node.child_by_field_name('right')
            if left and left.type == 'identifier':
                name = self._get_node_text(left)
                if name:
                    # Get type from right side if available
                    type_info = 'Any'
                    if right:
                        type_info = self._get_type_info(right)
                    # Add or update symbol
                    self.symbols[name] = {
                        'type': type_info,
                        'scope': self.current_scope or 'global',
                        'start': node.start_point,
                        'end': node.end_point
                    }
                    # Process right side for references
                    if right:
                        self._process_node(right)
        except Exception as e:
            logger.error(f"Failed to process assignment: {e}")
    def _add_symbol(self, name: str, info: Dict[str, Any]) -> None:
        """Add a symbol to the symbols dictionary.
        Args:
            name: Symbol name
            info: Symbol information
        """
        self.symbols[name] = info
    def _add_reference(self, name: str, ref: Dict[str, Any]) -> None:
        """Add a reference to a name.
        Args:
            name: Name being referenced
            ref: Reference information
        """
        try:
            if name not in self.references:
                self.references[name] = []
            self.references[name].append(ref)
        except Exception as e:
            logger.error(f"Failed to add reference: {e}")
    def _find_child(self, node: Node, child_type: str) -> Optional[Node]:
        """Find a child node of a specific type.
        Args:
            node: Parent node
            child_type: Type of child to find
        Returns:
            Node: Found child node or None
        """
        try:
            for child in node.children:
                if child.type == child_type:
                    return child
            return None
        except Exception as e:
            logger.error(f"Failed to find child: {e}")
            return None
    def _get_node_text(self, node: Node) -> str:
        """Get text from a node.
        Args:
            node: Node to get text from
        Returns:
            Text from the node
        """
        try:
            if not node or not node.text:
                return ""
            # Handle bytes
            if isinstance(node.text, bytes):
                return node.text.decode('utf-8').strip()
            # Handle non-string types
            if not isinstance(node.text, str):
                return ""
            return node.text.strip()
        except Exception as e:
            logger.error(f"Failed to get node text: {e}")
            return ""
    def _get_function_params(self, node: Node) -> List[str]:
        """Get function parameters.
        Args:
            node: Function definition node
        Returns:
            List of parameter strings
        """
        params = []
        try:
            params_node = node.child_by_field_name('parameters')
            if params_node:
                for param in params_node.children:
                    param_text = self._get_node_text(param)
                    if param_text and param_text not in ('self', 'cls'):
                        if ':' in param_text:
                            name = param_text.split(':')[0].strip()
                            params.append(name)
                        else:
                            params.append(param_text)
        except Exception as e:
            logger.error(f"Failed to get function parameters: {e}")
        return params
    def _get_class_bases(self, node: Node) -> List[str]:
        """Get class base classes.
        Args:
            node: Class definition node
        Returns:
            List[str]: List of base class names
        """
        try:
            bases = []
            bases_node = node.child_by_field_name('bases')
            if bases_node:
                for base in bases_node.children:
                    if base.type == 'identifier':
                        bases.append(self._get_node_text(base))
            return bases
        except Exception as e:
            logger.error(f"Failed to get class bases: {e}")
            return []
    def _get_type_info(self, node: Optional[Node]) -> str:
        """Get type information from a node.
        Args:
            node: Node to get type info from
        Returns:
            Type string
        """
        if not node:
            return 'Any'
        try:
            text = self._get_node_text(node)
            if ':' in text:
                _, type_hint = text.split(':', 1)
                return type_hint.strip()
            if node.type == 'string':
                return 'str'
            elif node.type == 'integer':
                return 'int'
            elif node.type == 'float':
                return 'float'
            elif node.type == 'true' or node.type == 'false':
                return 'bool'
            elif node.type == 'list':
                return 'List'
            elif node.type == 'dict':
                return 'Dict'
            elif node.type == 'call':
                return self._get_node_text(node)
            elif node.type == 'identifier':
                return self._get_node_text(node)
            else:
                return 'Any'
        except Exception as e:
            logger.error(f"Failed to get type info: {e}")
            return 'Any'
    def _is_parameter(self, name: str) -> bool:
        """Check if a name is a parameter in the current scope.
        Args:
            name: Name to check
        Returns:
            True if the name is a parameter, False otherwise
        """
        try:
            if not self.current_scope:
                return False
            # Check if the name is a parameter in any parent scope
            scope = self.current_scope
            while scope:
                for symbol in self.symbols.values():
                    if (symbol.get('scope') == scope and 
                        symbol.get('type') == 'function' and 
                        name in symbol.get('params', [])):
                        return True
                # Move up to parent scope
                scope = scope.rsplit('.', 1)[0] if '.' in scope else None
            return False
        except Exception as e:
            logger.error(f"Failed to check if name is parameter: {e}")
            return False
</file>

<file path="server/__init__.py">
"""
MCP Server package for managing LLM and core tools.
"""
__version__ = "0.1.0"
from .core import (
    is_command_safe,
    execute_command,
    read_output,
    force_terminate,
    block_command,
    unblock_command,
    blacklisted_commands
)
</file>

<file path="server/core.py">
from mcp.server.fastmcp import FastMCP
import os
import platform
import subprocess
import shlex
import time
import signal
import re
import glob
import stat
import shutil
import threading
import queue
import json
from typing import Dict, List, Optional, Union, Any
from datetime import datetime
import socket
import math
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.semconv.resource import ResourceAttributes
from functools import wraps
from opentelemetry.metrics import get_meter_provider, set_meter_provider
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter
import cProfile
import pstats
import io
import tempfile
from debugger import create_debugger
from decorators import set_debugger
import sys
import ast
import psutil
from opentelemetry.sdk.metrics._internal.measurement import Measurement
import asyncio
import metrics
# import yaml
# Initialize the MCP server
mcp = FastMCP("Terminal Command Runner MCP", port=7443, log_level="DEBUG")
# Global variables for process management
session_lock = threading.Lock()
active_sessions = {}
blacklisted_commands = set(['rm -rf /', 'mkfs'])
output_queues = {}
# Global variables for debug state management
debug_sessions = {}
debug_breakpoints = {}
# Initialize tracer
resource = Resource(attributes={
    ResourceAttributes.SERVICE_NAME: "mcp-server",
    ResourceAttributes.SERVICE_VERSION: "1.0.0",
})
def get_memory_usage(options):
    """Get memory usage of the MCP server."""
    try:
        memory_info = psutil.Process().memory_info()
        return [Measurement(
            value=memory_info.rss,
            attributes={"unit": "bytes"},
            time_unix_nano=int(time.time_ns()),
            instrument=memory_usage,
            context=None
        )]
    except Exception as e:
        print(f"Error getting memory usage: {e}")
        return [Measurement(
            value=0,
            attributes={"unit": "bytes", "error": str(e)},
            time_unix_nano=int(time.time_ns()),
            instrument=memory_usage,
            context=None
        )]
# Only enable tracing and metrics if not in test mode
is_test_mode = "pytest" in sys.modules
enable_telemetry = os.environ.get("ENABLE_TELEMETRY", "0") == "1"
if not is_test_mode and enable_telemetry:
    trace.set_tracer_provider(TracerProvider(resource=resource))
    tracer = trace.get_tracer(__name__)
    # Configure exporter
    otlp_exporter = OTLPSpanExporter(endpoint="http://localhost:4317")
    span_processor = BatchSpanProcessor(otlp_exporter)
    trace.get_tracer_provider().add_span_processor(span_processor)
    # Get metrics from metrics module
    meter = metrics.get_meter()
    # Create metrics
    tool_duration = meter.create_histogram(
        name="mcp.tool.duration",
        description="Duration of MCP tool execution",
        unit="s"
    )
    tool_calls = meter.create_counter(
        name="mcp.tool.calls",
        description="Number of MCP tool calls",
        unit="1"
    )
    tool_errors = meter.create_counter(
        name="mcp.tool.errors",
        description="Number of MCP tool errors",
        unit="1"
    )
    # Create a counter for active sessions
    active_sessions_counter = meter.create_up_down_counter(
        name="mcp.sessions.active",
        description="Number of active MCP sessions",
        unit="1"
    )
    memory_usage = meter.create_observable_gauge(
        name="mcp.system.memory_usage",
        description="Memory usage of the MCP server",
        unit="bytes",
        callbacks=[get_memory_usage]
    )
else:
    # Mock objects for test mode or when telemetry is disabled
    class MockTracer:
        def start_as_current_span(self, *args, **kwargs):
            class MockSpan:
                def __enter__(self): return self
                def __exit__(self, *args): pass
                def set_attribute(self, *args): pass
                def record_exception(self, *args): pass
            return MockSpan()
    tracer = MockTracer()
    class MockMeter:
        def create_histogram(self, *args, **kwargs): return self
        def create_counter(self, *args, **kwargs): return self
        def create_up_down_counter(self, *args, **kwargs): return self
        def create_observable_gauge(self, *args, **kwargs): return self
        def add(self, *args, **kwargs): pass
        def record(self, *args, **kwargs): pass
    meter = MockMeter()
    tool_duration = meter
    tool_calls = meter
    tool_errors = meter
    active_sessions_counter = meter
    memory_usage = meter
def update_active_sessions_metric():
    """Update the active sessions metric."""
    with session_lock:
        active_sessions_counter.add(len(active_sessions))
def trace_tool(func):
    """Decorator to add tracing to MCP tools"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        with tracer.start_as_current_span(
            name=f"mcp.tool.{func.__name__}",
            attributes={
                "mcp.tool.name": func.__name__,
                "mcp.tool.args": str(args),
                "mcp.tool.kwargs": str(kwargs)
            }
        ) as span:
            try:
                result = func(*args, **kwargs)
                if isinstance(result, dict):
                    span.set_attribute("mcp.tool.status", result.get("status", "unknown"))
                    if "error" in result:
                        span.set_attribute("mcp.tool.error", result["error"])
                return result
            except Exception as e:
                span.set_attribute("mcp.tool.error", str(e))
                span.record_exception(e)
                raise
    return wrapper
def metrics_tool(func):
    """Decorator to add metrics to MCP tools"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        tool_calls.add(1, {"tool": func.__name__})
        try:
            result = func(*args, **kwargs)
            duration = time.time() - start_time
            tool_duration.record(duration, {"tool": func.__name__})
            return result
        except Exception as e:
            tool_errors.add(1, {"tool": func.__name__, "error": str(e)})
            raise
    return wrapper
# Add tracing to existing tools
async def add_tracing_to_tools():
    """Add tracing to all registered MCP tools"""
    tools = await mcp.list_tools()
    for tool_name in tools:
        tool_func = tools[tool_name]
        if not hasattr(tool_func, "_traced"):
            traced_func = trace_tool(tool_func)
            traced_func._traced = True
            mcp.add_tool(tool_name)(traced_func)
@mcp.tool()
def get_trace_info() -> Dict[str, Any]:
    """
    Get information about the current tracing configuration
    Returns:
        Dictionary with tracing information
    """
    try:
        current_span = trace.get_current_span()
        return {
            'status': 'success',
            'tracer': {
                'name': tracer.name,
                'version': trace.get_tracer_provider().__class__.__name__
            },
            'current_span': {
                'name': current_span.name if current_span else None,
                'context': str(current_span.get_span_context()) if current_span else None,
                'active': bool(current_span)
            },
            'exporter': {
                'type': otlp_exporter.__class__.__name__,
                'endpoint': otlp_exporter.endpoint
            }
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
@mcp.tool()
def configure_tracing(exporter_endpoint: str = None, service_name: str = None, service_version: str = None) -> Dict[str, Any]:
    """
    Configure tracing settings
    Args:
        exporter_endpoint: OTLP exporter endpoint URL
        service_name: Service name for tracing
        service_version: Service version for tracing
    Returns:
        Dictionary with configuration result
    """
    try:
        global otlp_exporter, resource
        # Update exporter if endpoint provided
        if exporter_endpoint:
            otlp_exporter = OTLPSpanExporter(endpoint=exporter_endpoint)
            trace.get_tracer_provider().add_span_processor(BatchSpanProcessor(otlp_exporter))
        # Update resource if service info provided
        if service_name or service_version:
            attributes = {}
            if service_name:
                attributes[ResourceAttributes.SERVICE_NAME] = service_name
            if service_version:
                attributes[ResourceAttributes.SERVICE_VERSION] = service_version
            resource = Resource(attributes=attributes)
            # Update tracer provider with new resource
            trace.set_tracer_provider(TracerProvider(resource=resource))
        return {
            'status': 'success',
            'config': {
                'exporter_endpoint': otlp_exporter.endpoint,
                'service_name': resource.attributes.get(ResourceAttributes.SERVICE_NAME),
                'service_version': resource.attributes.get(ResourceAttributes.SERVICE_VERSION)
            }
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
@mcp.tool()
def get_metrics_info() -> Dict[str, Any]:
    """
    Get information about the current metrics configuration
    Returns:
        Dictionary with metrics information
    """
    try:
        return {
            'status': 'success',
            'meter': {
                'name': meter.name,
                'version': meter_provider.__class__.__name__
            },
            'metrics': {
                'tool_duration': {
                    'name': tool_duration.name,
                    'description': tool_duration.description,
                    'unit': tool_duration.unit
                },
                'tool_calls': {
                    'name': tool_calls.name,
                    'description': tool_calls.description,
                    'unit': tool_calls.unit
                },
                'tool_errors': {
                    'name': tool_errors.name,
                    'description': tool_errors.description,
                    'unit': tool_errors.unit
                },
                'active_sessions': {
                    'name': active_sessions_counter.name,
                    'description': active_sessions_counter.description,
                    'unit': active_sessions_counter.unit
                },
                'memory_usage': {
                    'name': memory_usage.name,
                    'description': memory_usage.description,
                    'unit': memory_usage.unit
                }
            }
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
@mcp.tool()
def configure_metrics(exporter_endpoint: str = None) -> Dict[str, Any]:
    """
    Configure metrics settings
    Args:
        exporter_endpoint: OTLP exporter endpoint URL
    Returns:
        Dictionary with configuration result
    """
    try:
        global meter_provider, meter
        if exporter_endpoint:
            # Create new meter provider with updated endpoint
            meter_provider = MeterProvider(
                metric_readers=[PeriodicExportingMetricReader(
                    OTLPMetricExporter(endpoint=exporter_endpoint)
                )]
            )
            set_meter_provider(meter_provider)
            meter = get_meter_provider().get_meter("mcp-server")
            # Recreate metrics with new meter
            global tool_duration, tool_calls, tool_errors, active_sessions, memory_usage
            tool_duration = meter.create_histogram(
                name="mcp.tool.duration",
                description="Duration of MCP tool execution",
                unit="s"
            )
            tool_calls = meter.create_counter(
                name="mcp.tool.calls",
                description="Number of MCP tool calls",
                unit="1"
            )
            tool_errors = meter.create_counter(
                name="mcp.tool.errors",
                description="Number of MCP tool errors",
                unit="1"
            )
            active_sessions = meter.create_up_down_counter(
                name="mcp.sessions.active",
                description="Number of active MCP sessions",
                unit="1"
            )
            memory_usage = meter.create_observable_gauge(
                name="mcp.system.memory_usage",
                description="Memory usage of the MCP server",
                unit="bytes",
                callbacks=[lambda _: [(None, psutil.Process().memory_info().rss)]]
            )
        return {
            'status': 'success',
            'config': {
                'exporter_endpoint': exporter_endpoint or "http://localhost:4317"
            }
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
# Add metrics to all tools
async def add_metrics_to_tools():
    """Add metrics to all registered MCP tools"""
    tools = await mcp.list_tools()
    for tool_name in tools:
        tool_func = tools[tool_name]
        if not hasattr(tool_func, "_metrics"):
            metriced_func = metrics_tool(tool_func)
            metriced_func._metrics = True
            mcp.add_tool(tool_name)(metriced_func)
# Add profiling to all tools
async def add_profiling_to_tools():
    """Add profiling to all registered MCP tools"""
    tools = await mcp.list_tools()
    for tool_name in tools:
        tool_func = tools[tool_name]
        if not hasattr(tool_func, "_profiled"):
            profiled_func = profile_tool(tool_func)
            profiled_func._profiled = True
            mcp.add_tool(tool_name)(profiled_func)
def is_command_safe(cmd: str) -> bool:
    """Check if a command is safe to execute"""
    if not cmd.strip():
        return False
    for blocked in blacklisted_commands:
        if blocked in cmd:
            return False
    return True
# Terminal Tools
def execute_command(command: str, timeout: int = 10, allow_background: bool = True) -> dict:
    """Execute a command with timeout and output capture."""
    if not command or not isinstance(command, str):
        return {
            "status": "error",
            "error": "Invalid command",
            "pid": None,
            "exit_code": 1,
            "stdout": "",
            "stderr": "Invalid command provided"
        }
    if not is_command_safe(command):
        return {
            "status": "error",
            "error": "Command blocked",
            "pid": None,
            "exit_code": 1,
            "stdout": "",
            "stderr": "Command was blocked for security reasons"
        }
    try:
        # Split command into args while preserving quoted strings
        args = shlex.split(command)
        # Create output queues
        stdout_queue = queue.Queue()
        stderr_queue = queue.Queue()
        # Start process
        process = subprocess.Popen(
            args,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            bufsize=1,
            universal_newlines=True
        )
        pid = process.pid
        # Create reader thread event for signaling
        stop_event = threading.Event()
        def reader_thread():
            """Thread to read process output."""
            try:
                while not stop_event.is_set():
                    # Read with timeout to allow checking stop_event
                    stdout_line = process.stdout.readline()
                    if stdout_line:
                        stdout_queue.put(stdout_line)
                    stderr_line = process.stderr.readline()
                    if stderr_line:
                        stderr_queue.put(stderr_line)
                    # Check if process has finished
                    if process.poll() is not None:
                        break
                    time.sleep(0.1)  # Prevent busy waiting
            except Exception as e:
                stderr_queue.put(f"Error reading output: {str(e)}")
            finally:
                # Ensure remaining output is read
                for line in process.stdout:
                    stdout_queue.put(line)
                for line in process.stderr:
                    stderr_queue.put(line)
        # Start reader thread
        reader = threading.Thread(target=reader_thread)
        reader.daemon = True
        reader.start()
        # Store session info
        with session_lock:
            active_sessions[pid] = {
                "process": process,
                "command": command,
                "start_time": time.time(),
                "stdout_queue": stdout_queue,
                "stderr_queue": stderr_queue,
                "reader_thread": reader,
                "stop_event": stop_event
            }
            update_active_sessions_metric()
        # Wait for timeout
        try:
            process.wait(timeout=timeout)
        except subprocess.TimeoutExpired:
            if not allow_background:
                # Clean up if background not allowed
                stop_event.set()
                process.terminate()
                try:
                    process.wait(timeout=1)
                except subprocess.TimeoutExpired:
                    process.kill()
                with session_lock:
                    if pid in active_sessions:
                        del active_sessions[pid]
                        update_active_sessions_metric()
                return {
                    "status": "error",
                    "error": "Command timed out",
                    "pid": None,
                    "runtime": timeout,
                    "exit_code": None,
                    "stdout": "".join(read_queue_contents(stdout_queue)),
                    "stderr": "".join(read_queue_contents(stderr_queue))
                }
        # Process completed within timeout
        if process.returncode is not None:
            stop_event.set()
            reader.join(timeout=1)
            with session_lock:
                if pid in active_sessions:
                    del active_sessions[pid]
                    update_active_sessions_metric()
            return {
                "status": "success" if process.returncode == 0 else "error",
                "pid": None,
                "runtime": time.time() - active_sessions[pid]["start_time"],
                "exit_code": process.returncode,
                "stdout": "".join(read_queue_contents(stdout_queue)),
                "stderr": "".join(read_queue_contents(stderr_queue)),
                "complete": True
            }
        # Process is running in background
        return {
            "status": "running",
            "pid": pid,
            "runtime": time.time() - active_sessions[pid]["start_time"],
            "exit_code": None,
            "stdout": "".join(read_queue_contents(stdout_queue)),
            "stderr": "".join(read_queue_contents(stderr_queue)),
            "complete": False
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "pid": None,
            "exit_code": 1,
            "stdout": "",
            "stderr": str(e)
        }
def read_queue_contents(q: queue.Queue) -> List[str]:
    """Read all available content from a queue without blocking."""
    contents = []
    while True:
        try:
            contents.append(q.get_nowait())
        except queue.Empty:
            break
    return contents
@mcp.tool()
def read_output(pid: int) -> Dict[str, Any]:
    """Read output from a running command session."""
    try:
        with session_lock:
            if pid not in active_sessions:
                return {
                    "status": "error",
                    "error": f"No active session for PID {pid}",
                    "pid": None,
                    "stdout": "",
                    "stderr": f"No active session found for PID {pid}",
                    "complete": True
                }
            session = active_sessions[pid]
            process = session["process"]
            stdout_queue = session["stdout_queue"]
            stderr_queue = session["stderr_queue"]
        # Read available output
        stdout = "".join(read_queue_contents(stdout_queue))
        stderr = "".join(read_queue_contents(stderr_queue))
        # Check if process has completed
        returncode = process.poll()
        if returncode is not None:
            # Process finished, clean up
            session["stop_event"].set()
            session["reader_thread"].join(timeout=1)
            with session_lock:
                if pid in active_sessions:
                    del active_sessions[pid]
                    update_active_sessions_metric()
            return {
                "status": "success" if returncode == 0 else "error",
                "pid": None,
                "stdout": stdout,
                "stderr": stderr,
                "exit_code": returncode,
                "complete": True
            }
        # Process still running
        return {
            "status": "running",
            "pid": pid,
            "stdout": stdout,
            "stderr": stderr,
            "exit_code": None,
            "complete": False
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "pid": None,
            "stdout": "",
            "stderr": str(e),
            "complete": True
        }
@mcp.resource("debug://state")
def debug_state() -> Dict[str, Any]:
    """
    Resource that provides current debug state information
    Returns:
        Dictionary with debug state information
    """
    active_sessions = []
    for session_id, session in debug_sessions.items():
        if session.get('active', False):
            active_sessions.append({
                'session_id': session_id,
                'file': session.get('file'),
                'line': session.get('line'),
                'variables': session.get('variables', {}),
                'call_stack': session.get('call_stack', []),
                'breakpoints': debug_breakpoints.get(session_id, [])
            })
    return {
        'active_sessions': active_sessions,
        'global_breakpoints': [bp for bp_list in debug_breakpoints.values() for bp in bp_list],
        'timestamp': datetime.now().isoformat()
    }
@mcp.tool()
def debug_control(action: str, session_id: str = None, file_path: str = None, line_number: int = None, expression: str = None) -> Dict[str, Any]:
    """
    Control debugging sessions and evaluate expressions
    Args:
        action: Debug action ('start', 'stop', 'step', 'continue', 'breakpoint', 'evaluate')
        session_id: Debug session identifier
        file_path: Path to the file being debugged
        line_number: Line number for breakpoint
        expression: Expression to evaluate in current context
    Returns:
        Dictionary with operation result
    """
    try:
        if action == 'start':
            if not file_path:
                return {'status': 'error', 'error': 'File path required to start debugging'}
            # Create new debug session
            session_id = f"debug_{int(time.time())}"
            debug_sessions[session_id] = {
                'active': True,
                'file': file_path,
                'line': 1,
                'variables': {},
                'call_stack': [],
                'start_time': datetime.now().isoformat()
            }
            # Start debug process
            process = subprocess.Popen(
                ['python', '-m', 'pdb', file_path],
                stdin=subprocess.PIPE,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True
            )
            debug_sessions[session_id]['process'] = process
            return {
                'status': 'success',
                'session_id': session_id,
                'message': f'Debug session started for {file_path}'
            }
        elif action == 'stop':
            if not session_id or session_id not in debug_sessions:
                return {'status': 'error', 'error': 'Invalid session ID'}
            session = debug_sessions[session_id]
            if session.get('process'):
                session['process'].terminate()
            session['active'] = False
            return {
                'status': 'success',
                'message': f'Debug session {session_id} stopped'
            }
        elif action == 'breakpoint':
            if not file_path or not line_number:
                return {'status': 'error', 'error': 'File path and line number required for breakpoint'}
            if session_id not in debug_breakpoints:
                debug_breakpoints[session_id] = []
            breakpoint_info = {
                'file': file_path,
                'line': line_number,
                'enabled': True
            }
            debug_breakpoints[session_id].append(breakpoint_info)
            return {
                'status': 'success',
                'message': f'Breakpoint set at {file_path}:{line_number}'
            }
        elif action in ['step', 'continue']:
            if not session_id or session_id not in debug_sessions:
                return {'status': 'error', 'error': 'Invalid session ID'}
            session = debug_sessions[session_id]
            if not session.get('process'):
                return {'status': 'error', 'error': 'Debug process not running'}
            # Send appropriate command to debugger
            cmd = 'n' if action == 'step' else 'c'
            session['process'].stdin.write(f'{cmd}\n')
            session['process'].stdin.flush()
            # Read output until next break
            output = []
            while True:
                line = session['process'].stdout.readline()
                if not line or '(Pdb)' in line:
                    break
                output.append(line.strip())
            return {
                'status': 'success',
                'output': output,
                'action': action
            }
        elif action == 'evaluate':
            if not session_id or session_id not in debug_sessions:
                return {'status': 'error', 'error': 'Invalid session ID'}
            if not expression:
                return {'status': 'error', 'error': 'Expression required for evaluation'}
            session = debug_sessions[session_id]
            if not session.get('process'):
                return {'status': 'error', 'error': 'Debug process not running'}
            # Send expression to debugger
            session['process'].stdin.write(f'p {expression}\n')
            session['process'].stdin.flush()
            # Read result
            result = session['process'].stdout.readline().strip()
            return {
                'status': 'success',
                'expression': expression,
                'result': result
            }
        else:
            return {'status': 'error', 'error': f'Unknown action: {action}'}
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
@mcp.tool()
def git_operation(command: str, parameters: Dict[str, str] = None) -> Dict[str, Any]:
    """
    Execute Git operations safely
    Args:
        command: Git command to execute ('status', 'diff', 'log', 'branch', 'commit')
        parameters: Additional parameters for the command
    Returns:
        Dictionary with operation result
    """
    if not parameters:
        parameters = {}
    # Validate command
    allowed_commands = {
        'status': [],
        'diff': ['file', 'staged'],
        'log': ['limit', 'file'],
        'branch': ['name', 'delete'],
        'commit': ['message', 'files']
    }
    if command not in allowed_commands:
        return {
            'status': 'error',
            'error': f'Unsupported git command: {command}'
        }
    try:
        if command == 'status':
            result = subprocess.run(['git', 'status', '--porcelain'], 
                                 capture_output=True, text=True, check=True)
            # Parse status output
            changes = {
                'staged': [],
                'unstaged': [],
                'untracked': []
            }
            for line in result.stdout.split('\n'):
                if not line:
                    continue
                status = line[:2]
                file = line[3:]
                if status[0] != ' ':
                    changes['staged'].append({'file': file, 'status': status[0]})
                if status[1] != ' ':
                    changes['unstaged'].append({'file': file, 'status': status[1]})
                if status == '??':
                    changes['untracked'].append(file)
            return {
                'status': 'success',
                'changes': changes
            }
        elif command == 'diff':
            cmd = ['git', 'diff']
            if parameters.get('staged'):
                cmd.append('--staged')
            if parameters.get('file'):
                cmd.append(parameters['file'])
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return {
                'status': 'success',
                'diff': result.stdout
            }
        elif command == 'log':
            cmd = ['git', 'log', '--pretty=format:%H|%an|%ad|%s']
            if parameters.get('limit'):
                cmd.append(f'-n{parameters["limit"]}')
            if parameters.get('file'):
                cmd.append(parameters['file'])
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            commits = []
            for line in result.stdout.split('\n'):
                if line:
                    hash, author, date, message = line.split('|')
                    commits.append({
                        'hash': hash,
                        'author': author,
                        'date': date,
                        'message': message
                    })
            return {
                'status': 'success',
                'commits': commits
            }
        elif command == 'branch':
            if parameters.get('delete'):
                if not parameters.get('name'):
                    return {'status': 'error', 'error': 'Branch name required for deletion'}
                cmd = ['git', 'branch', '-d', parameters['name']]
            elif parameters.get('name'):
                cmd = ['git', 'checkout', '-b', parameters['name']]
            else:
                cmd = ['git', 'branch']
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            return {
                'status': 'success',
                'output': result.stdout
            }
        elif command == 'commit':
            if not parameters.get('message'):
                return {'status': 'error', 'error': 'Commit message required'}
            # Stage files if specified
            if parameters.get('files'):
                files = parameters['files'] if isinstance(parameters['files'], list) else [parameters['files']]
                for file in files:
                    subprocess.run(['git', 'add', file], check=True)
            # Create commit
            result = subprocess.run(['git', 'commit', '-m', parameters['message']], 
                                 capture_output=True, text=True, check=True)
            return {
                'status': 'success',
                'message': result.stdout
            }
    except subprocess.CalledProcessError as e:
        return {
            'status': 'error',
            'error': e.stderr
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
# Development Tools
@mcp.tool()
def install_dependency(package: str, dev: bool = False) -> Dict[str, Any]:
    """
    Install Python package using uv
    Args:
        package: Package name and optional version spec
        dev: Whether to install as a development dependency
    Returns:
        Dictionary with installation result
    """
    try:
        cmd = ['uv', 'add']
        if dev:
            cmd.append('--dev')
        cmd.append(package)
        result = subprocess.run(cmd, capture_output=True, text=True, check=True)
        # Read pyproject.toml to verify installation
        with open('pyproject.toml', 'r') as f:
            pyproject_content = f.read()
        return {
            'status': 'success',
            'output': result.stdout,
            'package': package,
            'pyproject_toml': pyproject_content
        }
    except subprocess.CalledProcessError as e:
        return {
            'status': 'error',
            'error': e.stderr
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
@mcp.tool()
def run_tests(target: str = None, docker: bool = False) -> Dict[str, Any]:
    """
    Run tests with proper isolation
    Args:
        target: Specific test target (file or directory)
        docker: Whether to run tests in Docker
    Returns:
        Dictionary with test results
    """
    try:
        if docker:
            cmd = ['make', 'test']
            if target:
                cmd.extend(['TEST_TARGET=' + target])
        else:
            cmd = ['pytest']
            if target:
                cmd.append(target)
            cmd.extend(['-v', '--capture=no'])
        result = subprocess.run(cmd, capture_output=True, text=True)
        # Process the output to be more LLM-friendly
        output_lines = result.stdout.split('\n')
        filtered_output = _filter_test_output(output_lines)
        return {
            'status': 'success' if result.returncode == 0 else 'failure',
            'output': filtered_output,
            'exit_code': result.returncode,
            'errors': result.stderr if result.stderr else None
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
def _filter_test_output(lines: List[str]) -> str:
    """Helper to filter and format test output for LLM consumption"""
    important_lines = []
    summary_stats = {}
    for line in lines:
        # Keep test results
        if line.startswith('test_'):
            important_lines.append(line)
        # Keep error messages
        elif 'ERROR' in line or 'FAILED' in line:
            important_lines.append(line)
        # Extract summary statistics
        elif 'failed' in line and 'passed' in line:
            summary_stats['summary'] = line.strip()
    return {
        'details': important_lines,
        'summary': summary_stats
    }
@mcp.tool()
def format_code(path: str = '.') -> Dict[str, Any]:
    """
    Format code using ruff
    Args:
        path: Path to format (file or directory)
    Returns:
        Dictionary with formatting result
    """
    try:
        cmd = ['ruff', 'format', path]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return {
            'status': 'success' if result.returncode == 0 else 'error',
            'output': result.stdout,
            'errors': result.stderr if result.stderr else None
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
@mcp.tool()
def lint_code(path: str = '.', fix: bool = False) -> Dict[str, Any]:
    """
    Run ruff linting
    Args:
        path: Path to lint (file or directory)
        fix: Whether to automatically fix issues
    Returns:
        Dictionary with linting result
    """
    try:
        cmd = ['ruff', 'check']
        if fix:
            cmd.append('--fix')
        cmd.append(path)
        result = subprocess.run(cmd, capture_output=True, text=True)
        # Process output to be more LLM-friendly
        output_lines = result.stdout.split('\n')
        filtered_output = _filter_lint_output(output_lines)
        return {
            'status': 'success' if result.returncode == 0 else 'warning',
            'issues': filtered_output,
            'errors': result.stderr if result.stderr else None
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
def _filter_lint_output(lines: List[str]) -> List[Dict[str, Any]]:
    """Helper to filter and format lint output for LLM consumption"""
    issues = []
    for line in lines:
        if not line.strip():
            continue
        # Parse ruff output format
        try:
            file_path, line_no, message = line.split(':', 2)
            issues.append({
                'file': file_path.strip(),
                'line': int(line_no),
                'message': message.strip()
            })
        except ValueError:
            continue
    return issues
@mcp.tool()
def monitor_performance(duration: int = 60, interval: float = 1.0) -> Dict[str, Any]:
    """
    Monitor system performance metrics
    Args:
        duration: Monitoring duration in seconds
        interval: Sampling interval in seconds
    Returns:
        Dictionary with performance metrics
    """
    try:
        import psutil
        from datetime import datetime, timedelta
        metrics = {
            'cpu': [],
            'memory': [],
            'disk': [],
            'network': []
        }
        start_time = datetime.now()
        end_time = start_time + timedelta(seconds=duration)
        while datetime.now() < end_time:
            # CPU metrics
            metrics['cpu'].append({
                'timestamp': datetime.now().isoformat(),
                'percent': psutil.cpu_percent(interval=0.1),
                'count': psutil.cpu_count(),
                'freq': psutil.cpu_freq()._asdict() if psutil.cpu_freq() else None
            })
            # Memory metrics
            mem = psutil.virtual_memory()
            metrics['memory'].append({
                'timestamp': datetime.now().isoformat(),
                'total': mem.total,
                'available': mem.available,
                'percent': mem.percent,
                'used': mem.used,
                'free': mem.free
            })
            # Disk metrics
            disk = psutil.disk_usage('/')
            metrics['disk'].append({
                'timestamp': datetime.now().isoformat(),
                'total': disk.total,
                'used': disk.used,
                'free': disk.free,
                'percent': disk.percent
            })
            # Network metrics
            net = psutil.net_io_counters()
            metrics['network'].append({
                'timestamp': datetime.now().isoformat(),
                'bytes_sent': net.bytes_sent,
                'bytes_recv': net.bytes_recv,
                'packets_sent': net.packets_sent,
                'packets_recv': net.packets_recv
            })
            time.sleep(interval)
        # Calculate summary statistics
        summary = {
            'cpu': {
                'avg': sum(m['percent'] for m in metrics['cpu']) / len(metrics['cpu']),
                'max': max(m['percent'] for m in metrics['cpu']),
                'min': min(m['percent'] for m in metrics['cpu'])
            },
            'memory': {
                'avg_percent': sum(m['percent'] for m in metrics['memory']) / len(metrics['memory']),
                'max_percent': max(m['percent'] for m in metrics['memory']),
                'min_available': min(m['available'] for m in metrics['memory'])
            },
            'disk': {
                'avg_percent': sum(m['percent'] for m in metrics['disk']) / len(metrics['disk']),
                'available': metrics['disk'][-1]['free']
            },
            'network': {
                'total_sent': metrics['network'][-1]['bytes_sent'] - metrics['network'][0]['bytes_sent'],
                'total_recv': metrics['network'][-1]['bytes_recv'] - metrics['network'][0]['bytes_recv']
            }
        }
        return {
            'status': 'success',
            'metrics': metrics,
            'summary': summary,
            'duration': duration,
            'samples': len(metrics['cpu'])
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
@mcp.tool()
def generate_documentation(target: str, doc_type: str = "api", template: str = None) -> Dict[str, Any]:
    """Generate documentation for code.
    Args:
        target: File or directory to generate docs for
        doc_type: Type of documentation ('api', 'readme', 'wiki')
        template: Optional template file to use
    Returns:
        Dictionary with generated documentation
    """
    try:
        if doc_type == "api":
            return _generate_api_docs(target)
        elif doc_type == "readme":
            return _generate_readme(target, template)
        elif doc_type == "wiki":
            return _generate_wiki(target, template)
        else:
            return {
                "status": "error",
                "error": f"Unknown documentation type: {doc_type}"
            }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _generate_api_docs(target: str) -> Dict[str, Any]:
    """Generate API documentation using pdoc."""
    try:
        import pdoc
        # Generate HTML documentation
        doc = pdoc.doc.Module(pdoc.import_module(target))
        html = doc.html()
        # Save to file
        output_dir = "docs/api"
        os.makedirs(output_dir, exist_ok=True)
        output_file = os.path.join(output_dir, f"{target}.html")
        with open(output_file, "w") as f:
            f.write(html)
        return {
            "status": "success",
            "output_file": output_file,
            "module": target
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _generate_readme(target: str, template: str = None) -> Dict[str, Any]:
    """Generate README documentation."""
    try:
        # Get project info
        project_info = _analyze_project_info(target)
        # Load template or use default
        if template and os.path.exists(template):
            with open(template) as f:
                template_content = f.read()
        else:
            template_content = DEFAULT_README_TEMPLATE
        # Generate README content
        content = template_content.format(
            project_name=project_info["name"],
            description=project_info["description"],
            setup=project_info["setup"],
            usage=project_info["usage"],
            api=project_info["api"],
            contributing=project_info["contributing"]
        )
        # Save README
        output_file = os.path.join(target, "README.md")
        with open(output_file, "w") as f:
            f.write(content)
        return {
            "status": "success",
            "output_file": output_file,
            "content": content
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _generate_wiki(target: str, template: str = None) -> Dict[str, Any]:
    """Generate wiki documentation."""
    try:
        # Analyze codebase
        analysis = _analyze_codebase_for_wiki(target)
        # Generate wiki pages
        pages = {}
        wiki_dir = "docs/wiki"
        os.makedirs(wiki_dir, exist_ok=True)
        for topic, content in analysis.items():
            page_file = os.path.join(wiki_dir, f"{topic}.md")
            with open(page_file, "w") as f:
                f.write(content)
            pages[topic] = page_file
        return {
            "status": "success",
            "pages": pages,
            "index": os.path.join(wiki_dir, "index.md")
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _setup_validation_gates_internal(config: Dict[str, Any] = None) -> Dict[str, Any]:
    """Set up validation gates for the project."""
    if config is None:
        config = {
            "pre_commit": True,
            "ci": {
                "linting": True,
                "testing": True,
                "coverage": {
                    "required": 90
                },
                "security": True
            },
            "benchmarks": {
                "performance": True,
                "memory": True
            }
        }
    results = {}
    # Set up pre-commit hooks
    if config.get("pre_commit"):
        results["pre_commit"] = _setup_pre_commit_hooks(config)
    # Set up CI validation
    if config.get("ci"):
        results["ci"] = _setup_ci_validation(config["ci"])
    # Set up benchmarks
    if config.get("benchmarks"):
        results["benchmarks"] = _setup_benchmarks(config["benchmarks"])
    return {
        "status": "success" if all(r.get("status") == "success" for r in results.values()) else "error",
        "results": results
    }
def _analyze_project_internal(path: str = ".") -> Dict[str, Any]:
    """Analyze project for documentation and insights."""
    try:
        # Get project info
        info = _analyze_project_info(path)
        # Get wiki content
        wiki = _analyze_codebase_for_wiki(path)
        # Analyze dependencies
        dependencies = {}
        if os.path.exists("requirements.txt"):
            with open("requirements.txt") as f:
                dependencies["requirements"] = [line.strip() for line in f if line.strip()]
        elif os.path.exists("pyproject.toml"):
            with open("pyproject.toml") as f:
                content = f.read()
                deps_match = re.search(r'dependencies\s*=\s*\[(.*?)\]', content, re.DOTALL)
                if deps_match:
                    dependencies["pyproject"] = [
                        dep.strip().strip('"\'') 
                        for dep in deps_match.group(1).split(",")
                        if dep.strip()
                    ]
        # Analyze code metrics
        metrics = {
            "files": 0,
            "lines": 0,
            "functions": 0,
            "classes": 0,
            "imports": set()
        }
        for root, _, files in os.walk(path):
            if ".git" in root or "__pycache__" in root:
                continue
            for file in files:
                if file.endswith(".py"):
                    metrics["files"] += 1
                    file_path = os.path.join(root, file)
                    with open(file_path) as f:
                        content = f.read()
                    metrics["lines"] += len(content.splitlines())
                    tree = ast.parse(content)
                    metrics["functions"] += len([
                        node for node in ast.walk(tree) 
                        if isinstance(node, ast.FunctionDef)
                    ])
                    metrics["classes"] += len([
                        node for node in ast.walk(tree)
                        if isinstance(node, ast.ClassDef)
                    ])
                    metrics["imports"].update([
                        node.names[0].name
                        for node in ast.walk(tree)
                        if isinstance(node, ast.Import) and node.names
                    ])
                    metrics["imports"].update([
                        node.module
                        for node in ast.walk(tree)
                        if isinstance(node, ast.ImportFrom) and node.module
                    ])
        metrics["imports"] = sorted(metrics["imports"])
        return {
            "status": "success",
            "info": info,
            "wiki": wiki,
            "dependencies": dependencies,
            "metrics": metrics
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _manage_changes_internal(action: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
    """Manage code changes and releases."""
    try:
        if action == "branch":
            return _manage_branch(params)
        elif action == "pr":
            return _create_pull_request(params)
        elif action == "release":
            return _create_release(params)
        elif action == "changelog":
            return _generate_changelog(params)
        else:
            return {
                "status": "error",
                "error": f"Unknown action: {action}"
            }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
@mcp.tool()
def setup_validation_gates(config: Dict[str, Any] = None) -> Dict[str, Any]:
    """Set up validation gates for the project.
    Args:
        config: Configuration for validation gates. If None, uses default config.
    Returns:
        Dict with setup results for each validation gate.
    """
    return _setup_validation_gates_internal(config)
@mcp.tool()
def analyze_project(path: str = ".") -> Dict[str, Any]:
    """Analyze project for documentation and insights.
    Args:
        path: Path to project root. Defaults to current directory.
    Returns:
        Dict with project analysis results.
    """
    return _analyze_project_internal(path)
@mcp.tool()
def manage_changes(action: str, params: Dict[str, Any] = None) -> Dict[str, Any]:
    """Manage code changes and releases.
    Args:
        action: Action to perform ('branch', 'pr', 'release', 'changelog')
        params: Parameters for the action
    Returns:
        Dict with action results.
    """
    return _manage_changes_internal(action, params)
# Constants
DEFAULT_README_TEMPLATE = """
# {project_name}
{description}
## Setup
{setup}
## Usage
{usage}
## API Documentation
{api}
## Contributing
{contributing}
"""
DEFAULT_VALIDATION_CONFIG = {
    "pre_commit": {
        "hooks": [
            "black",
            "flake8",
            "mypy",
            "pytest"
        ]
    },
    "ci": {
        "linting": True,
        "testing": True,
        "coverage": {
            "required": 90
        },
        "security": True
    },
    "benchmarks": {
        "performance": True,
        "memory": True
    }
}
def _setup_pre_commit_hooks(config: Dict[str, Any]) -> Dict[str, Any]:
    """Set up pre-commit hooks."""
    try:
        hooks = config.get("hooks", [])
        # Create pre-commit config
        pre_commit_config = {
            "repos": [
                {
                    "repo": "https://github.com/psf/black",
                    "rev": "stable",
                    "hooks": [{"id": "black"}]
                },
                {
                    "repo": "https://github.com/pycqa/flake8",
                    "rev": "master",
                    "hooks": [{"id": "flake8"}]
                },
                {
                    "repo": "https://github.com/pre-commit/mirrors-mypy",
                    "rev": "master",
                    "hooks": [{"id": "mypy"}]
                }
            ]
        }
        # Save config
        with open(".pre-commit-config.yaml", "w") as f:
            yaml.dump(pre_commit_config, f)
        # Install hooks
        result = subprocess.run(
            ["pre-commit", "install"],
            capture_output=True,
            text=True
        )
        return {
            "status": "success" if result.returncode == 0 else "error",
            "output": result.stdout,
            "error": result.stderr if result.returncode != 0 else None
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _setup_ci_validation(config: Dict[str, Any]) -> Dict[str, Any]:
    """Set up CI validation."""
    try:
        # Create GitHub Actions workflow
        workflow = {
            "name": "CI",
            "on": ["push", "pull_request"],
            "jobs": {
                "validate": {
                    "runs-on": "ubuntu-latest",
                    "steps": [
                        {
                            "uses": "actions/checkout@v2"
                        },
                        {
                            "uses": "actions/setup-python@v2",
                            "with": {
                                "python-version": "3.x"
                            }
                        }
                    ]
                }
            }
        }
        # Add validation steps based on config
        steps = workflow["jobs"]["validate"]["steps"]
        if config.get("linting"):
            steps.append({
                "name": "Lint",
                "run": "pip install flake8 && flake8"
            })
        if config.get("testing"):
            steps.append({
                "name": "Test",
                "run": "pip install pytest && pytest"
            })
        if config.get("coverage"):
            steps.append({
                "name": "Coverage",
                "run": f"pip install pytest-cov && pytest --cov=. --cov-fail-under={config['coverage'].get('required', 90)}"
            })
        if config.get("security"):
            steps.append({
                "name": "Security Check",
                "run": "pip install bandit && bandit -r ."
            })
        # Save workflow
        os.makedirs(".github/workflows", exist_ok=True)
        with open(".github/workflows/ci.yml", "w") as f:
            yaml.dump(workflow, f)
        return {
            "status": "success",
            "workflow": workflow
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _setup_benchmarks(config: Dict[str, Any]) -> Dict[str, Any]:
    """Set up performance benchmarks."""
    try:
        benchmarks = []
        if config.get("performance"):
            # Create performance benchmark
            benchmark = """
import pytest
from your_module import your_function
@pytest.mark.benchmark
def test_performance(benchmark):
    result = benchmark(your_function)
    assert result  # Add appropriate assertion
"""
            benchmarks.append(("tests/test_performance.py", benchmark))
        if config.get("memory"):
            # Create memory benchmark
            benchmark = """
import pytest
import memory_profiler
@pytest.mark.benchmark
def test_memory():
    @memory_profiler.profile
    def wrapper():
        # Add your function call here
        pass
    wrapper()
"""
            benchmarks.append(("tests/test_memory.py", benchmark))
        # Save benchmarks
        os.makedirs("tests", exist_ok=True)
        for file_path, content in benchmarks:
            with open(file_path, "w") as f:
                f.write(content)
        return {
            "status": "success",
            "benchmarks": [path for path, _ in benchmarks]
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _analyze_project_info(path: str) -> Dict[str, Any]:
    """Analyze project information for documentation."""
    try:
        info = {
            "name": os.path.basename(os.path.abspath(path)),
            "description": "",
            "setup": "",
            "usage": "",
            "api": "",
            "contributing": ""
        }
        # Try to get description from setup.py/pyproject.toml
        if os.path.exists("setup.py"):
            with open("setup.py") as f:
                content = f.read()
                desc_match = re.search(r'description\s*=\s*[\'"](.+?)[\'"]', content)
                if desc_match:
                    info["description"] = desc_match.group(1)
        elif os.path.exists("pyproject.toml"):
            with open("pyproject.toml") as f:
                content = f.read()
                desc_match = re.search(r'description\s*=\s*[\'"](.+?)[\'"]', content)
                if desc_match:
                    info["description"] = desc_match.group(1)
        # Get setup instructions
        if os.path.exists("pyproject.toml"):
            info["setup"] = """
1. Install dependencies:
   ```
   pip install .
   ```
"""
        elif os.path.exists("requirements.txt"):
            info["setup"] = """
1. Install dependencies:
   ```
   pip install -r requirements.txt
   ```
"""
        # Get usage examples from docstrings
        for root, _, files in os.walk(path):
            for file in files:
                if file.endswith(".py"):
                    with open(os.path.join(root, file)) as f:
                        content = f.read()
                        docstring_match = re.search(r'"""(.+?)"""', content, re.DOTALL)
                        if docstring_match and "Example" in docstring_match.group(1):
                            info["usage"] += f"\n### {file}\n\n{docstring_match.group(1)}"
        # Get API documentation
        info["api"] = "See the [API Documentation](docs/api/index.html) for detailed reference."
        # Get contributing guidelines
        if os.path.exists("CONTRIBUTING.md"):
            with open("CONTRIBUTING.md") as f:
                info["contributing"] = f.read()
        else:
            info["contributing"] = """
1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Submit a pull request
"""
        return info
    except Exception as e:
        return {
            "name": "Unknown",
            "description": "Error analyzing project info: " + str(e),
            "setup": "",
            "usage": "",
            "api": "",
            "contributing": ""
        }
def _analyze_codebase_for_wiki(path: str) -> Dict[str, str]:
    """Analyze codebase for wiki documentation."""
    try:
        analysis = {
            "index": "# Project Wiki\n\n",
            "architecture": "# Architecture\n\n",
            "modules": "# Modules\n\n",
            "workflows": "# Workflows\n\n",
            "development": "# Development Guide\n\n"
        }
        # Analyze architecture
        analysis["architecture"] += "## Overview\n\n"
        for root, dirs, files in os.walk(path):
            if ".git" in dirs:
                dirs.remove(".git")
            rel_path = os.path.relpath(root, path)
            if rel_path == ".":
                analysis["architecture"] += "Project structure:\n\n```\n"
            else:
                analysis["architecture"] += "  " * rel_path.count(os.sep) + rel_path.split(os.sep)[-1] + "/\n"
            for file in sorted(files):
                if file.endswith(".py"):
                    analysis["architecture"] += "  " * (rel_path.count(os.sep) + 1) + file + "\n"
        analysis["architecture"] += "```\n"
        # Analyze modules
        for root, _, files in os.walk(path):
            for file in files:
                if file.endswith(".py"):
                    with open(os.path.join(root, file)) as f:
                        content = f.read()
                    # Extract classes and functions
                    tree = ast.parse(content)
                    classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
                    functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
                    if classes or functions:
                        rel_path = os.path.relpath(os.path.join(root, file), path)
                        analysis["modules"] += f"## {rel_path}\n\n"
                        if classes:
                            analysis["modules"] += "### Classes\n\n"
                            for cls in classes:
                                analysis["modules"] += f"- `{cls}`\n"
                        if functions:
                            analysis["modules"] += "\n### Functions\n\n"
                            for func in functions:
                                analysis["modules"] += f"- `{func}`\n"
                        analysis["modules"] += "\n"
        # Add development guide
        analysis["development"] += """
## Setup Development Environment
1. Clone the repository
2. Install dependencies
3. Set up pre-commit hooks
4. Run tests
## Code Style
Follow PEP 8 guidelines and use the provided linting tools.
## Testing
Write tests for new features and ensure all tests pass before submitting changes.
## Pull Request Process
1. Create a feature branch
2. Make your changes
3. Run tests and linting
4. Submit a pull request
"""
        # Update index
        analysis["index"] += """
- [Architecture](architecture.md)
- [Modules](modules.md)
- [Workflows](workflows.md)
- [Development Guide](development.md)
"""
        return analysis
    except Exception as e:
        return {
            "index": f"Error analyzing codebase: {str(e)}"
        }
def _manage_branch(params: Dict[str, Any]) -> Dict[str, Any]:
    """Manage git branches."""
    try:
        action = params.get("action")
        branch = params.get("branch")
        if not action or not branch:
            return {
                "status": "error",
                "error": "Missing required parameters"
            }
        if action == "create":
            cmd = ["git", "checkout", "-b", branch]
        elif action == "delete":
            cmd = ["git", "branch", "-D", branch]
        elif action == "merge":
            target = params.get("target", "main")
            cmd = ["git", "merge", branch, target]
        else:
            return {
                "status": "error",
                "error": f"Unknown branch action: {action}"
            }
        result = subprocess.run(cmd, capture_output=True, text=True)
        return {
            "status": "success" if result.returncode == 0 else "error",
            "output": result.stdout,
            "error": result.stderr if result.returncode != 0 else None
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _create_pull_request(params: Dict[str, Any]) -> Dict[str, Any]:
    """Create a pull request."""
    try:
        title = params.get("title")
        body = params.get("body")
        base = params.get("base", "main")
        head = params.get("head")
        if not all([title, body, head]):
            return {
                "status": "error",
                "error": "Missing required parameters"
            }
        # Create PR using GitHub CLI
        cmd = [
            "gh", "pr", "create",
            "--title", title,
            "--body", body,
            "--base", base,
            "--head", head
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        return {
            "status": "success" if result.returncode == 0 else "error",
            "output": result.stdout,
            "error": result.stderr if result.returncode != 0 else None
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _create_release(params: Dict[str, Any]) -> Dict[str, Any]:
    """Create a release."""
    try:
        version = params.get("version")
        notes = params.get("notes")
        if not version:
            return {
                "status": "error",
                "error": "Version is required"
            }
        # Create git tag
        tag_cmd = ["git", "tag", "-a", f"v{version}", "-m", f"Release {version}"]
        tag_result = subprocess.run(tag_cmd, capture_output=True, text=True)
        if tag_result.returncode != 0:
            return {
                "status": "error",
                "error": f"Failed to create tag: {tag_result.stderr}"
            }
        # Push tag
        push_cmd = ["git", "push", "origin", f"v{version}"]
        push_result = subprocess.run(push_cmd, capture_output=True, text=True)
        if push_result.returncode != 0:
            return {
                "status": "error",
                "error": f"Failed to push tag: {push_result.stderr}"
            }
        # Create GitHub release
        release_cmd = [
            "gh", "release", "create",
            f"v{version}",
            "--title", f"Release {version}",
            "--notes", notes or f"Release {version}"
        ]
        release_result = subprocess.run(release_cmd, capture_output=True, text=True)
        return {
            "status": "success" if release_result.returncode == 0 else "error",
            "version": version,
            "output": release_result.stdout,
            "error": release_result.stderr if release_result.returncode != 0 else None
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _generate_changelog(params: Dict[str, Any]) -> Dict[str, Any]:
    """Generate a changelog."""
    try:
        since = params.get("since")
        until = params.get("until", "HEAD")
        # Get git log
        cmd = [
            "git", "log",
            "--pretty=format:%h %s",
            f"{since}..{until}" if since else ""
        ]
        result = subprocess.run(cmd, capture_output=True, text=True)
        if result.returncode != 0:
            return {
                "status": "error",
                "error": f"Failed to get git log: {result.stderr}"
            }
        # Parse commits and categorize
        changes = {
            "features": [],
            "fixes": [],
            "docs": [],
            "other": []
        }
        for line in result.stdout.split("\n"):
            if not line:
                continue
            hash, message = line.split(" ", 1)
            if message.startswith("feat"):
                changes["features"].append((hash, message))
            elif message.startswith("fix"):
                changes["fixes"].append((hash, message))
            elif message.startswith("docs"):
                changes["docs"].append((hash, message))
            else:
                changes["other"].append((hash, message))
        # Generate markdown
        content = ["# Changelog\n"]
        for category, commits in changes.items():
            if commits:
                content.append(f"\n## {category.title()}\n")
                for hash, message in commits:
                    content.append(f"- [{hash}] {message}")
        changelog = "\n".join(content)
        # Save to file
        output_file = "CHANGELOG.md"
        with open(output_file, "w") as f:
            f.write(changelog)
        return {
            "status": "success",
            "output_file": output_file,
            "content": changelog
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e)
        }
def _analyze_security(node: ast.AST) -> Dict[str, Any]:
    """Analyze code for security issues."""
    issues = []
    recommendations = []
    class SecurityVisitor(ast.NodeVisitor):
        def visit_Import(self, node):
            dangerous_imports = {
                "os": "System access",
                "subprocess": "Command execution",
                "pickle": "Unsafe deserialization",
                "marshal": "Unsafe deserialization",
                "shelve": "Unsafe file access"
            }
            for name in node.names:
                if name.name in dangerous_imports:
                    issues.append(f"Dangerous import: {name.name} ({dangerous_imports[name.name]})")
                    recommendations.append(f"Consider using a safer alternative to {name.name}")
            self.generic_visit(node)
        def visit_ImportFrom(self, node):
            dangerous_modules = {
                "os": "System access",
                "subprocess": "Command execution",
                "pickle": "Unsafe deserialization",
                "marshal": "Unsafe deserialization",
                "shelve": "Unsafe file access"
            }
            if node.module in dangerous_modules:
                issues.append(f"Dangerous import: {node.module} ({dangerous_modules[node.module]})")
                recommendations.append(f"Consider using a safer alternative to {node.module}")
            self.generic_visit(node)
        def visit_Call(self, node):
            dangerous_functions = {
                "eval": "Code execution",
                "exec": "Code execution",
                "input": "Unsanitized input",
                "open": "File access"
            }
            if isinstance(node.func, ast.Name):
                if node.func.id in dangerous_functions:
                    issues.append(f"Dangerous function call: {node.func.id} ({dangerous_functions[node.func.id]})")
                    recommendations.append(f"Replace {node.func.id}() with a safer alternative")
            self.generic_visit(node)
    visitor = SecurityVisitor()
    visitor.visit(node)
    status = "success"
    if issues:
        status = "error"
    return {
        "status": status,
        "issues": issues,
        "recommendations": recommendations
    }
def _analyze_style(code: str) -> Dict[str, Any]:
    """Analyze code style."""
    issues = []
    recommendations = []
    try:
        tree = ast.parse(code)
        class StyleVisitor(ast.NodeVisitor):
            def visit_Name(self, node):
                if not node.id.islower() and not node.id.isupper():
                    issues.append(f"Variable name '{node.id}' should be lowercase with underscores")
                    recommendations.append("Use lowercase with underscores for variable names")
                elif len(node.id) == 1 and node.id not in ['i', 'j', 'k', 'n', 'm']:
                    issues.append(f"Single-letter variable name '{node.id}' should be more descriptive")
                    recommendations.append("Use descriptive variable names")
                self.generic_visit(node)
            def visit_FunctionDef(self, node):
                if not node.name.islower():
                    issues.append(f"Function name '{node.name}' should be lowercase with underscores")
                    recommendations.append("Use lowercase with underscores for function names")
                if not node.args.args and not isinstance(node.body[0], ast.Expr):
                    issues.append(f"Function '{node.name}' is missing a docstring")
                    recommendations.append("Add docstrings to all functions")
                self.generic_visit(node)
            def visit_ClassDef(self, node):
                if not node.name[0].isupper():
                    issues.append(f"Class name '{node.name}' should use CapWords convention")
                    recommendations.append("Use CapWords for class names")
                if not isinstance(node.body[0], ast.Expr):
                    issues.append(f"Class '{node.name}' is missing a docstring")
                    recommendations.append("Add docstrings to all classes")
                self.generic_visit(node)
        visitor = StyleVisitor()
        visitor.visit(tree)
        # Check line length
        lines = code.split('\n')
        for i, line in enumerate(lines, 1):
            if len(line.strip()) > 100:
                issues.append(f"Line {i} is too long (>100 characters)")
                recommendations.append("Keep lines under 100 characters")
        status = "success"
        if issues:
            status = "warning"
        return {
            "status": status,
            "issues": issues,
            "recommendations": list(set(recommendations))  # Remove duplicates
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "message": "Failed to analyze style"
        }
def force_terminate(pid: int) -> Dict[str, Any]:
    """Force terminate a process.
    Args:
        pid: Process ID to terminate
    Returns:
        Dictionary containing success status
    """
    try:
        os.kill(pid, signal.SIGTERM)
        return {"success": True}
    except ProcessLookupError:
        return {"success": True}  # Process already terminated
    except Exception as e:
        return {"success": False, "error": str(e)}
def block_command(command: str) -> Dict[str, Any]:
    """Add a command to the blacklist.
    Args:
        command: Command to block
    Returns:
        Dictionary containing success status
    """
    try:
        blacklisted_commands.add(command)
        return {"success": True}
    except Exception as e:
        return {"success": False, "error": str(e)}
def unblock_command(command: str) -> Dict[str, Any]:
    """Remove a command from the blacklist.
    Args:
        command: Command to unblock
    Returns:
        Dictionary containing success status
    """
    try:
        blacklisted_commands.remove(command)
        return {"success": True}
    except Exception as e:
        return {"success": False, "error": str(e)}
def main():
    # Set up the server
    import uvicorn
    print("Starting server from MAIN")
    uvicorn.run(mcp.app, host="0.0.0.0", port=8000)
    # Only run the SSE transport when the script is run directly
    mcp.run(transport="sse")
    # Initialize tools
    asyncio.run(add_metrics_to_tools())
    asyncio.run(add_tracing_to_tools())
    asyncio.run(add_profiling_to_tools())
    update_active_sessions_metric()
if __name__ == "__main__":
    main()
# mcp.run(transport="sse")
</file>

<file path="server/llm.py">
from mcp.server.fastmcp import FastMCP
import os
import json
from typing import Dict, Any, Optional, List
import torch
from transformers import pipeline
import anthropic
import openai
from opentelemetry import trace
from opentelemetry.sdk.trace import TracerProvider
from opentelemetry.sdk.trace.export import BatchSpanProcessor
from opentelemetry.exporter.otlp.proto.grpc.trace_exporter import OTLPSpanExporter
from opentelemetry.sdk.resources import Resource
from opentelemetry.semconv.resource import ResourceAttributes
from functools import wraps
import re
import sys
# Initialize the MCP server
mcp = FastMCP("LLM Tools MCP", port=7444, log_level="DEBUG")
# Initialize tracer
resource = Resource(attributes={
    ResourceAttributes.SERVICE_NAME: "llm-mcp-server",
    ResourceAttributes.SERVICE_VERSION: "1.0.0",
})
# Configure tracing
trace.set_tracer_provider(TracerProvider(resource=resource))
tracer = trace.get_tracer(__name__)
otlp_exporter = OTLPSpanExporter(endpoint="http://localhost:4317")
span_processor = BatchSpanProcessor(otlp_exporter)
trace.get_tracer_provider().add_span_processor(span_processor)
def trace_tool(func):
    """Decorator to add tracing to MCP tools"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        with tracer.start_as_current_span(
            name=f"mcp.tool.{func.__name__}",
            attributes={
                "mcp.tool.name": func.__name__,
                "mcp.tool.args": str(args),
                "mcp.tool.kwargs": str(kwargs)
            }
        ) as span:
            try:
                result = func(*args, **kwargs)
                if isinstance(result, dict):
                    span.set_attribute("mcp.tool.status", result.get("status", "unknown"))
                    if "error" in result:
                        span.set_attribute("mcp.tool.error", result["error"])
                return result
            except Exception as e:
                span.set_attribute("mcp.tool.error", str(e))
                span.record_exception(e)
                raise
    return wrapper
@mcp.tool()
@trace_tool
def generate_code(prompt: str, model: str = "claude-3-sonnet", context: Optional[Dict[str, Any]] = None, system_prompt: Optional[str] = None) -> Dict[str, Any]:
    """Generate code using various models."""
    try:
        if not prompt:
            return {
                "status": "error",
                "error": "Empty prompt provided",
                "language": "python"
            }
        # Get workspace info
        workspace_info = _get_workspace_info()
        # Prepare context
        full_context = {
            "workspace": workspace_info,
            **(context or {})
        }
        # Get system prompt
        if system_prompt is None:
            system_prompt = _get_default_system_prompt("python")
        # Generate code based on model type
        if model in ["claude-3-sonnet", "claude-3-opus"]:
            result = _generate_with_api_model(
                prompt=prompt,
                model=model,
                system_prompt=system_prompt,
                max_tokens=None,
                temperature=0.7
            )
        elif model in ["gpt-4", "gpt-3.5-turbo"]:
            result = _generate_with_api_model(
                prompt=prompt,
                model=model,
                system_prompt=system_prompt,
                max_tokens=None,
                temperature=0.7
            )
        elif model in ["code-llama", "starcoder"]:
            result = _generate_with_local_model(
                prompt=prompt,
                model=model,
                system_prompt=system_prompt,
                max_tokens=None,
                temperature=0.7
            )
        else:
            return {
                "status": "error",
                "error": f"Invalid model: {model}",
                "language": "python"
            }
        return {
            "status": "success",
            "code": result,
            "model": model,
            "language": "python"
        }
    except Exception as e:
        return {
            "status": "error",
            "error": str(e),
            "model": model,
            "language": "python"
        }
@mcp.tool()
@trace_tool
def manage_llm_context(content: str, model: str = "claude-3-sonnet", max_tokens: int = None) -> Dict[str, Any]:
    """Advanced LLM context management and optimization."""
    try:
        # Model context limits
        model_limits = {
            'claude-3-opus': 200000,
            'claude-3-sonnet': 100000,
            'gpt-4': 128000,
            'gpt-3.5': 16000
        }
        if model not in model_limits:
            return {
                'status': 'error',
                'error': f'Unknown model: {model}'
            }
        # Use specified max_tokens or model limit
        token_limit = max_tokens or model_limits[model]
        # Analyze content
        words = content.split()
        chars = len(content)
        lines = content.count('\n') + 1
        # Estimate tokens (improved estimation)
        estimated_tokens = int(len(words) * 1.3)  # Rough approximation
        # Calculate context metrics
        metrics = {
            'estimated_tokens': estimated_tokens,
            'words': len(words),
            'characters': chars,
            'lines': lines,
            'usage_percent': (estimated_tokens / token_limit) * 100
        }
        # Generate optimization suggestions
        suggestions = []
        if estimated_tokens > token_limit:
            suggestions.append({
                'type': 'truncation',
                'message': f'Content exceeds {model} token limit by approximately {estimated_tokens - token_limit} tokens'
            })
            # Suggest specific optimizations
            if lines > 100:
                suggestions.append({
                    'type': 'structure',
                    'message': 'Consider reducing line count by combining related lines'
                })
            code_blocks = len(re.findall(r'```.*?```', content, re.DOTALL))
            if code_blocks > 5:
                suggestions.append({
                    'type': 'code',
                    'message': 'Consider reducing number of code blocks or showing only relevant portions'
                })
        # Optimize content if needed
        optimized_content = content
        if estimated_tokens > token_limit:
            optimized_content = _optimize_content(content, token_limit)
        return {
            'status': 'success',
            'metrics': metrics,
            'suggestions': suggestions,
            'optimized_content': optimized_content if optimized_content != content else None,
            'model': model,
            'token_limit': token_limit
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
@mcp.tool()
@trace_tool
def context_length(text: str) -> Dict[str, Any]:
    """Track LLM context usage."""
    try:
        # Simple tokenization (this is a basic approximation)
        words = text.split()
        characters = len(text)
        lines = text.count('\n') + 1
        # Rough token estimation (OpenAI GPT-style)
        estimated_tokens = len(words) * 1.3
        # Context length limits (example values)
        limits = {
            'claude-3-opus': 200000,
            'claude-3-sonnet': 100000,
            'gpt-4': 128000,
            'gpt-3.5': 16000
        }
        # Calculate percentage of context used
        usage = {model: (estimated_tokens / limit) * 100 for model, limit in limits.items()}
        return {
            'estimated_tokens': int(estimated_tokens),
            'words': len(words),
            'characters': characters,
            'lines': lines,
            'context_usage_percent': usage,
            'approaching_limit': any(pct > 75 for pct in usage.values())
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
@mcp.tool()
@trace_tool
def filter_output(content: str, max_lines: int = 50, important_patterns: List[str] = None) -> Dict[str, Any]:
    """Process and format long command outputs for better LLM consumption."""
    try:
        lines = content.split('\n')
        total_lines = len(lines)
        if not important_patterns:
            important_patterns = [
                r'error', r'warning', r'fail', r'exception',
                r'success', r'completed', r'starting', r'finished'
            ]
        # Always keep lines matching important patterns
        important_lines = []
        other_lines = []
        for line in lines:
            if any(re.search(pattern, line, re.IGNORECASE) for pattern in important_patterns):
                important_lines.append(line)
            else:
                other_lines.append(line)
        # Calculate remaining space for other lines
        remaining_space = max_lines - len(important_lines)
        if remaining_space <= 0:
            filtered_lines = important_lines[:max_lines]
        else:
            # Select a representative sample of other lines
            step = len(other_lines) // remaining_space if remaining_space > 0 else 1
            sampled_lines = other_lines[::step][:remaining_space]
            filtered_lines = important_lines + sampled_lines
        return {
            'filtered_content': '\n'.join(filtered_lines),
            'total_lines': total_lines,
            'included_lines': len(filtered_lines),
            'important_lines': len(important_lines),
            'truncated': total_lines > len(filtered_lines)
        }
    except Exception as e:
        return {
            'status': 'error',
            'error': str(e)
        }
def _get_workspace_info() -> Dict[str, Any]:
    """Get information about the current workspace."""
    return {
        "files": _list_workspace_files(),
        "dependencies": _get_dependencies(),
        "environment": _get_environment_info()
    }
def _get_default_system_prompt(language: str) -> str:
    """Get the default system prompt for code generation."""
    return f"""You are an expert {language} programmer. Generate code that is:
1. Well-documented
2. Follows best practices
3. Is efficient and maintainable
4. Includes error handling
5. Is properly formatted"""
def _generate_with_api_model(prompt: str, model: str, system_prompt: str, max_tokens: Optional[int], temperature: float) -> str:
    """Generate code using API-based models (Claude, GPT)."""
    if model.startswith("claude"):
        client = anthropic.Client()
        response = client.messages.create(
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            system=system_prompt,
            messages=[{"role": "user", "content": prompt}]
        )
        return response.content[0].text
    elif model.startswith("gpt"):
        client = openai.OpenAI()
        response = client.chat.completions.create(
            model=model,
            max_tokens=max_tokens,
            temperature=temperature,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": prompt}
            ]
        )
        return response.choices[0].message.content
    else:
        raise ValueError(f"Unsupported API model: {model}")
def _generate_with_local_model(prompt: str, model: str, system_prompt: str, max_tokens: Optional[int], temperature: float) -> str:
    """Generate code using local models (CodeLlama, StarCoder)."""
    config = _get_local_model_config(model)
    pipe = pipeline(
        "text-generation",
        model=config["model_name"],
        device=config["device"]
    )
    full_prompt = f"{system_prompt}\n\n{prompt}"
    response = pipe(
        full_prompt,
        max_new_tokens=max_tokens or 1000,
        temperature=temperature,
        do_sample=True
    )
    return response[0]["generated_text"]
def _get_local_model_config(model: str) -> Dict[str, Any]:
    """Get configuration for local models."""
    configs = {
        "code-llama": {
            "model_name": "codellama/CodeLlama-34b-Python",
            "device": "cuda" if torch.cuda.is_available() else "cpu"
        },
        "starcoder": {
            "model_name": "bigcode/starcoder",
            "device": "cuda" if torch.cuda.is_available() else "cpu"
        }
    }
    if model not in configs:
        raise ValueError(f"Unknown model: {model}")
    return configs[model]
def _optimize_content(content: str, token_limit: int) -> str:
    """Optimize content to fit within token limit."""
    # Simple optimization: truncate content while preserving important parts
    words = content.split()
    estimated_tokens = len(words) * 1.3
    if estimated_tokens <= token_limit:
        return content
    # Keep important parts (error messages, warnings, etc.)
    important_patterns = [
        r'error', r'warning', r'fail', r'exception',
        r'success', r'completed', r'starting', r'finished'
    ]
    lines = content.split('\n')
    important_lines = []
    other_lines = []
    for line in lines:
        if any(re.search(pattern, line, re.IGNORECASE) for pattern in important_patterns):
            important_lines.append(line)
        else:
            other_lines.append(line)
    # Calculate how many other lines we can keep
    important_tokens = sum(len(line.split()) * 1.3 for line in important_lines)
    remaining_tokens = token_limit - important_tokens
    other_tokens_per_line = sum(len(line.split()) * 1.3 for line in other_lines) / len(other_lines)
    max_other_lines = int(remaining_tokens / other_tokens_per_line)
    # Select a representative sample of other lines
    if max_other_lines > 0:
        step = len(other_lines) // max_other_lines if max_other_lines > 0 else 1
        sampled_lines = other_lines[::step][:max_other_lines]
        return '\n'.join(important_lines + sampled_lines)
    else:
        return '\n'.join(important_lines[:int(token_limit / (sum(len(line.split()) * 1.3 for line in important_lines) / len(important_lines)))])
def _list_workspace_files() -> List[str]:
    """List files in the current workspace."""
    try:
        return [f for f in os.listdir('.') if os.path.isfile(f)]
    except Exception:
        return []
def _get_dependencies() -> Dict[str, str]:
    """Get project dependencies."""
    try:
        with open('requirements.txt', 'r') as f:
            return {line.split('==')[0]: line.split('==')[1] if '==' in line else 'latest'
                   for line in f if line.strip() and not line.startswith('#')}
    except Exception:
        return {}
def _get_environment_info() -> Dict[str, str]:
    """Get environment information."""
    return {
        "python_version": sys.version,
        "platform": sys.platform,
        "cuda_available": torch.cuda.is_available() if torch else False
    }
</file>

<file path="tests/code_understanding/conftest.py">
"""Shared fixtures for code understanding tests."""
import pytest
from server.code_understanding import CodeParser, CodeAnalyzer, SymbolExtractor
@pytest.fixture
def mock_tree_sitter(mocker):
    """Mock tree-sitter components."""
    mock_language = mocker.MagicMock()
    mock_parser = mocker.MagicMock()
    mock_tree = mocker.MagicMock()
    mock_root = mocker.MagicMock()
    mocker.patch('tree_sitter.Language', return_value=mock_language)
    mocker.patch('tree_sitter.Parser', return_value=mock_parser)
    mock_parser.parse.return_value = mock_tree
    mock_tree.root_node = mock_root
    return {
        'language': mock_language,
        'parser': mock_parser,
        'tree': mock_tree,
        'root': mock_root
    }
@pytest.fixture
def mock_node_factory(mocker):
    """Factory for creating mock nodes."""
    def create_node(node_type, text, start_point, end_point):
        node = mocker.MagicMock()
        node.type = node_type
        node.text = text if isinstance(text, bytes) else text.encode()
        node.start_point = start_point
        node.end_point = end_point
        return node
    return create_node
@pytest.fixture
def sample_python_code():
    """Sample Python code for testing."""
    return '''
import os
from typing import List, Optional
class BaseClass:
    def base_method(self) -> None:
        pass
class TestClass(BaseClass):
    def __init__(self, name: str):
        self.name = name
    def test_method(self, value: int) -> Optional[str]:
        result = f"Processing {value}"
        return result if value > 0 else None
def helper_function(items: List[int]) -> int:
    return sum(items)
test_variable = "Hello World"
numbers = [1, 2, 3]
result = helper_function(numbers)
'''
@pytest.fixture
def mock_analysis_result():
    """Sample analysis result for testing."""
    return {
        'imports': [{
            'name': 'os',
            'type': 'import',
            'start_line': 1,
            'end_line': 1
        }],
        'functions': [{
            'name': 'test_func',
            'start_line': 3,
            'end_line': 5,
            'parameters': [{'name': 'x', 'type': 'int'}]
        }],
        'classes': [{
            'name': 'TestClass',
            'start_line': 7,
            'end_line': 12,
            'methods': [{
                'name': 'test_method',
                'start_line': 8,
                'end_line': 9,
                'parameters': []
            }],
            'bases': ['BaseClass']
        }],
        'variables': [{
            'name': 'test_var',
            'start_line': 14,
            'end_line': 14,
            'type': 'str'
        }]
    }
@pytest.fixture
def code_parser(mock_tree_sitter):
    """Create a CodeParser instance with mocked dependencies."""
    parser = CodeParser()
    parser.language = mock_tree_sitter['language']
    parser.parser = mock_tree_sitter['parser']
    return parser
@pytest.fixture
def code_analyzer():
    """Create a CodeAnalyzer instance."""
    return CodeAnalyzer()
@pytest.fixture
def symbol_extractor():
    """Create a SymbolExtractor instance."""
    return SymbolExtractor()
</file>

<file path="tests/code_understanding/test_analyzer.py">
"""Tests for the CodeAnalyzer class."""
import pytest
from server.code_understanding.analyzer import CodeAnalyzer
from server.code_understanding.parser import MockNode, MockTree
@pytest.fixture
def analyzer():
    """Create a CodeAnalyzer instance for testing."""
    return CodeAnalyzer()
def test_analyze_tree(analyzer):
    """Test analyzing a complete syntax tree."""
    # Create mock nodes
    mock_import = MockNode(
        type="import",
        text="import os",
        start_point=(1, 0),
        end_point=(1, 9)
    )
    mock_function = MockNode(
        type="function_definition",
        text="test_func",
        start_point=(2, 0),
        end_point=(4, 0),
        fields={"name": MockNode(type="identifier", text="test_func")}
    )
    mock_class = MockNode(
        type="class_definition",
        text="TestClass",
        start_point=(5, 0),
        end_point=(7, 0),
        fields={"name": MockNode(type="identifier", text="TestClass")}
    )
    mock_var = MockNode(
        type="assignment",
        text="test_var = 'hello'",
        start_point=(8, 0),
        end_point=(8, 10),
        fields={
            "left": MockNode(type="identifier", text="test_var"),
            "right": MockNode(type="string", text="'hello'")
        }
    )
    # Create root node with all nodes as children
    root = MockNode(
        type="module",
        children=[mock_import, mock_function, mock_class, mock_var]
    )
    # Create tree
    tree = MockTree(root)
    result = analyzer.analyze_tree(tree)
    assert result == {
        'imports': [{'type': 'import', 'name': 'import os', 'start_line': 2, 'end_line': 2}],
        'functions': [{'name': 'test_func', 'start_line': 3, 'end_line': 5, 'parameters': []}],
        'classes': [{'name': 'TestClass', 'start_line': 6, 'end_line': 8, 'methods': [], 'bases': []}],
        'variables': [{'name': 'test_var', 'start_line': 9, 'end_line': 9, 'type': 'str'}]
    }
def test_analyze_tree_error(analyzer):
    """Test error handling during tree analysis."""
    # Create a tree that will raise an exception
    root = MockNode(type="module")  # Empty node will cause error
    tree = MockTree(root)
    result = analyzer.analyze_tree(tree)
    assert result == {
        'imports': [],
        'functions': [],
        'classes': [],
        'variables': []
    }
def test_extract_imports(analyzer):
    """Test extracting import statements."""
    mock_import = MockNode(
        type="import",
        text="import os",
        start_point=(1, 0),
        end_point=(1, 9)
    )
    root = MockNode(type="module", children=[mock_import])
    result = analyzer._extract_imports(root)
    assert result == [{
        'type': 'import',
        'name': 'import os',
        'start_line': 2,
        'end_line': 2
    }]
def test_extract_functions(analyzer):
    """Test extracting function definitions."""
    mock_function = MockNode(
        type="function_definition",
        text="test_func",
        start_point=(1, 0),
        end_point=(3, 0),
        fields={
            "name": MockNode(type="identifier", text="test_func"),
            "parameters": MockNode(type="parameters", children=[])
        }
    )
    root = MockNode(type="module", children=[mock_function])
    result = analyzer._extract_functions(root)
    assert result == [{
        'name': 'test_func',
        'start_line': 2,
        'end_line': 4,
        'parameters': []
    }]
def test_extract_classes(analyzer):
    """Test extracting class definitions."""
    mock_class = MockNode(
        type="class_definition",
        text="TestClass",
        start_point=(1, 0),
        end_point=(5, 0),
        fields={
            "name": MockNode(type="identifier", text="TestClass"),
            "bases": MockNode(type="bases", children=[])
        }
    )
    root = MockNode(type="module", children=[mock_class])
    result = analyzer._extract_classes(root)
    assert result == [{
        'name': 'TestClass',
        'start_line': 2,
        'end_line': 6,
        'methods': [],
        'bases': []
    }]
def test_extract_variables(analyzer):
    """Test extracting variable assignments."""
    mock_var = MockNode(
        type="assignment",
        text="test_var = 'hello'",
        start_point=(1, 0),
        end_point=(1, 10),
        fields={
            "left": MockNode(type="identifier", text="test_var"),
            "right": MockNode(type="string", text="'hello'")
        }
    )
    root = MockNode(type="module", children=[mock_var])
    result = analyzer._extract_variables(root)
    assert result == [{
        'name': 'test_var',
        'start_line': 2,
        'end_line': 2,
        'type': 'str'
    }]
def test_infer_type(analyzer):
    """Test type inference from value nodes."""
    type_tests = [
        ('string', 'str'),
        ('integer', 'int'),
        ('float', 'float'),
        ('true', 'bool'),
        ('false', 'bool'),
        ('none', 'None'),
        ('list', 'list'),
        ('dictionary', 'dict'),
        ('unknown_type', 'unknown')
    ]
    for node_type, expected_type in type_tests:
        node = MockNode(type=node_type)
        result = analyzer._infer_type(node)
        assert result == expected_type
def test_analyze_code_basic(analyzer):
    """Test analyzing basic Python code."""
    code = """
def greet(name: str) -> str:
    return f"Hello, {name}!"
"""
    result = analyzer.analyze_code(code)
    assert 'imports' in result
    assert 'functions' in result
    assert 'classes' in result
    assert 'variables' in result
    # Check function
    functions = result['functions']
    assert len(functions) == 1
    assert functions[0]['name'] == 'greet'
def test_analyze_code_with_imports(analyzer):
    """Test analyzing code with imports."""
    code = """
import os
from typing import List, Optional
def get_files() -> List[str]:
    return os.listdir('.')
"""
    result = analyzer.analyze_code(code)
    # Check imports
    imports = result['imports']
    assert len(imports) == 3
    assert any(imp['name'] == 'import os' for imp in imports)
def test_analyze_code_with_classes(analyzer):
    """Test analyzing code with class definitions."""
    code = """
class Animal:
    def __init__(self, name: str):
        self.name = name
    def speak(self) -> str:
        return "..."
class Dog(Animal):
    def speak(self) -> str:
        return "Woof!"
"""
    result = analyzer.analyze_code(code)
    # Check classes
    classes = result['classes']
    assert len(classes) == 2
    assert any(cls['name'] == 'Animal' for cls in classes)
    assert any(cls['name'] == 'Dog' for cls in classes)
def test_analyze_code_with_references(analyzer):
    """Test analyzing code with various references."""
    code = """
from math import sqrt
def calculate_distance(x: float, y: float) -> float:
    return sqrt(x*x + y*y)
def main():
    result = calculate_distance(3.0, 4.0)
    print(f"Distance: {result}")
"""
    result = analyzer.analyze_code(code)
    # Check functions
    functions = result['functions']
    assert len(functions) == 2
    assert any(func['name'] == 'calculate_distance' for func in functions)
    assert any(func['name'] == 'main' for func in functions)
def test_analyze_code_error_handling(analyzer):
    """Test error handling in code analysis."""
    # Test with invalid syntax
    code = "def invalid_syntax("
    result = analyzer.analyze_code(code)
    assert result == {
        'imports': [],
        'functions': [],
        'classes': [],
        'variables': []
    }
def test_analyze_file(analyzer, tmp_path):
    """Test analyzing a Python file."""
    # Create a test file
    test_file = tmp_path / "test.py"
    code = """
def test_function():
    return "Hello, World!"
"""
    test_file.write_text(code)
    # Analyze the file
    result = analyzer.analyze_file(str(test_file))
    # Check results
    assert len(result['functions']) == 1
    assert result['functions'][0]['name'] == 'test_function'
def test_analyze_file_error(analyzer, tmp_path):
    """Test error handling when analyzing a file."""
    # Test with non-existent file
    result = analyzer.analyze_file(str(tmp_path / "nonexistent.py"))
    assert result == {
        'imports': [],
        'functions': [],
        'classes': [],
        'variables': []
    }
</file>

<file path="tests/code_understanding/test_extractor.py">
"""Tests for the symbol extractor module."""
import pytest
from server.code_understanding.extractor import SymbolExtractor
from server.code_understanding.parser import MockNode as Node, MockTree as Tree
@pytest.fixture
def extractor():
    """Create a symbol extractor instance."""
    return SymbolExtractor()
@pytest.fixture
def mock_tree():
    """Create a mock syntax tree."""
    # Create mock nodes
    import_node = Node(
        type="import",
        text="import os",
        start_point=(1, 0),
        end_point=(1, 9)
    )
    function_node = Node(
        type="function_definition",
        text="def test_func(x: int) -> str:",
        start_point=(3, 0),
        end_point=(4, 12),
        fields={
            'name': Node(type="identifier", text="test_func"),
            'parameters': Node(type="parameters", children=[
                Node(type="identifier", text="x: int")
            ])
        }
    )
    class_node = Node(
        type="class_definition",
        text="class TestClass(BaseClass):",
        start_point=(6, 0),
        end_point=(9, 12),
        fields={
            'name': Node(type="identifier", text="TestClass"),
            'bases': Node(type="bases", children=[
                Node(type="identifier", text="BaseClass")
            ])
        }
    )
    # Create root node
    root = Node(type="module")
    root.children = [import_node, function_node, class_node]
    return Tree(root)
def test_extract_symbols_basic(extractor, mock_tree):
    """Test basic symbol extraction."""
    result = extractor.extract_symbols(mock_tree)
    assert 'symbols' in result
    assert 'references' in result
    symbols = result['symbols']
    assert 'os' in symbols
    assert 'test_func' in symbols
    assert 'TestClass' in symbols
    # Check symbol types
    assert symbols['os']['type'] == 'import'
    assert symbols['test_func']['type'] == 'function'
    assert symbols['TestClass']['type'] == 'class'
def test_process_import(extractor):
    """Test processing import statements."""
    # Test simple import
    node = Node(type="import", text="import os")
    extractor._process_import(node)
    assert 'os' in extractor.symbols
    assert extractor.symbols['os']['type'] == 'import'
    # Test from import
    node = Node(type="import", text="from typing import List, Optional")
    extractor._process_import(node)
    assert 'List' in extractor.symbols
    assert 'Optional' in extractor.symbols
    assert extractor.symbols['List']['type'] == 'import'
    assert extractor.symbols['Optional']['type'] == 'import'
def test_process_function(extractor):
    """Test processing function definitions."""
    node = Node(
        type="function_definition",
        fields={
            'name': Node(type="identifier", text="test_func"),
            'parameters': Node(type="parameters", children=[
                Node(type="identifier", text="x: int"),
                Node(type="identifier", text="y: str")
            ])
        }
    )
    extractor._process_function(node)
    assert 'test_func' in extractor.symbols
    assert extractor.symbols['test_func']['type'] == 'function'
    assert len(extractor.symbols['test_func']['params']) == 2
    assert 'x' in extractor.symbols['test_func']['params']
    assert 'y' in extractor.symbols['test_func']['params']
def test_process_class(extractor):
    """Test processing class definitions."""
    node = Node(
        type="class_definition",
        fields={
            'name': Node(type="identifier", text="TestClass"),
            'bases': Node(type="bases", children=[
                Node(type="identifier", text="BaseClass")
            ])
        }
    )
    extractor._process_class(node)
    assert 'TestClass' in extractor.symbols
    assert extractor.symbols['TestClass']['type'] == 'class'
    assert len(extractor.symbols['TestClass']['bases']) == 1
    assert 'BaseClass' in extractor.symbols['TestClass']['bases']
def test_process_identifier(extractor):
    """Test processing identifiers."""
    node = Node(type="identifier", text="test_var")
    extractor._process_identifier(node)
    assert 'test_var' in extractor.references
    assert len(extractor.references['test_var']) == 1
    assert extractor.references['test_var'][0]['scope'] == 'global'
def test_process_assignment(extractor):
    """Test processing assignments."""
    node = Node(
        type="assignment",
        fields={
            'left': Node(type="identifier", text="test_var")
        }
    )
    extractor._process_assignment(node)
    assert 'test_var' in extractor.symbols
    assert extractor.symbols['test_var']['type'] == 'variable'
def test_scope_handling(extractor):
    """Test scope handling during symbol extraction."""
    # Create a class with a method
    class_node = Node(
        type="class_definition",
        fields={
            'name': Node(type="identifier", text="TestClass"),
            'body': Node(type="body", children=[
                Node(
                    type="function_definition",
                    fields={
                        'name': Node(type="identifier", text="test_method"),
                        'parameters': Node(type="parameters", children=[])
                    }
                )
            ])
        }
    )
    extractor._process_node(class_node)
    # Check that method is in class scope
    assert 'test_method' in extractor.symbols
    assert extractor.symbols['test_method']['scope'] == 'TestClass'
def test_error_handling(extractor):
    """Test error handling during symbol extraction."""
    # Test with invalid tree
    result = extractor.extract_symbols(None)
    assert result == {'symbols': {}, 'references': {}}
    # Test with invalid node
    extractor._process_node(None)
    assert extractor.symbols == {}
    assert extractor.references == {}
</file>

<file path="tests/code_understanding/test_integration.py">
"""Integration tests for code understanding components."""
import pytest
from server.code_understanding.parser import MockNode as Node, MockTree as Tree, CodeParser
from server.code_understanding.analyzer import CodeAnalyzer
from server.code_understanding.symbols import SymbolExtractor
@pytest.fixture
def sample_code():
    """Sample Python code for testing."""
    return """
import os
from typing import List, Optional
class BaseClass:
    def base_method(self) -> None:
        pass
class TestClass(BaseClass):
    def __init__(self, name: str) -> None:
        self.name = name
    def test_method(self, value: int) -> None:
        pass
def helper_function(items: List[int]) -> int:
    return sum(items)
test_variable = "Hello World"
numbers = [1, 2, 3]
result = helper_function(numbers)
"""
def create_mock_node(mocker, type_name, text, start_point, end_point):
    """Create a mock node with the given properties."""
    return Node(type=type_name, text=text, start_point=start_point, end_point=end_point)
def create_mock_function_node(mocker, name, start_point, end_point, params):
    """Create a mock function node."""
    param_nodes = []
    for param in params:
        param_node = Node(type="identifier", text=param['name'])
        param_nodes.append(param_node)
    name_node = Node(type="identifier", text=name)
    params_node = Node(type="parameters", children=param_nodes)
    return Node(
        type="function_definition",
        start_point=start_point,
        end_point=end_point,
        fields={
            'name': name_node,
            'parameters': params_node
        }
    )
def create_mock_class_node(mocker, name, start_point, end_point, methods, bases=None):
    """Create a mock class node."""
    name_node = Node(type="identifier", text=name)
    base_nodes = []
    if bases:
        for base in bases:
            base_node = Node(type="identifier", text=base)
            base_nodes.append(base_node)
    bases_node = Node(type="bases", children=base_nodes)
    return Node(
        type="class_definition",
        start_point=start_point,
        end_point=end_point,
        children=methods,
        fields={
            'name': name_node,
            'bases': bases_node
        }
    )
def create_mock_variable_node(mocker, name, start_point, end_point, type_name):
    """Create a mock variable node."""
    name_node = Node(type="identifier", text=name)
    return Node(
        type="assignment",
        start_point=start_point,
        end_point=end_point,
        fields={
            'left': name_node
        }
    )
def test_end_to_end_analysis(mocker, sample_code):
    """Test the complete code analysis pipeline."""
    # Create instances of our components
    parser = CodeParser()
    analyzer = CodeAnalyzer()
    extractor = SymbolExtractor()
    # Configure mock nodes for imports
    mock_import_nodes = [
        create_mock_node(mocker, "import", b"import os", (1, 0), (1, 9)),
        create_mock_node(mocker, "import", b"from typing import List, Optional", (2, 0), (2, 32))
    ]
    # Configure mock nodes for classes
    mock_class_nodes = [
        create_mock_class_node(mocker, "BaseClass", (4, 0), (6, 8), [
            create_mock_function_node(mocker, "base_method", (5, 4), (6, 8), [])
        ]),
        create_mock_class_node(mocker, "TestClass", (8, 0), (14, 8), [
            create_mock_function_node(mocker, "__init__", (9, 4), (10, 8), [
                {'name': 'name', 'type': 'str'}
            ]),
            create_mock_function_node(mocker, "test_method", (12, 4), (14, 8), [
                {'name': 'value', 'type': 'int'}
            ])
        ], ["BaseClass"])
    ]
    # Configure mock nodes for functions
    mock_function_nodes = [
        create_mock_function_node(mocker, "helper_function", (16, 0), (17, 16), [
            {'name': 'items', 'type': 'List[int]'}
        ])
    ]
    # Configure mock nodes for variables
    mock_variable_nodes = [
        create_mock_variable_node(mocker, "test_variable", (19, 0), (19, 24), "str"),
        create_mock_variable_node(mocker, "numbers", (20, 0), (20, 14), "list"),
        create_mock_variable_node(mocker, "result", (21, 0), (21, 31), "int")
    ]
    # Configure root node to return our mock nodes
    def mock_children_by_field_name(field):
        if field == 'body':  # Changed from specific fields to 'body'
            return mock_import_nodes + mock_class_nodes + mock_function_nodes + mock_variable_nodes
        return []
    # Create a mock tree using our Node and Tree classes
    mock_root = Node('module')
    mock_root.children = mock_import_nodes + mock_class_nodes + mock_function_nodes + mock_variable_nodes
    mock_root._fields = {'body': mock_root.children}
    mock_tree = Tree(mock_root)
    # Run the analysis pipeline
    tree = mock_tree  # Use our mock tree directly
    analysis_result = analyzer.analyze_tree(tree)
    result = extractor.extract_symbols(tree)  # Pass tree instead of analysis_result
    # Verify the results
    assert len(result['symbols']) > 0
    # Check imports
    assert 'os' in result['symbols']
    assert 'List' in result['symbols']
    assert 'Optional' in result['symbols']
    # Check classes
    assert 'BaseClass' in result['symbols']
    assert 'TestClass' in result['symbols']
    # Check functions
    assert 'base_method' in result['symbols']
    assert '__init__' in result['symbols']
    assert 'test_method' in result['symbols']
    assert 'helper_function' in result['symbols']
    # Check variables
    assert 'test_variable' in result['symbols']
    assert 'numbers' in result['symbols']
    assert 'result' in result['symbols']
</file>

<file path="tests/code_understanding/test_symbols.py">
"""Tests for the symbols module."""
import pytest
from server.code_understanding.symbols import SymbolExtractor
from server.code_understanding.parser import MockNode as Node, MockTree as Tree
@pytest.fixture
def extractor():
    """Create a symbol extractor instance."""
    return SymbolExtractor()
@pytest.fixture
def mock_tree():
    """Create a mock syntax tree."""
    # Create mock nodes
    import_node = Node(
        type="import",
        text="import os",
        start_point=(1, 0),
        end_point=(1, 9)
    )
    function_node = Node(
        type="function_definition",
        text="def test_func(x: int) -> str:",
        start_point=(3, 0),
        end_point=(4, 12),
        fields={
            'name': Node(type="identifier", text="test_func"),
            'parameters': Node(type="parameters", children=[
                Node(type="identifier", text="x: int")
            ])
        }
    )
    class_node = Node(
        type="class_definition",
        text="class TestClass(BaseClass):",
        start_point=(6, 0),
        end_point=(9, 12),
        fields={
            'name': Node(type="identifier", text="TestClass"),
            'bases': Node(type="bases", children=[
                Node(type="identifier", text="BaseClass")
            ])
        }
    )
    # Create root node
    root = Node(type="module")
    root.children = [import_node, function_node, class_node]
    return Tree(root)
def test_symbol_table_management(extractor):
    """Test symbol table management."""
    # Add symbols
    extractor._add_symbol('test_var', {
        'type': 'variable',
        'scope': 'global',
        'start': (1, 0),
        'end': (1, 10)
    })
    extractor._add_symbol('test_func', {
        'type': 'function',
        'scope': 'global',
        'start': (3, 0),
        'end': (5, 0),
        'params': ['x', 'y']
    })
    # Check symbols
    assert 'test_var' in extractor.symbols
    assert extractor.symbols['test_var']['type'] == 'variable'
    assert extractor.symbols['test_var']['scope'] == 'global'
    assert 'test_func' in extractor.symbols
    assert extractor.symbols['test_func']['type'] == 'function'
    assert len(extractor.symbols['test_func']['params']) == 2
def test_scope_resolution(extractor):
    """Test scope resolution."""
    # Create nested scopes
    extractor.current_scope = 'global'
    # Add class
    extractor._add_symbol('TestClass', {
        'type': 'class',
        'scope': 'global',
        'start': (1, 0),
        'end': (10, 0)
    })
    # Add method in class scope
    extractor.current_scope = 'TestClass'
    extractor._add_symbol('test_method', {
        'type': 'method',
        'scope': 'TestClass',
        'start': (2, 4),
        'end': (4, 4)
    })
    # Add variable in method scope
    extractor.current_scope = 'TestClass.test_method'
    extractor._add_symbol('local_var', {
        'type': 'variable',
        'scope': 'TestClass.test_method',
        'start': (3, 8),
        'end': (3, 20)
    })
    # Check scopes
    assert extractor.symbols['TestClass']['scope'] == 'global'
    assert extractor.symbols['test_method']['scope'] == 'TestClass'
    assert extractor.symbols['local_var']['scope'] == 'TestClass.test_method'
def test_reference_tracking(extractor):
    """Test reference tracking."""
    # Add references
    extractor._add_reference('test_var', {
        'scope': 'global',
        'start': (1, 0),
        'end': (1, 10)
    })
    extractor._add_reference('test_var', {
        'scope': 'test_func',
        'start': (3, 4),
        'end': (3, 14)
    })
    # Check references
    assert 'test_var' in extractor.references
    assert len(extractor.references['test_var']) == 2
    assert extractor.references['test_var'][0]['scope'] == 'global'
    assert extractor.references['test_var'][1]['scope'] == 'test_func'
def test_type_handling(extractor):
    """Test type handling."""
    # Test basic types
    node = Node(type="identifier", text="x: int")
    type_info = extractor._get_type_info(node)
    assert type_info == 'int'
    # Test complex types
    node = Node(type="identifier", text="x: List[str]")
    type_info = extractor._get_type_info(node)
    assert type_info == 'List[str]'
    # Test optional types
    node = Node(type="identifier", text="x: Optional[int]")
    type_info = extractor._get_type_info(node)
    assert type_info == 'Optional[int]'
def test_node_text_handling(extractor):
    """Test node text handling."""
    # Test string text
    node = Node(type="identifier", text="test_var")
    text = extractor._get_node_text(node)
    assert text == "test_var"
    # Test bytes text
    node = Node(type="identifier", text=b"test_var")
    text = extractor._get_node_text(node)
    assert text == "test_var"
    # Test invalid text
    node = Node(type="identifier", text=None)
    text = extractor._get_node_text(node)
    assert text == ""
def test_error_handling(extractor):
    """Test error handling."""
    # Test with invalid tree
    result = extractor.extract_symbols(None)
    assert result == {'symbols': {}, 'references': {}}
    # Test with invalid node
    extractor._process_node(None)
    assert extractor.symbols == {}
    assert extractor.references == {}
    # Test with invalid node text
    node = Node(type="identifier", text=123)  # Invalid text type
    text = extractor._get_node_text(node)
    assert text == ""
</file>

<file path="tests/test_l3_agent/test_code_generation.py">
import pytest
from unittest.mock import Mock, patch
import server
@pytest.fixture
def mock_anthropic():
    with patch('server.anthropic') as mock:
        mock.messages.create.return_value = Mock(
            content="def test_function():\n    return True",
            usage=Mock(total_tokens=50)
        )
        yield mock
@pytest.fixture
def mock_openai():
    with patch('server.openai.ChatCompletion') as mock:
        mock.create.return_value = Mock(
            choices=[Mock(message=Mock(content="def test_function():\n    return True"))],
            usage=Mock(total_tokens=50)
        )
        yield mock
def test_generate_code_with_claude(mock_anthropic):
    result = server.generate_code(
        prompt="Write a test function",
        model="claude-3-sonnet"
    )
    assert result['status'] == 'success'
    assert 'def test_function()' in result['code']
    assert result['tokens_used'] == 50
    assert 'generation_time' in result
    mock_anthropic.messages.create.assert_called_once()
def test_generate_code_with_gpt4(mock_openai):
    result = server.generate_code(
        prompt="Write a test function",
        model="gpt-4"
    )
    assert result['status'] == 'success'
    assert 'def test_function()' in result['code']
    assert result['tokens_used'] == 50
    assert 'generation_time' in result
    mock_openai.create.assert_called_once()
@pytest.mark.parametrize("model", ["code-llama", "starcoder"])
def test_generate_code_with_local_model(model):
    with patch('server.pipeline') as mock_pipeline:
        mock_pipeline.return_value = lambda **kwargs: [
            {'generated_text': "def test_function():\n    return True"}
        ]
        result = server.generate_code(
            prompt="Write a test function",
            model=model
        )
        assert result['status'] == 'success'
        assert 'def test_function()' in result['code']
        assert 'tokens_used' in result
        assert 'generation_time' in result
        mock_pipeline.assert_called_once()
def test_generate_code_with_invalid_model():
    result = server.generate_code(
        prompt="Write a test function",
        model="invalid-model"
    )
    assert result['status'] == 'error'
    assert 'error' in result
    assert 'Invalid model' in result['error']
def test_generate_code_with_empty_prompt():
    result = server.generate_code(prompt="")
    assert result['status'] == 'error'
    assert 'error' in result
    assert 'Empty prompt' in result['error']
def test_generate_code_with_context():
    with patch('server._get_workspace_info') as mock_workspace:
        mock_workspace.return_value = {'workspace': 'test'}
        with patch('server._generate_with_api_model') as mock_generate:
            mock_generate.return_value = {
                'status': 'success',
                'code': 'def test_function():\n    return True',
                'tokens_used': 50,
                'generation_time': 0.5
            }
            result = server.generate_code(
                prompt="Write a test function",
                context={'file': 'test.py'}
            )
            assert result['status'] == 'success'
            assert 'workspace' in result['context']
            assert 'file' in result['context']
def test_generate_code_with_custom_system_prompt():
    custom_prompt = "You are a Python expert"
    with patch('server._generate_with_api_model') as mock_generate:
        server.generate_code(
            prompt="Write a test function",
            system_prompt=custom_prompt
        )
        mock_generate.assert_called_with(
            prompt="Write a test function",
            model="claude-3-sonnet",
            system_prompt=custom_prompt,
            max_tokens=None,
            temperature=0.7
        )
def test_generate_code_metrics_tracking():
    with patch('server._track_generation_metrics') as mock_track:
        with patch('server._generate_with_api_model') as mock_generate:
            mock_generate.return_value = {
                'status': 'success',
                'code': 'def test():\n    pass',
                'tokens_used': 50,
                'generation_time': 0.5
            }
            server.generate_code(
                prompt="Write a test function",
                model="claude-3-sonnet"
            )
            mock_track.assert_called_with(
                model="claude-3-sonnet",
                language="python",
                tokens_used=50,
                success=True
            )
</file>

<file path="tests/test_l3_agent/test_code_validation.py">
import pytest
from unittest.mock import Mock, patch
import server
@pytest.fixture
def valid_python_code():
    return """
def test_function():
    x = 1
    y = 2
    return x + y
"""
@pytest.fixture
def invalid_python_code():
    return """
def test_function()
    x = 1
    y = 2
    return x + y
"""
def test_validate_code_quality_success(valid_python_code):
    with patch('server.lint_code') as mock_lint:
        mock_lint.return_value = {'status': 'success'}
        result = server.validate_code_quality(valid_python_code)
        assert result['status'] == 'success'
        assert result['language'] == 'python'
        assert 'results' in result
        assert 'summary' in result
def test_validate_code_quality_syntax_error(invalid_python_code):
    result = server.validate_code_quality(invalid_python_code)
    assert result['status'] == 'error'
    assert result['results']['syntax']['status'] == 'error'
    assert 'SyntaxError' in result['results']['syntax']['error']
def test_validate_code_quality_empty_code():
    result = server.validate_code_quality("")
    assert result['status'] == 'error'
    assert result['error'] == 'No code provided'
def test_validate_code_quality_specific_checks():
    code = "x = 1"
    checks = ['syntax', 'style']
    with patch('server.lint_code') as mock_lint:
        mock_lint.return_value = {'status': 'success'}
        result = server.validate_code_quality(code, checks=checks)
        assert set(result['results'].keys()) == set(checks)
def test_validate_code_quality_complexity():
    complex_code = """
def complex_function(x):
    if x > 0:
        if x < 10:
            for i in range(x):
                if i % 2 == 0:
                    while True:
                        try:
                            if i > 5:
                                return True
                        except:
                            pass
    return False
"""
    result = server.validate_code_quality(complex_code, checks=['complexity'])
    assert result['results']['complexity']['status'] == 'warning'
    assert result['results']['complexity']['complexity_score'] > 10
def test_validate_code_quality_security():
    insecure_code = """
import pickle
def process_data(data):
    return pickle.loads(data)
"""
    with patch('subprocess.run') as mock_run:
        mock_run.return_value = Mock(
            returncode=1,
            stdout="Found security issue: Use of unsafe pickle.loads()"
        )
        result = server.validate_code_quality(insecure_code, checks=['security'])
        assert result['results']['security']['status'] == 'error'
        assert 'issues' in result['results']['security']
def test_validate_code_quality_performance():
    inefficient_code = """
def process_list(items):
    result = []
    for item in items:
        result.append(item * 2)
    return result
"""
    result = server.validate_code_quality(inefficient_code, checks=['performance'])
    assert 'performance' in result['results']
    assert 'recommendations' in result['results']['performance']
    assert any('list comprehension' in r.lower() 
              for r in result['results']['performance']['recommendations'])
def test_validate_code_quality_summary_generation():
    with patch('server.lint_code') as mock_lint:
        mock_lint.return_value = {'status': 'success'}
        result = server.validate_code_quality("x = 1")
        assert '✅' in result['summary']  # Success markers
        assert isinstance(result['summary'], str)
        assert len(result['summary'].split('\n')) >= 1  # At least one check
def test_validate_code_quality_all_checks_failed():
    with patch('server.lint_code') as mock_lint:
        mock_lint.return_value = {'status': 'error', 'error': 'Style error'}
        with patch('server._analyze_complexity') as mock_complexity:
            mock_complexity.return_value = {
                'status': 'error',
                'error': 'Too complex'
            }
            with patch('server._analyze_security') as mock_security:
                mock_security.return_value = {
                    'status': 'error',
                    'error': 'Security issue'
                }
                result = server.validate_code_quality("x = 1")
                assert result['status'] == 'error'
                assert '❌' in result['summary']  # Error markers
def test_validate_code_quality_exception_handling():
    with patch('ast.parse') as mock_parse:
        mock_parse.side_effect = Exception("Unexpected error")
        result = server.validate_code_quality("x = 1")
        assert result['status'] == 'error'
        assert 'error' in result
</file>

<file path="tests/conftest.py">
"""Common test fixtures for Terminal Command Runner MCP tests."""
import os
import sys
import pytest
import tempfile
import shutil
from contextlib import contextmanager
from typing import Generator, Dict, Any
import threading
import time
# Add the parent directory to path to import the server module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
@pytest.fixture
def temp_dir() -> Generator[str, None, None]:
    """Create a temporary directory for file operation tests."""
    test_dir = tempfile.mkdtemp()
    yield test_dir
    # Clean up after tests
    shutil.rmtree(test_dir)
@pytest.fixture
def sample_text_file(temp_dir: str) -> Generator[str, None, None]:
    """Create a sample text file for testing file operations."""
    file_path = os.path.join(temp_dir, "sample.txt")
    with open(file_path, "w") as f:
        f.write("This is a sample test file.\nIt has multiple lines.\nThird line.")
    yield file_path
@pytest.fixture
def long_running_process() -> Generator[Dict[str, Any], None, None]:
    """Start a long-running process for testing process management."""
    if sys.platform == "win32":
        cmd = "ping -t localhost"
        proc = None
    else:
        cmd = "sleep 30"
        proc = None
    # Use subprocess to start a process, but do it in a way that can be imported elsewhere
    # We'll implement this in the test functions
    yield {"command": cmd, "process": proc}
    # Cleanup will be handled in the tests
@contextmanager
def mock_active_sessions() -> Generator[Dict[int, Dict[str, Any]], None, None]:
    """Context manager to mock the active_sessions global variable."""
    # This is a placeholder - in actual tests, we'll need to mock the server's global state
    mock_sessions = {}
    yield mock_sessions
@contextmanager
def mock_output_queues() -> Generator[Dict[int, Any], None, None]:
    """Context manager to mock the output_queues global variable."""
    # This is a placeholder - in actual tests, we'll need to mock the server's global state
    mock_queues = {}
    yield mock_queues
@pytest.fixture
def blacklisted_commands() -> Generator[set, None, None]:
    """Fixture to provide and restore the blacklisted_commands set."""
    # This is a placeholder - in actual tests, we'll need to mock the server's global state
    original_blacklist = {'rm -rf /', 'mkfs'}
    mock_blacklist = original_blacklist.copy()
    yield mock_blacklist
    # Reset blacklist in cleanup
</file>

<file path="tests/debug_js_variable.py">
import logging
import sys
import json
from server.code_understanding.analyzer import CodeAnalyzer
from server.code_understanding.language_adapters import JavaScriptParserAdapter
from server.code_understanding.parser import CodeParser
# Set up logging to see detailed debug information
logging.basicConfig(level=logging.DEBUG, 
                   format='%(levelname)s:%(name)s:%(message)s')
# Create a simple JavaScript code with a variable declaration
js_code = """
const fs = require('fs');
const instance = new MyClass(123);
let myVar = 'test';
const myConst = 456;
"""
def print_node_structure(node, indent=0):
    """Print the structure of an AST node for debugging."""
    if not node:
        print(f"{'  ' * indent}None")
        return
    print(f"{'  ' * indent}Type: {node.type}, Text: {node.text[:50] + '...' if len(node.text) > 50 else node.text}")
    print(f"{'  ' * indent}Fields: {node.fields}")
    print(f"{'  ' * indent}Children: {len(node.children)}")
    for i, child in enumerate(node.children):
        print(f"{'  ' * indent}Child {i}:")
        print_node_structure(child, indent + 1)
# Parse the JavaScript code
parser = CodeParser()
tree = parser.parse(js_code, language='javascript')
print("\n--- AST Structure ---")
print_node_structure(tree.root_node)
# Analyze the code with our analyzer
analyzer = CodeAnalyzer()
result = analyzer.analyze_code(js_code, language='javascript')
print("\n--- Analysis Result ---")
print(f"Imports: {json.dumps(result.get('imports', []), indent=2)}")
print(f"Variables: {json.dumps(result.get('variables', []), indent=2)}")
print(f"Functions: {json.dumps(result.get('functions', []), indent=2)}")
print(f"Classes: {json.dumps(result.get('classes', []), indent=2)}")
</file>

<file path="tests/missing_functions.py">
#!/usr/bin/env python3
"""Check for missing functions in server.py that are referenced in tests."""
import os
import sys
import inspect
import importlib
import glob
import importlib.util
from types import ModuleType
# Add parent dir to path
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
def get_functions_from_file(file_path: str) -> set:
    """Extract all function names from a Python file without executing it."""
    with open(file_path, 'r') as f:
        content = f.read()
    # Simple approach: look for function definitions
    functions = set()
    lines = content.split('\n')
    for line in lines:
        line = line.strip()
        if line.startswith('def ') and '(' in line:
            # Extract function name
            func_name = line[4:line.find('(')].strip()
            if not func_name.startswith('_'):
                functions.add(func_name)
        # Also look for @mcp.tool() decorator functions
        if '@mcp.tool()' in line:
            # The next line should be a function definition
            next_line_idx = lines.index(line) + 1
            if next_line_idx < len(lines):
                next_line = lines[next_line_idx].strip()
                if next_line.startswith('def ') and '(' in next_line:
                    func_name = next_line[4:next_line.find('(')].strip()
                    if not func_name.startswith('_'):
                        functions.add(func_name)
    return functions
def get_referenced_functions_from_file(file_path: str) -> set:
    """Extract function references to server module from a test file."""
    with open(file_path, 'r') as f:
        content = f.read()
    # Simple approach: look for "server.function_name" patterns
    # This is not perfect but should work for our test files
    references = set()
    lines = content.split('\n')
    for line in lines:
        if 'server.' in line:
            parts = line.split('server.')
            for part in parts[1:]:
                # Extract the function name (up to the first non-identifier character)
                func_name = ''
                for char in part:
                    if char.isalnum() or char == '_':
                        func_name += char
                    else:
                        break
                if func_name:
                    references.add(func_name)
    return references
def main():
    """Main function to check missing functions."""
    # Get all functions defined in server.py
    server_file = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'server.py'))
    defined_functions = get_functions_from_file(server_file)
    # Get all functions referenced in test files
    test_files = glob.glob(os.path.join(os.path.dirname(__file__), 'test_*.py'))
    referenced_functions = set()
    for test_file in test_files:
        referenced_functions.update(get_referenced_functions_from_file(test_file))
    # Find missing functions
    missing_functions = referenced_functions - defined_functions
    if missing_functions:
        print("Missing functions in server.py that are referenced in tests:")
        for func in sorted(missing_functions):
            print(f"- {func}")
    else:
        print("No missing functions found. All referenced functions are defined in server.py")
    # Also show what files reference each missing function
    if missing_functions:
        print("\nFiles referencing missing functions:")
        for func in sorted(missing_functions):
            print(f"\n{func}:")
            for test_file in test_files:
                if func in get_referenced_functions_from_file(test_file):
                    print(f"  - {os.path.basename(test_file)}")
if __name__ == "__main__":
    main()
</file>

<file path="tests/test_analyzer.py">
"""Tests for the code analyzer."""
import pytest
import logging
import os
from pathlib import Path
from unittest.mock import Mock, patch
from server.code_understanding.analyzer import CodeAnalyzer
from server.code_understanding.common_types import MockNode, MockTree
# Configure logging to show INFO level logs
logging.basicConfig(level=logging.INFO) # Keep basic config as fallback
logging.getLogger().setLevel(logging.INFO) # Set root logger level
# Explicitly configure loggers for modules under test
logging.getLogger('server.code_understanding.analyzer').setLevel(logging.INFO)
logging.getLogger('server.code_understanding.mock_parser').setLevel(logging.INFO)
logger = logging.getLogger(__name__) # Logger for the test file itself
@pytest.fixture
def analyzer():
    """Create a code analyzer for testing."""
    return CodeAnalyzer()
@pytest.fixture
def sample_code():
    """Sample Python code for testing."""
    return """
import os
from sys import path
def hello(name: str) -> str:
    print(f"Hello {name}")
    return path
class Greeter:
    def __init__(self, prefix: str = "Hello"):
        self.prefix = prefix
    def greet(self, name: str) -> str:
        return f"{self.prefix} {name}"
    @staticmethod
    def say_hi() -> str:
        return "Hi there!"
def main():
    g = Greeter("Hey")
    hello("World")
    print(g.greet("Universe"))
    print(Greeter.say_hi())
"""
def test_analyze_code(analyzer, sample_code):
    """Test analyzing code string."""
    tree = analyzer.parser.parse(sample_code)
    logger.info(f"Root node type: {tree.root_node.type}")
    logger.info(f"Root node children types: {[child.type for child in tree.root_node.children]}")
    for child in tree.root_node.children:
        logger.info(f"Child node type: {child.type}")
        if child.type in ('import_statement', 'import_from_statement'):
            logger.info(f"Import node children: {[c.type for c in child.children]}")
            for c in child.children:
                logger.info(f"Import child text: {c.text}")
    result = analyzer.analyze_code(sample_code)
    # Verify imports
    assert len(result['imports']) == 2
    assert any(imp['name'] == 'os' for imp in result['imports'])
    # Check the 'from sys import path' correctly
    assert any(imp['type'] == 'from_import' and imp['name'] == 'path' and imp['module'] == 'sys' for imp in result['imports'])
    # Verify functions (top-level only)
    assert len(result['functions']) == 2 # Should only find 'hello' and 'main'
    assert any(func['name'] == 'hello' for func in result['functions'])
    assert any(func['name'] == 'main' for func in result['functions'])
    # Ensure class methods are NOT in the top-level functions list
    assert not any(func['name'] == '__init__' for func in result['functions'])
    assert not any(func['name'] == 'greet' for func in result['functions'])
    assert not any(func['name'] == 'say_hi' for func in result['functions'])
    # Verify classes
    assert len(result['classes']) == 1
    greeter_class = result['classes'][0]
    assert greeter_class['name'] == 'Greeter'
    # Verify methods within the class
    assert len(greeter_class['methods']) == 3 
    assert any(meth['name'] == '__init__' for meth in greeter_class['methods'])
    assert any(meth['name'] == 'greet' for meth in greeter_class['methods'])
    assert any(meth['name'] == 'say_hi' for meth in greeter_class['methods'])
    # Verify variables (Note: Only finds top-level assignments currently)
    # The `prefix` variable inside __init__ is not extracted by the current simple logic
    assert len(result['variables']) == 0 # Expect 0 top-level variables
    # assert len(result['variables']) == 1
    # assert result['variables'][0]['name'] == 'prefix' # This would fail
def test_analyze_tree(analyzer):
    """Test analyzing a mock tree."""
    # Create a mock tree
    root = MockNode('module', children=[
        MockNode('import_statement', text='import os', start_point=(0, 0), end_point=(0, 2), children=[
            MockNode('identifier', text='os')
        ]),
        MockNode('function_definition', text='hello', start_point=(2, 0), end_point=(4, 0),
                fields={'name': MockNode('name', text='hello')}),
        MockNode('class_definition', text='Greeter', start_point=(6, 0), end_point=(8, 0),
                fields={'name': MockNode('name', text='Greeter')}),
        MockNode('assignment', text='x = 42', start_point=(10, 0), end_point=(10, 6),
                fields={'left': MockNode('name', text='x'),
                       'right': MockNode('integer', text='42')})
    ])
    tree = MockTree(root)
    result = analyzer.analyze_tree(tree)
    # Verify results
    assert len(result['imports']) == 1
    assert result['imports'][0]['name'] == 'os'
    assert len(result['functions']) == 1
    assert result['functions'][0]['name'] == 'hello'
    assert len(result['classes']) == 1
    assert result['classes'][0]['name'] == 'Greeter'
    assert len(result['variables']) == 1
    assert result['variables'][0]['name'] == 'x'
    assert result['variables'][0]['type'] == 'unknown'
def test_extract_function(analyzer):
    """Test extracting function information."""
    # Test with None node
    result = analyzer._extract_function(None)
    assert result['name'] == ''
    assert result['start_line'] == 0
    assert result['end_line'] == 0
    assert result['parameters'] == []
    assert result['decorators'] == []
    assert not result['is_async']
    # Test with valid node
    node = MockNode('function_definition', text='hello', start_point=(0, 0), end_point=(2, 0),
                   children=[
                       MockNode('decorator', text='@staticmethod'),
                       MockNode('async', text='async'),
                       MockNode('parameters', children=[
                           MockNode('identifier', text='name', start_point=(0, 8), end_point=(0, 12))
                       ])
                   ])
    result = analyzer._extract_function(node)
    assert result['name'] == 'hello'
    assert result['start_line'] == 1
    assert result['end_line'] == 2
    assert len(result['parameters']) == 1
    assert result['parameters'][0]['name'] == 'name'
    assert result['decorators'] == ['@staticmethod']
    assert result['is_async']
def test_extract_class(analyzer):
    """Test extracting class information."""
    # Test with None node
    result = analyzer._extract_class(None)
    assert result['name'] == ''
    assert result['start_line'] == 0
    assert result['end_line'] == 0
    assert result['methods'] == []
    assert result['bases'] == []
    # Test with valid node
    node = MockNode('class_definition', text='Greeter', start_point=(0, 0), end_point=(4, 0),
                   children=[
                       MockNode('bases', children=[
                           MockNode('identifier', text='BaseClass'),
                           MockNode('keyword_argument', children=[
                               MockNode('name', text='metaclass'),
                               MockNode('value', text='MetaClass')
                           ])
                       ]),
                       MockNode('body', children=[
                           MockNode('function_definition', text='__init__', start_point=(1, 4), end_point=(2, 4))
                       ])
                   ])
    result = analyzer._extract_class(node)
    assert result['name'] == 'Greeter'
    assert result['start_line'] == 1
    assert result['end_line'] == 4
    assert len(result['bases']) == 2
    assert 'BaseClass' in result['bases']
    assert 'metaclass=MetaClass' in result['bases']
    assert len(result['methods']) == 1
    assert result['methods'][0]['name'] == '__init__'
def test_extract_parameters(analyzer):
    """Test extracting function parameters."""
    # Test with None node
    assert analyzer._extract_parameters(None) == []
    # Test with valid node
    node = MockNode('parameters', children=[
        MockNode('identifier', text='name', start_point=(0, 0), end_point=(0, 4)),
        MockNode('typed_parameter', children=[
            MockNode('name', text='age'),
            MockNode('type', text='int')
        ], start_point=(0, 6), end_point=(0, 12)),
        MockNode('list_splat_pattern', children=[
            MockNode('name', text='args')
        ], start_point=(0, 14), end_point=(0, 18))
    ])
    result = analyzer._extract_parameters(node)
    assert len(result) == 3
    assert result[0]['name'] == 'name'
    assert result[0]['type'] == 'parameter'
    assert result[1]['name'] == 'age'
    assert result[1]['type'] == 'int'
    assert result[2]['name'] == '*args'
    assert result[2]['type'] == 'parameter'
def test_analyze_file(analyzer, tmp_path, sample_code):
    """Test analyzing a file."""
    # Create test file
    file_path = tmp_path / "test.py"
    file_path.write_text(sample_code)
    # Analyze file
    result = analyzer.analyze_file(str(file_path))
    # Verify results are similar to analyze_code
    assert len(result['imports']) == 2
    assert len(result['functions']) == 2 # Top-level only
    assert len(result['classes']) == 1
    assert len(result['classes'][0]['methods']) == 3
    assert len(result['variables']) == 0 # Top-level only
def test_analyze_directory(analyzer, tmp_path):
    """Test analyzing a directory."""
    # Create test directory structure
    module1 = tmp_path / "module1.py"
    module1.write_text("def func1(): return 'Hello'")
    module2 = tmp_path / "module2.py"
    module2.write_text("from module1 import func1\ndef func2(): return func1()")
    # Create a subdirectory with another file
    subdir = tmp_path / "subdir"
    subdir.mkdir()
    module3 = subdir / "module3.py"
    module3.write_text("from ..module2 import func2\ndef func3(): return func2()")
    # Analyze directory
    result = analyzer.analyze_directory(str(tmp_path))
    # Verify results
    assert len(result) == 3
    # Find the result for each module
    module1_result = next((r for r in result if r['file'] == str(module1)), None)
    module2_result = next((r for r in result if r['file'] == str(module2)), None)
    module3_result = next((r for r in result if r['file'] == str(module3)), None)
    assert module1_result is not None
    assert module2_result is not None
    assert module3_result is not None
    # Verify module1 results
    assert len(module1_result['functions']) == 1
    assert module1_result['functions'][0]['name'] == 'func1'
    # Verify module2 results
    assert len(module2_result['imports']) == 1
    assert module2_result['imports'][0]['name'] == 'func1'
    assert module2_result['imports'][0]['module'] == 'module1'
    assert len(module2_result['functions']) == 1
    assert module2_result['functions'][0]['name'] == 'func2'
    # Verify module3 results
    assert len(module3_result['imports']) == 1
    assert module3_result['imports'][0]['name'] == 'func2'
    assert module3_result['imports'][0]['module'] == '..module2'
    assert len(module3_result['functions']) == 1
    assert module3_result['functions'][0]['name'] == 'func3'
def test_analyze_file_not_found(analyzer):
    """Test handling of non-existent files."""
    with pytest.raises(FileNotFoundError):
        analyzer.analyze_file("non_existent_file.py")
def test_analyze_directory_not_found(analyzer):
    """Test handling of non-existent directories."""
    with pytest.raises(FileNotFoundError):
        analyzer.analyze_directory("non_existent_directory")
</file>

<file path="tests/test_command_execution.py">
"""Tests for command execution functionality."""
import os
import sys
import pytest
import time
import subprocess
from unittest.mock import patch, MagicMock
import threading
import queue
# Import the server module - this assumes server.py is in the parent directory
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import server
def test_is_command_safe():
    """Test command safety validation function."""
    # Test empty command
    assert not server.is_command_safe("")
    # Test safe command
    assert server.is_command_safe("echo 'hello world'")
    # Test unsafe command
    assert not server.is_command_safe("rm -rf /")
    # Test another unsafe command
    assert not server.is_command_safe("mkfs")
@pytest.mark.parametrize("cmd,expected_exit_code", [
    ("echo 'hello world'", 0),
    ("exit 1", 1),
    ("non-existent-command", None)  # This command should fail to execute
])
def test_execute_command_basic(cmd, expected_exit_code):
    """Test basic command execution with different commands."""
    # Skip the non-existent command test in Windows
    if sys.platform == "win32" and cmd == "non-existent-command":
        pytest.skip("Skipping non-existent command test on Windows")
    result = server.execute_command(cmd, timeout=2)
    # Check command execution result
    if expected_exit_code is None:
        # For non-existent commands, we expect an error
        assert "error" in result or result["exit_code"] != 0
    else:
        # For valid commands, check exit code
        assert result["exit_code"] == expected_exit_code
        assert "stdout" in result
        assert "stderr" in result
def test_execute_command_output():
    """Test command execution output capture."""
    # Simple echo command with predictable output
    cmd = "echo 'hello world'"
    result = server.execute_command(cmd, timeout=2)
    # Verify output
    assert result["exit_code"] == 0
    assert "hello world" in result["stdout"]
    assert result["stderr"] == ""  # No stderr output expected
def test_execute_command_timeout():
    """Test command execution with timeout."""
    # Choose a command that will run longer than the timeout
    if sys.platform == "win32":
        cmd = "ping -n 10 localhost"  # Will take ~10 seconds on Windows
    else:
        cmd = "sleep 5"  # Will take 5 seconds on Unix-like
    # Set timeout to 1 second
    result = server.execute_command(cmd, timeout=1, allow_background=False)
    # Command should be terminated after timeout
    assert result["runtime"] >= 1.0
    assert result["pid"] is None  # No background process should be running
    # Now allow background process
    result = server.execute_command(cmd, timeout=1, allow_background=True)
    # Command should be running in background
    assert result["pid"] is not None
    assert result["exit_code"] is None
    assert result["complete"] is False
    # Clean up the process
    if result["pid"]:
        server.force_terminate(result["pid"])
def test_read_output():
    """Test reading output from a background process."""
    # Start a command that produces output periodically
    if sys.platform == "win32":
        cmd = "ping -n 3 localhost"
    else:
        cmd = "for i in 1 2 3; do echo $i; sleep 0.5; done"
    # Execute command with background allowed
    result = server.execute_command(cmd, timeout=1, allow_background=True)
    assert result["pid"] is not None
    pid = result["pid"]
    # Wait a bit for some output to be generated
    time.sleep(1)
    # Read the output
    output_result = server.read_output(pid)
    # Check that there's some output
    assert output_result["stdout"] != "" or output_result["stderr"] != ""
    # Wait for process to complete
    max_wait = 5
    start_time = time.time()
    while time.time() - start_time < max_wait:
        output_result = server.read_output(pid)
        if output_result["complete"]:
            break
        time.sleep(0.5)
    # Verify the process completed
    assert output_result["complete"]
    assert output_result["pid"] is None
def test_force_terminate():
    """Test terminating a background process."""
    # Start a long-running command
    if sys.platform == "win32":
        cmd = "ping -n 30 localhost"  # Will run for ~30 seconds
    else:
        cmd = "sleep 30"  # Will run for 30 seconds
    # Execute command with background allowed
    result = server.execute_command(cmd, timeout=1, allow_background=True)
    assert result["pid"] is not None
    pid = result["pid"]
    # Force terminate the process
    terminate_result = server.force_terminate(pid)
    # Verify termination was successful
    assert terminate_result["success"]
    # Check that the process is no longer running
    time.sleep(0.5)  # Give some time for cleanup
    try:
        os.kill(pid, 0)  # This will raise an error if the process is gone
        process_still_running = True
    except OSError:
        process_still_running = False
    assert not process_still_running
@patch("server.is_command_safe")
def test_blacklisted_command(mock_is_command_safe):
    """Test behavior with blacklisted commands."""
    # Mock is_command_safe to return False
    mock_is_command_safe.return_value = False
    # Try to execute a "dangerous" command
    result = server.execute_command("dangerous_command")
    # Verify it was blocked
    assert "error" in result
    assert result["pid"] is None
    assert "blocked" in result["stderr"].lower()
def test_block_and_unblock_command():
    """Test adding and removing commands from the blacklist."""
    test_command = "test_block_command"
    # Ensure command is not in the blacklist initially
    if test_command in server.blacklisted_commands:
        server.blacklisted_commands.remove(test_command)
    # Verify command is considered safe initially
    assert server.is_command_safe(test_command)
    # Block the command
    result = server.block_command(test_command)
    assert result["success"]
    # Verify command is now considered unsafe
    assert not server.is_command_safe(test_command)
    # Unblock the command
    result = server.unblock_command(test_command)
    assert result["success"]
    # Verify command is safe again
    assert server.is_command_safe(test_command)
@pytest.mark.skipif(sys.platform == "win32", reason="Process list format differs on Windows")
def test_list_sessions():
    """Test listing active command sessions."""
    # Clear any existing sessions
    with server.session_lock:
        server.active_sessions.clear()
    # Start a background process
    cmd = "sleep 10"  # Will run for 10 seconds
    result = server.execute_command(cmd, timeout=1, allow_background=True)
    pid = result["pid"]
    # Get session list
    sessions_result = server.list_sessions()
    # Verify session is in the list
    assert "sessions" in sessions_result
    assert len(sessions_result["sessions"]) > 0
    # Find our session in the list
    found = False
    for session in sessions_result["sessions"]:
        if session["pid"] == pid:
            found = True
            assert session["command"] == cmd
    assert found
    # Clean up
    server.force_terminate(pid)
</file>

<file path="tests/test_context_mapper.py">
"""Tests for the JavaScript context mapper."""
import unittest
import tempfile
import os
from pathlib import Path
from server.code_understanding.context_mapper import ContextMapper
class TestContextMapper(unittest.TestCase):
    """Test cases for the ContextMapper class."""
    def setUp(self):
        """Set up test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.mapper = ContextMapper(self.temp_dir)
        # Create test files
        self._create_test_files()
    def tearDown(self):
        """Clean up test environment."""
        import shutil
        shutil.rmtree(self.temp_dir)
    def _create_test_files(self):
        """Create test JavaScript files with various relationships."""
        # Create main.js
        main_content = """
        import { helper } from './utils/helper';
        import { Component } from 'react';
        import { asyncOperation } from './services/async';
        class MainComponent extends Component {
            constructor() {
                super();
                this.state = {};
            }
            async componentDidMount() {
                const result = await asyncOperation();
                helper(result);
            }
        }
        export default MainComponent;
        """
        self._write_file('main.js', main_content)
        # Create utils/helper.js
        helper_content = """
        import { format } from 'date-fns';
        export function helper(data) {
            return format(new Date(), 'yyyy-MM-dd');
        }
        """
        self._write_file('utils/helper.js', helper_content)
        # Create services/async.js
        async_content = """
        import { fetch } from 'node-fetch';
        export async function asyncOperation() {
            const response = await fetch('https://api.example.com');
            return response.json();
        }
        """
        self._write_file('services/async.js', async_content)
        # Create package.json
        package_json = {
            'dependencies': {
                'react': '^17.0.0',
                'date-fns': '^2.29.0',
                'node-fetch': '^2.6.0'
            }
        }
        self._write_file('package.json', str(package_json))
    def _write_file(self, rel_path: str, content: str):
        """Helper to write test files."""
        full_path = Path(self.temp_dir) / rel_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        full_path.write_text(content)
    def test_analyze_file(self):
        """Test analyzing a single JavaScript file."""
        with open(Path(self.temp_dir) / 'main.js') as f:
            content = f.read()
        result = self.mapper.analyze_file('main.js', content)
        # Check basic structure
        self.assertIn('file_path', result)
        self.assertIn('types', result)
        self.assertIn('contexts', result)
        self.assertIn('dependencies', result)
        self.assertIn('relationships', result)
        # Check types
        self.assertIn('MainComponent', result['types'])
        # Check contexts
        self.assertIn('MainComponent', result['contexts'])
        self.assertEqual(result['contexts']['MainComponent']['type'], 'class')
        # Check dependencies
        self.assertIn('direct', result['dependencies'])
        self.assertIn('transitive', result['dependencies'])
    def test_get_context(self):
        """Test getting context for symbols."""
        with open(Path(self.temp_dir) / 'main.js') as f:
            content = f.read()
        self.mapper.analyze_file('main.js', content)
        # Test class context
        context = self.mapper.get_context('main.js', 'MainComponent')
        self.assertIsNotNone(context)
        self.assertEqual(context['type'], 'class')
        # Test non-existent symbol
        context = self.mapper.get_context('main.js', 'NonExistent')
        self.assertIsNone(context)
    def test_get_relationships(self):
        """Test getting relationships between code elements."""
        with open(Path(self.temp_dir) / 'main.js') as f:
            content = f.read()
        self.mapper.analyze_file('main.js', content)
        # Get all relationships
        relationships = self.mapper.get_relationships('main.js')
        self.assertGreater(len(relationships), 0)
        # Check relationship types
        relationship_types = {r['type'] for r in relationships}
        self.assertIn('class_definition', relationship_types)
        self.assertIn('method', relationship_types)
        self.assertIn('module_dependency', relationship_types)
        # Get relationships for specific symbol
        main_relationships = self.mapper.get_relationships('main.js', 'MainComponent')
        self.assertGreater(len(main_relationships), 0)
        self.assertTrue(all(r['from'] == 'MainComponent' or r['to'] == 'MainComponent'
                          for r in main_relationships))
    def test_get_symbol_usage(self):
        """Test getting usage information for symbols."""
        with open(Path(self.temp_dir) / 'main.js') as f:
            content = f.read()
        self.mapper.analyze_file('main.js', content)
        # Test class usage
        usages = self.mapper.get_symbol_usage('main.js', 'MainComponent')
        self.assertGreater(len(usages), 0)
        self.assertTrue(any(u['type'] == 'class_usage' for u in usages))
        # Test method usage
        usages = self.mapper.get_symbol_usage('main.js', 'componentDidMount')
        self.assertGreater(len(usages), 0)
        self.assertTrue(any(u['type'] == 'function_usage' for u in usages))
    def test_get_dependency_graph(self):
        """Test generating the dependency graph."""
        # Analyze all files
        for file_path in ['main.js', 'utils/helper.js', 'services/async.js']:
            with open(Path(self.temp_dir) / file_path) as f:
                content = f.read()
            self.mapper.analyze_file(file_path, content)
        graph = self.mapper.get_dependency_graph()
        # Check nodes
        self.assertEqual(len(graph['nodes']), 3)  # main.js, helper.js, async.js
        # Check edges
        self.assertGreater(len(graph['edges']), 0)
        self.assertTrue(any(e['type'] == 'module_dependency' for e in graph['edges']))
    def test_get_symbol_graph(self):
        """Test generating the symbol relationship graph."""
        with open(Path(self.temp_dir) / 'main.js') as f:
            content = f.read()
        self.mapper.analyze_file('main.js', content)
        graph = self.mapper.get_symbol_graph('main.js')
        # Check nodes
        self.assertGreater(len(graph['nodes']), 0)
        self.assertTrue(any(n['type'] == 'class' for n in graph['nodes']))
        self.assertTrue(any(n['type'] == 'function' for n in graph['nodes']))
        # Check edges
        self.assertGreater(len(graph['edges']), 0)
        self.assertTrue(any(e['type'] == 'class_definition' for e in graph['edges']))
        self.assertTrue(any(e['type'] == 'method' for e in graph['edges']))
    def test_error_handling(self):
        """Test handling of invalid files and symbols."""
        # Test non-existent file
        result = self.mapper.analyze_file('nonexistent.js', '')
        self.assertEqual(result['types'], {})
        self.assertEqual(result['contexts'], {})
        # Test invalid JavaScript
        result = self.mapper.analyze_file('invalid.js', 'invalid javascript code')
        self.assertEqual(result['types'], {})
        self.assertEqual(result['contexts'], {})
        # Test getting context for non-existent file
        context = self.mapper.get_context('nonexistent.js', 'symbol')
        self.assertIsNone(context)
        # Test getting relationships for non-existent file
        relationships = self.mapper.get_relationships('nonexistent.js')
        self.assertEqual(relationships, [])
    def test_complex_relationships(self):
        """Test handling of complex code relationships."""
        complex_content = """
        class Base {
            constructor() {
                this.baseProp = 'base';
            }
            baseMethod() {
                return this.baseProp;
            }
        }
        class Derived extends Base {
            constructor() {
                super();
                this.derivedProp = 'derived';
            }
            derivedMethod() {
                return this.baseMethod() + this.derivedProp;
            }
        }
        const instance = new Derived();
        const result = instance.derivedMethod();
        """
        self._write_file('complex.js', complex_content)
        with open(Path(self.temp_dir) / 'complex.js') as f:
            content = f.read()
        result = self.mapper.analyze_file('complex.js', content)
        # Check class relationships
        self.assertIn('Base', result['contexts'])
        self.assertIn('Derived', result['contexts'])
        # Check inheritance
        relationships = self.mapper.get_relationships('complex.js')
        self.assertTrue(any(r['type'] == 'class_definition' and r['from'] == 'Derived'
                          for r in relationships))
        # Check method relationships
        self.assertTrue(any(r['type'] == 'method' and r['from'] == 'Derived'
                          and r['to'] == 'derivedMethod' for r in relationships))
        # Check property relationships
        self.assertTrue(any(r['type'] == 'property' and r['from'] == 'Derived'
                          and r['to'] == 'derivedProp' for r in relationships))
if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_file_operations.py">
"""Tests for file operation functionality."""
import os
import sys
import pytest
import json
from unittest.mock import patch
# Import the server module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import server
def test_read_file(sample_text_file):
    """Test reading file contents."""
    # Test with a valid file
    result = server.read_file(sample_text_file)
    # Verify result
    assert result["success"] is True
    assert "This is a sample test file." in result["content"]
    assert "Third line." in result["content"]
    assert result["size"] > 0
    # Test with a non-existent file
    result = server.read_file("/path/to/nonexistent/file")
    assert result["success"] is False
    assert "error" in result
def test_read_file_max_size(temp_dir):
    """Test file size limit when reading files."""
    # Create a large file
    large_file_path = os.path.join(temp_dir, "large.txt")
    with open(large_file_path, "w") as f:
        # Write ~1MB of data
        f.write("x" * 1_000_000)
    # Test with a small size limit (100KB)
    result = server.read_file(large_file_path, max_size_mb=0.1)
    # Should fail due to size limit
    assert result["success"] is False
    assert "size limit" in result["error"].lower()
    # Test with a larger size limit (2MB)
    result = server.read_file(large_file_path, max_size_mb=2)
    # Should succeed
    assert result["success"] is True
    assert len(result["content"]) > 0
def test_write_file(temp_dir):
    """Test writing content to a file."""
    test_file_path = os.path.join(temp_dir, "test_write.txt")
    test_content = "Hello, this is a test file content.\nSecond line."
    # Write to file
    result = server.write_file(test_file_path, test_content)
    # Verify result
    assert result["success"] is True
    assert os.path.exists(test_file_path)
    # Verify file content
    with open(test_file_path, "r") as f:
        content = f.read()
        assert content == test_content
def test_write_file_create_dirs(temp_dir):
    """Test writing to a file with directory creation."""
    nested_file_path = os.path.join(temp_dir, "nested", "dir", "test.txt")
    test_content = "File in nested directory."
    # Write to file with directory creation
    result = server.write_file(nested_file_path, test_content, create_dirs=True)
    # Verify result
    assert result["success"] is True
    assert os.path.exists(nested_file_path)
    # Verify file content
    with open(nested_file_path, "r") as f:
        content = f.read()
        assert content == test_content
def test_create_directory(temp_dir):
    """Test creating a directory."""
    new_dir_path = os.path.join(temp_dir, "new_directory")
    # Create directory
    result = server.create_directory(new_dir_path)
    # Verify result
    assert result["success"] is True
    assert os.path.exists(new_dir_path)
    assert os.path.isdir(new_dir_path)
    # Test creating a directory that already exists
    result = server.create_directory(new_dir_path)
    assert result["success"] is False  # Should fail
    # Test creating a nested directory
    nested_dir_path = os.path.join(temp_dir, "nested", "directory")
    result = server.create_directory(nested_dir_path)
    assert result["success"] is True
    assert os.path.exists(nested_dir_path)
def test_list_directory(temp_dir):
    """Test listing directory contents."""
    # Create some test files and directories
    os.mkdir(os.path.join(temp_dir, "test_dir"))
    with open(os.path.join(temp_dir, "test1.txt"), "w") as f:
        f.write("Test file 1")
    with open(os.path.join(temp_dir, "test2.txt"), "w") as f:
        f.write("Test file 2")
    with open(os.path.join(temp_dir, ".hidden"), "w") as f:
        f.write("Hidden file")
    # List directory without showing hidden files
    result = server.list_directory(temp_dir, show_hidden=False)
    # Verify result
    assert result["success"] is True
    assert len(result["contents"]) == 3  # test_dir, test1.txt, test2.txt
    assert not any(item["name"] == ".hidden" for item in result["contents"])
    # List directory showing hidden files
    result = server.list_directory(temp_dir, show_hidden=True)
    # Verify result
    assert result["success"] is True
    assert len(result["contents"]) == 4  # test_dir, test1.txt, test2.txt, .hidden
    assert any(item["name"] == ".hidden" for item in result["contents"])
    # Verify directory info
    dir_item = next(item for item in result["contents"] if item["name"] == "test_dir")
    assert dir_item["type"] == "directory"
    # Verify file info
    file_item = next(item for item in result["contents"] if item["name"] == "test1.txt")
    assert file_item["type"] == "file"
    assert file_item["size"] > 0
def test_move_file(temp_dir):
    """Test moving/renaming a file."""
    # Create a test file
    source_path = os.path.join(temp_dir, "source.txt")
    with open(source_path, "w") as f:
        f.write("Test file content")
    # Move the file
    destination_path = os.path.join(temp_dir, "destination.txt")
    result = server.move_file(source_path, destination_path)
    # Verify result
    assert result["success"] is True
    assert not os.path.exists(source_path)
    assert os.path.exists(destination_path)
    # Verify file content
    with open(destination_path, "r") as f:
        content = f.read()
        assert content == "Test file content"
    # Test moving a non-existent file
    result = server.move_file("/path/to/nonexistent/file", destination_path)
    assert result["success"] is False
def test_search_files(temp_dir):
    """Test searching for files matching a pattern."""
    # Create test files
    os.makedirs(os.path.join(temp_dir, "subdir"))
    files = [
        "test1.txt",
        "test2.log",
        "hello.txt",
        "subdir/nested.txt",
        "subdir/data.csv"
    ]
    for file_path in files:
        full_path = os.path.join(temp_dir, file_path)
        with open(full_path, "w") as f:
            f.write(f"Content of {file_path}")
    # Search for .txt files
    result = server.search_files(temp_dir, "*.txt", recursive=True)
    # Verify result
    assert result["success"] is True
    assert len(result["matches"]) == 3  # test1.txt, hello.txt, subdir/nested.txt
    # Search with non-recursive
    result = server.search_files(temp_dir, "*.txt", recursive=False)
    # Verify result - should only find files in the top directory
    assert result["success"] is True
    assert len(result["matches"]) == 2  # test1.txt, hello.txt
    # Search with limited results
    result = server.search_files(temp_dir, "*.*", recursive=True, max_results=2)
    # Verify result - should be limited to 2 results
    assert result["success"] is True
    assert len(result["matches"]) == 2
def test_get_file_info(sample_text_file):
    """Test getting file information."""
    # Get info for a regular file
    result = server.get_file_info(sample_text_file)
    # Verify result
    assert result["success"] is True
    assert result["exists"] is True
    assert result["type"] == "file"
    assert result["size"] > 0
    assert "modified" in result
    assert "permissions" in result
    # Get info for a directory
    dir_path = os.path.dirname(sample_text_file)
    result = server.get_file_info(dir_path)
    # Verify result
    assert result["success"] is True
    assert result["exists"] is True
    assert result["type"] == "directory"
    # Get info for a non-existent file
    result = server.get_file_info("/path/to/nonexistent/file")
    # Verify result
    assert result["success"] is True  # The operation succeeded even though file doesn't exist
    assert result["exists"] is False
</file>

<file path="tests/test_graph.py">
"""Tests for the graph data structure."""
import pytest
from server.code_understanding.graph import Graph, Node, Edge, RelationType
@pytest.fixture
def graph():
    """Create a test graph."""
    return Graph()
@pytest.fixture
def sample_nodes(graph):
    """Create sample nodes in the graph."""
    node1 = graph.add_node(
        name="test_function",
        type="function",
        file_path="test.py",
        start_line=1,
        end_line=10
    )
    node2 = graph.add_node(
        name="test_class",
        type="class",
        file_path="test.py",
        start_line=11,
        end_line=20
    )
    return node1, node2
def test_add_node(graph):
    """Test adding nodes to the graph."""
    # Test basic node addition
    node = graph.add_node(
        name="test",
        type="function",
        file_path="test.py"
    )
    assert node.name == "test"
    assert node.type == "function"
    assert node.file_path == "test.py"
    assert node.start_line == 0
    assert node.end_line == 0
    assert node.properties == {}
    # Test node with properties
    node_with_props = graph.add_node(
        name="test_with_props",
        type="class",
        file_path="test.py",
        start_line=1,
        end_line=10,
        properties={"key": "value"}
    )
    assert node_with_props.properties == {"key": "value"}
    # Test duplicate node (should return existing node)
    duplicate = graph.add_node(
        name="test",
        type="function",
        file_path="test.py"
    )
    assert duplicate.id == node.id
def test_add_edge(graph, sample_nodes):
    """Test adding edges to the graph."""
    node1, node2 = sample_nodes
    # Test basic edge addition
    edge = graph.add_edge(node1, node2, RelationType.CALLS)
    assert edge.source == node1
    assert edge.target == node2
    assert edge.type == RelationType.CALLS
    assert edge.properties == {}
    # Test edge with properties
    edge_with_props = graph.add_edge(
        node2,
        node1,
        RelationType.CONTAINS,
        properties={"line": 15}
    )
    assert edge_with_props.properties == {"line": 15}
def test_get_node(graph, sample_nodes):
    """Test getting nodes from the graph."""
    node1, node2 = sample_nodes
    # Test getting existing node
    found = graph.get_node(node1.id)
    assert found == node1
    # Test getting non-existent node
    not_found = graph.get_node("non_existent")
    assert not_found is None
def test_get_edges(graph, sample_nodes):
    """Test getting edges from the graph."""
    node1, node2 = sample_nodes
    # Add some edges
    edge1 = graph.add_edge(node1, node2, RelationType.CALLS)
    edge2 = graph.add_edge(node2, node1, RelationType.CONTAINS)
    # Test getting all edges
    all_edges = graph.get_edges()
    assert len(all_edges) == 2
    assert edge1 in all_edges
    assert edge2 in all_edges
    # Test filtering by source
    source_edges = graph.get_edges(source_id=node1.id)
    assert len(source_edges) == 1
    assert source_edges[0] == edge1
    # Test filtering by target
    target_edges = graph.get_edges(target_id=node2.id)
    assert len(target_edges) == 1
    assert target_edges[0] == edge1
    # Test filtering by relationship type
    calls_edges = graph.get_edges(rel_type=RelationType.CALLS)
    assert len(calls_edges) == 1
    assert calls_edges[0] == edge1
    # Test filtering with multiple criteria
    filtered_edges = graph.get_edges(
        source_id=node1.id,
        target_id=node2.id,
        rel_type=RelationType.CALLS
    )
    assert len(filtered_edges) == 1
    assert filtered_edges[0] == edge1
def test_get_nodes_by_type(graph, sample_nodes):
    """Test getting nodes by type."""
    node1, node2 = sample_nodes
    # Test getting function nodes
    function_nodes = graph.get_nodes_by_type("function")
    assert len(function_nodes) == 1
    assert function_nodes[0] == node1
    # Test getting class nodes
    class_nodes = graph.get_nodes_by_type("class")
    assert len(class_nodes) == 1
    assert class_nodes[0] == node2
    # Test getting non-existent type
    empty_nodes = graph.get_nodes_by_type("non_existent")
    assert len(empty_nodes) == 0
def test_get_nodes_by_file(graph, sample_nodes):
    """Test getting nodes by file path."""
    node1, node2 = sample_nodes
    # Test getting nodes from existing file
    file_nodes = graph.get_nodes_by_file("test.py")
    assert len(file_nodes) == 2
    assert node1 in file_nodes
    assert node2 in file_nodes
    # Test getting nodes from non-existent file
    empty_nodes = graph.get_nodes_by_file("non_existent.py")
    assert len(empty_nodes) == 0
def test_clear(graph, sample_nodes):
    """Test clearing the graph."""
    node1, node2 = sample_nodes
    graph.add_edge(node1, node2, RelationType.CALLS)
    # Verify graph has data
    assert len(graph.nodes) == 2
    assert len(graph.edges) == 1
    # Clear the graph
    graph.clear()
    # Verify graph is empty
    assert len(graph.nodes) == 0
    assert len(graph.edges) == 0
def test_node_creation():
    """Test creating and retrieving nodes in the graph."""
    graph = Graph()
    # Test node creation
    node = graph.add_node(
        name="test_function",
        type="function",
        file_path="test.py",
        start_line=10,
        end_line=20,
        properties={"visibility": "public"}
    )
    # Verify node properties
    assert node.name == "test_function"
    assert node.type == "function"
    assert node.file_path == "test.py"
    assert node.start_line == 10
    assert node.end_line == 20
    assert node.properties == {"visibility": "public"}
    # Test node retrieval
    retrieved_node = graph.get_node(node.id)
    assert retrieved_node == node
    # Test retrieval by type
    nodes_by_type = graph.get_nodes_by_type("function")
    assert len(nodes_by_type) == 1
    assert nodes_by_type[0] == node
def test_edge_creation():
    """Test creating and retrieving edges in the graph."""
    graph = Graph()
    # Create nodes
    node1 = graph.add_node(name="source", type="function", file_path="test.py")
    node2 = graph.add_node(name="target", type="function", file_path="test.py")
    # Create edge
    edge = graph.add_edge(
        source=node1,
        target=node2,
        type=RelationType.CALLS,
        properties={"line_number": 15}
    )
    # Verify edge properties
    assert edge.source == node1
    assert edge.target == node2
    assert edge.type == RelationType.CALLS
    assert edge.properties == {"line_number": 15}
    # Test edge retrieval
    edges = graph.get_edges(source_id=node1.id)
    assert len(edges) == 1
    assert edges[0] == edge
    edges = graph.get_edges(target_id=node2.id)
    assert len(edges) == 1
    assert edges[0] == edge
    edges = graph.get_edges(rel_type=RelationType.CALLS)
    assert len(edges) == 1
    assert edges[0] == edge
def test_edge_filtering():
    """Test filtering edges with multiple criteria."""
    graph = Graph()
    # Create nodes
    node1 = graph.add_node(name="source", type="function", file_path="test.py")
    node2 = graph.add_node(name="target", type="function", file_path="test.py")
    node3 = graph.add_node(name="other", type="function", file_path="test.py")
    # Create edges
    edge1 = graph.add_edge(
        source=node1,
        target=node2,
        type=RelationType.CALLS,
        properties={"line_number": 15}
    )
    edge2 = graph.add_edge(
        source=node2,
        target=node3,
        type=RelationType.REFERENCES,
        properties={"line_number": 20}
    )
    # Test filtering by source and type
    edges = graph.get_edges(source_id=node1.id, rel_type=RelationType.CALLS)
    assert len(edges) == 1
    assert edges[0] == edge1
    # Test filtering by target and type
    edges = graph.get_edges(target_id=node2.id, rel_type=RelationType.CALLS)
    assert len(edges) == 1
    assert edges[0] == edge1
    # Test filtering with no matches
    edges = graph.get_edges(source_id=node1.id, rel_type=RelationType.REFERENCES)
    assert len(edges) == 0
def test_node_filtering():
    """Test filtering nodes by type and file."""
    graph = Graph()
    # Create nodes
    node1 = graph.add_node(name="func1", type="function", file_path="test.py")
    node2 = graph.add_node(name="func2", type="function", file_path="test.py")
    node3 = graph.add_node(name="class1", type="class", file_path="test.py")
    node4 = graph.add_node(name="func3", type="function", file_path="other.py")
    # Test filtering by type
    function_nodes = graph.get_nodes_by_type("function")
    assert len(function_nodes) == 3
    assert all(n.type == "function" for n in function_nodes)
    class_nodes = graph.get_nodes_by_type("class")
    assert len(class_nodes) == 1
    assert class_nodes[0] == node3
    # Test filtering by file
    test_nodes = graph.get_nodes_by_file("test.py")
    assert len(test_nodes) == 3
    assert all(n.file_path == "test.py" for n in test_nodes)
    other_nodes = graph.get_nodes_by_file("other.py")
    assert len(other_nodes) == 1
    assert other_nodes[0] == node4
def test_duplicate_node_handling():
    """Test handling of duplicate node creation."""
    graph = Graph()
    # Create initial node
    node1 = graph.add_node(
        name="test_function",
        type="function",
        file_path="test.py"
    )
    # Try to create duplicate node
    node2 = graph.add_node(
        name="test_function",
        type="function",
        file_path="test.py"
    )
    # Verify same node is returned
    assert node1 == node2
    assert len(graph.nodes) == 1
def test_edge_properties():
    """Test edge property handling."""
    graph = Graph()
    # Create nodes
    node1 = graph.add_node(name="source", type="function", file_path="test.py")
    node2 = graph.add_node(name="target", type="function", file_path="test.py")
    # Create edge with properties
    properties = {
        "line_number": 15,
        "scope": "local",
        "context": "function_call"
    }
    edge = graph.add_edge(
        source=node1,
        target=node2,
        type=RelationType.CALLS,
        properties=properties
    )
    # Verify properties
    assert edge.properties == properties
    assert edge.properties["line_number"] == 15
    assert edge.properties["scope"] == "local"
    assert edge.properties["context"] == "function_call"
def test_node_properties():
    """Test node property handling."""
    graph = Graph()
    # Create node with properties
    properties = {
        "visibility": "public",
        "async": True,
        "decorators": ["@property"]
    }
    node = graph.add_node(
        name="test_function",
        type="function",
        file_path="test.py",
        properties=properties
    )
    # Verify properties
    assert node.properties == properties
    assert node.properties["visibility"] == "public"
    assert node.properties["async"] is True
    assert node.properties["decorators"] == ["@property"]
def test_relation_types():
    """Test all relation types are properly defined."""
    # Verify all expected relation types exist
    assert RelationType.IMPORTS.value == "imports"
    assert RelationType.INHERITS.value == "inherits"
    assert RelationType.CONTAINS.value == "contains"
    assert RelationType.CALLS.value == "calls"
    assert RelationType.REFERENCES.value == "references"
    # Verify no unexpected relation types
    relation_types = {t.value for t in RelationType}
    expected_types = {"imports", "inherits", "contains", "calls", "references"}
    assert relation_types == expected_types
</file>

<file path="tests/test_javascript_parser.py">
"""Tests for JavaScript code analysis via CodeAnalyzer and JS Adapter."""
import pytest
import logging
from pathlib import Path
# Import the main analyzer entry point
from server.code_understanding.analyzer import CodeAnalyzer 
# Import the JS Adapter to potentially force its loading if needed for testing
# No longer need direct import if CodeParser handles loading
# from server.code_understanding.javascript_parser import JavaScriptParserAdapter 
# Import common types if needed for constructing expected results
from server.code_understanding.common_types import MockNode, MockTree
logger = logging.getLogger(__name__)
# Configure logging - Set level to DEBUG to see detailed parser/adapter logs
logging.basicConfig(level=logging.DEBUG)
logging.getLogger('server.code_understanding').setLevel(logging.DEBUG)
def analyzer_fixture():
    """Fixture to provide a CodeAnalyzer instance."""
    # Ensure JS adapter tries to load if not already
    # We don't need to explicitly load it here anymore, 
    # CodeParser __init__ handles adapter loading.
    # try:
    #     JavaScriptParserAdapter() 
    # except Exception as e:
    #     logger.warning(f"Could not pre-initialize JS Adapter (might be ok if CodeParser handles it): {e}")
    return CodeAnalyzer()
@pytest.fixture
def analyzer():
    """Fixture to provide a CodeAnalyzer instance."""
    # We don't need to explicitly load it here anymore, 
    # CodeParser __init__ handles adapter loading.
    return CodeAnalyzer()
@pytest.fixture
def sample_js_code():
    """Sample JavaScript code for testing."""
    return """
const fs = require('fs'); // CommonJS require
import path from 'path'; // ES6 import default
import { readFileSync } from 'fs'; // ES6 named import
function greet(name) {
  console.log(`Hello, ${name}!`);
  return `Hello, ${name}!`;
}
// Arrow function assignment
const farewell = (name) => {
    console.log(`Goodbye, ${name}!`);
    return `Goodbye, ${name}!`;
};
class MyClass {
  constructor(value) {
    this.value = value;
  }
  getValue() {
    return this.value;
  }
  static staticMethod() {
    return 'Static method called';
  }
}
const instance = new MyClass(123);
greet('World');
farewell('Moon');
console.log(instance.getValue());
console.log(MyClass.staticMethod());
console.log(readFileSync); // Reference imported symbol
let myVar = 'test';
const myConst = 456;
export { MyClass, greet }; // Named export
export default farewell; // Default export
"""
# Mark test as potentially failing if JS support isn't fully wired
# @pytest.mark.xfail(reason="JavaScript parser adapter implementation or integration might be incomplete.")
def test_analyze_javascript_code(analyzer, sample_js_code):
    """Test analyzing a string of JavaScript code using CodeAnalyzer."""
    # analyzer = analyzer_fixture # No need to reassign
    # Assuming CodeParser will detect JS or we modify it to accept a hint
    # Let's add a language hint for clarity in testing
    result = analyzer.analyze_code(sample_js_code, language='javascript') 
    assert result is not None, "Analysis should return a result dictionary"
    # --- Assertions based on expected MockTree structure --- 
    # Verify imports/requires
    imports = result.get('imports', [])
    assert len(imports) == 3, f"Expected 3 imports/requires, found {len(imports)}"
    # Note: Adjust 'type' based on how the adapter differentiates them
    # Example expected types: 'require', 'import' (default), 'import' (named)
    require_fs = next((imp for imp in imports if imp.get('name') == 'fs' and imp.get('type') == 'require'), None)
    import_path = next((imp for imp in imports if imp.get('name') == 'path' and imp.get('type') == 'import'), None)
    import_readfilesync = next((imp for imp in imports if imp.get('name') == 'readFileSync' and imp.get('type') == 'import'), None)
    assert require_fs is not None, "Did not find require('fs')"
    assert import_path is not None, "Did not find import path"
    assert import_readfilesync is not None, "Did not find import readFileSync"
    assert import_readfilesync.get('module') == 'fs', "Named import readFileSync should have module 'fs'"
    # TODO: Add check for default import if adapter adds that info (e.g., import_path.get('is_default') == True)
    # Verify functions (top-level)
    functions = result.get('functions', [])
    assert len(functions) == 2, f"Expected 2 top-level functions (greet, farewell), found {len(functions)}"
    assert any(f['name'] == 'greet' for f in functions), "Did not find function 'greet'"
    # Assuming arrow functions assigned to const/let are treated as functions
    assert any(f['name'] == 'farewell' for f in functions), "Did not find arrow function 'farewell'"
    # Verify classes
    classes = result.get('classes', [])
    assert len(classes) == 1, "Should find one class"
    my_class = classes[0]
    assert my_class['name'] == 'MyClass', "Class name mismatch"
    # Verify methods within the class
    methods = my_class.get('methods', [])
    assert len(methods) == 3, f"Expected 3 methods, found {len(methods)}"
    assert any(meth['name'] == 'constructor' for meth in methods), "Did not find constructor"
    assert any(meth['name'] == 'getValue' for meth in methods), "Did not find getValue"
    assert any(meth['name'] == 'staticMethod' for meth in methods), "Did not find staticMethod"
    # TODO: Check for static flag on staticMethod if adapter adds it
    # Verify variables (top-level const/let/var)
    variables = result.get('variables', [])
    # Expecting: instance, myVar, myConst. 
    # NOTE: The analyzer currently extracts names from `variable_declarator` children
    # It might not distinguish between `instance` and the functions `greet`/`farewell` if they are under `variable_declaration`.
    # Let's refine the assertion based on actual output later.
    # For now, check that the expected *variable* names are present.
    var_names = {v['name'] for v in variables}
    assert 'instance' in var_names, "Did not find 'instance' variable"
    assert 'myVar' in var_names, "Did not find 'myVar' variable"
    assert 'myConst' in var_names, "Did not find 'myConst' variable"
    # Check that functions declared with `function` keyword are NOT in variables
    assert 'greet' not in var_names, "Function 'greet' should not be in variables list"
    # Check if arrow function is treated as variable or function
    # assert 'farewell' not in var_names, "Arrow function 'farewell' should not be in variables list if treated as function"
    assert len(variables) >= 3, f"Expected at least 3 top-level variables, found {len(variables)}"
    # Verify exports (Need CodeAnalyzer to extract this, maybe from 'export_statement' nodes)
    # exports = result.get('exports', [])
    # assert len(exports) == 3, "Expected 3 exports"
    # assert any(exp['name'] == 'MyClass' for exp in exports)
    # assert any(exp['name'] == 'greet' for exp in exports)
    # assert any(exp['is_default'] for exp in exports)
# TODO: Add test_analyze_javascript_file using tmp_path fixture
# TODO: Add tests for syntax errors
# TODO: Add tests for different JS features (async/await, exports variants, etc.)
</file>

<file path="tests/test_javascript_support.py">
'''Tests for JavaScript parsing support.'''
import unittest
from server.code_understanding.language_adapters import JavaScriptParserAdapter
from server.code_understanding.symbols import SymbolExtractor
class TestJavaScriptParserAdapter(unittest.TestCase):
    def test_parse_valid_js(self):
        adapter = JavaScriptParserAdapter()
        code = "function test() { console.log('Hello'); }"
        tree = adapter.parse(code)
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'program')
    def test_parse_invalid_js(self):
        adapter = JavaScriptParserAdapter()
        code = ""  # Empty code
        with self.assertRaises(ValueError):
            adapter.parse(code)
    def test_parse_js_class(self):
        adapter = JavaScriptParserAdapter()
        code = "class Test { constructor() { console.log('Test'); } }"
        tree = adapter.parse(code)
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'program')
    def test_parse_js_variable(self):
        adapter = JavaScriptParserAdapter()
        code = "let x = 42;"
        tree = adapter.parse(code)
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'program')
class TestJavaScriptSymbolExtraction(unittest.TestCase):
    def test_extract_function(self):
        adapter = JavaScriptParserAdapter()
        code = "function test() { console.log('Hello'); }"
        tree = adapter.parse(code)
        extractor = SymbolExtractor()
        symbols = extractor.extract_symbols(tree)
        self.assertIn('functions', symbols)
        self.assertEqual(len(symbols['functions']), 1)
        self.assertEqual(symbols['functions'][0]['name'], 'test')
    def test_extract_class(self):
        adapter = JavaScriptParserAdapter()
        code = "class Test { constructor() { console.log('Test'); } }"
        tree = adapter.parse(code)
        extractor = SymbolExtractor()
        symbols = extractor.extract_symbols(tree)
        self.assertIn('classes', symbols)
        self.assertEqual(len(symbols['classes']), 1)
        self.assertEqual(symbols['classes'][0]['name'], 'Test')
    def test_extract_variable(self):
        adapter = JavaScriptParserAdapter()
        code = "let x = 42;"
        tree = adapter.parse(code)
        extractor = SymbolExtractor()
        symbols = extractor.extract_symbols(tree)
        self.assertIn('variables', symbols)
        self.assertEqual(len(symbols['variables']), 1)
        self.assertEqual(symbols['variables'][0]['name'], 'x')
if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_language_adapters.py">
'''Tests for JavaScript and Swift parsing adapters.'''
import unittest
from server.code_understanding.language_adapters import JavaScriptParserAdapter, SwiftParserAdapter
class TestJavaScriptParserAdapter(unittest.TestCase):
    def test_parse_valid_js(self):
        adapter = JavaScriptParserAdapter()
        code = "function test() { console.log('Hello'); }"
        tree = adapter.parse(code)
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'program')
    def test_parse_invalid_js(self):
        adapter = JavaScriptParserAdapter()
        code = ""  # Empty code
        with self.assertRaises(ValueError):
            adapter.parse(code)
    def test_parse_js_class(self):
        adapter = JavaScriptParserAdapter()
        code = "class Test { constructor() { console.log('Test'); } }"
        tree = adapter.parse(code)
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'program')
    def test_parse_js_variable(self):
        adapter = JavaScriptParserAdapter()
        code = "let x = 42;"
        tree = adapter.parse(code)
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'program')
class TestSwiftParserAdapter(unittest.TestCase):
    def test_parse_valid_swift(self):
        adapter = SwiftParserAdapter()
        code = "func test() { print(\"Hello\") }"
        tree = adapter.parse(code)
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'source_file')
    def test_parse_invalid_swift(self):
        adapter = SwiftParserAdapter()
        code = ""  # Empty code
        with self.assertRaises(ValueError):
            adapter.parse(code)
    def test_parse_swift_class(self):
        adapter = SwiftParserAdapter()
        code = "class Test { init() { print(\"Test\") } }"
        tree = adapter.parse(code)
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'source_file')
    def test_parse_swift_variable(self):
        adapter = SwiftParserAdapter()
        code = "var x: Int = 42"
        tree = adapter.parse(code)
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'source_file')
if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_language_support.py">
'''python
import pytest
from code_understanding.language_parser import MultiLanguageParser
def test_python_parsing():
    dummy_code = 'def foo(): pass'
    parser = MultiLanguageParser()
    result = parser.parse_code(dummy_code, 'python')
    assert isinstance(result, dict)
    # Assuming the python adapter returns a key 'language' with value 'python'
    assert result.get('language') == 'python'
def test_javascript_parsing():
    dummy_code = 'function foo() {}'
    parser = MultiLanguageParser()
    result = parser.parse_code(dummy_code, 'javascript')
    assert isinstance(result, dict)
    assert result.get('language') == 'javascript'
def test_swift_parsing():
    dummy_code = 'func foo() {}'
    parser = MultiLanguageParser()
    result = parser.parse_code(dummy_code, 'swift')
    assert isinstance(result, dict)
    assert result.get('language') == 'swift'
def test_unsupported_language():
    parser = MultiLanguageParser()
    with pytest.raises(ValueError):
        parser.parse_code('some code', 'ruby')
'''
</file>

<file path="tests/test_module_resolver.py">
"""Tests for the JavaScript module resolver."""
import unittest
import tempfile
import os
from pathlib import Path
from server.code_understanding.module_resolver import ModuleResolver
class TestModuleResolver(unittest.TestCase):
    """Test cases for the ModuleResolver class."""
    def setUp(self):
        """Set up test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.resolver = ModuleResolver(self.temp_dir)
        # Create test files
        self._create_test_files()
    def tearDown(self):
        """Clean up test environment."""
        import shutil
        shutil.rmtree(self.temp_dir)
    def _create_test_files(self):
        """Create test JavaScript files with various import patterns."""
        # Create main.js
        main_content = """
        import { helper } from './utils/helper';
        import { Component } from 'react';
        import { asyncOperation } from './services/async';
        """
        self._write_file('main.js', main_content)
        # Create utils/helper.js
        helper_content = """
        import { format } from 'date-fns';
        export function helper() {
            return format(new Date(), 'yyyy-MM-dd');
        }
        """
        self._write_file('utils/helper.js', helper_content)
        # Create services/async.js
        async_content = """
        import { fetch } from 'node-fetch';
        export async function asyncOperation() {
            const response = await fetch('https://api.example.com');
            return response.json();
        }
        """
        self._write_file('services/async.js', async_content)
        # Create package.json
        package_json = {
            'dependencies': {
                'react': '^17.0.0',
                'date-fns': '^2.29.0',
                'node-fetch': '^2.6.0'
            }
        }
        self._write_file('package.json', str(package_json))
    def _write_file(self, rel_path: str, content: str):
        """Helper to write test files."""
        full_path = Path(self.temp_dir) / rel_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        full_path.write_text(content)
    def test_resolve_import(self):
        """Test resolving import paths."""
        # Test relative import
        helper_path = self.resolver.resolve_import('./utils/helper', 'main.js')
        self.assertIsNotNone(helper_path)
        self.assertTrue(helper_path.exists())
        # Test package import
        react_path = self.resolver.resolve_import('react', 'main.js')
        self.assertIsNotNone(react_path)
        self.assertTrue(react_path.exists())
        # Test non-existent import
        invalid_path = self.resolver.resolve_import('./nonexistent', 'main.js')
        self.assertIsNone(invalid_path)
    def test_get_module_dependencies(self):
        """Test getting module dependencies."""
        deps = self.resolver.get_module_dependencies('main.js')
        # Check direct dependencies
        self.assertEqual(len(deps['direct']), 3)
        self.assertIn('utils/helper.js', deps['direct'])
        self.assertIn('services/async.js', deps['direct'])
        # Check transitive dependencies
        self.assertGreater(len(deps['transitive']), 0)
    def test_get_module_graph(self):
        """Test generating module dependency graph."""
        graph = self.resolver.get_module_graph()
        # Check nodes
        self.assertEqual(len(graph['nodes']), 3)  # main.js, helper.js, async.js
        # Check edges
        self.assertEqual(len(graph['edges']), 3)  # main.js -> helper.js, main.js -> async.js, helper.js -> date-fns
    def test_find_circular_dependencies(self):
        """Test finding circular dependencies."""
        # Create circular dependency
        circular_content = """
        import { circular } from './circular';
        export function circular() {}
        """
        self._write_file('circular.js', circular_content)
        cycles = self.resolver.find_circular_dependencies()
        self.assertEqual(len(cycles), 1)
    def test_get_module_stats(self):
        """Test getting module statistics."""
        stats = self.resolver.get_module_stats()
        # Check basic stats
        self.assertEqual(stats['total_modules'], 3)
        self.assertGreater(stats['total_dependencies'], 0)
        # Check module types
        self.assertEqual(stats['module_types']['.js'], 3)
        # Check dependency counts
        self.assertGreater(len(stats['dependency_counts']), 0)
    def test_resolve_package_import(self):
        """Test resolving package imports."""
        # Test existing package
        react_path = self.resolver._resolve_package_import('react')
        self.assertIsNotNone(react_path)
        # Test non-existent package
        invalid_path = self.resolver._resolve_package_import('nonexistent-package')
        self.assertIsNone(invalid_path)
    def test_extension_resolution(self):
        """Test resolving imports with different extensions."""
        # Create TypeScript file
        ts_content = "export const tsHelper = () => {};"
        self._write_file('utils/helper.ts', ts_content)
        # Test resolving .ts extension
        ts_path = self.resolver.resolve_import('./utils/helper', 'main.js')
        self.assertIsNotNone(ts_path)
        self.assertEqual(ts_path.suffix, '.ts')
    def test_index_file_resolution(self):
        """Test resolving index file imports."""
        # Create index.js
        index_content = "export const indexHelper = () => {};"
        self._write_file('utils/index.js', index_content)
        # Test resolving directory import
        index_path = self.resolver.resolve_import('./utils', 'main.js')
        self.assertIsNotNone(index_path)
        self.assertEqual(index_path.name, 'index.js')
    def test_dynamic_imports(self):
        """Test resolving dynamic imports."""
        # Create file with dynamic import
        dynamic_content = """
        const loadModule = () => import('./dynamic');
        """
        self._write_file('dynamic.js', dynamic_content)
        # Test resolving dynamic import
        deps = self.resolver.get_module_dependencies('dynamic.js')
        self.assertEqual(len(deps['direct']), 0)  # Dynamic imports are not included in static analysis
</file>

<file path="tests/test_observability_tools.py">
"""Tests for observability tools - tracing and metrics."""
import os
import sys
import pytest
from unittest.mock import patch, MagicMock
# Import the server module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
from server import core
@pytest.fixture
def mock_tracer():
    """Set up mock tracer for testing."""
    with patch('server.core.trace') as mock_trace:
        mock_span = MagicMock()
        mock_span.name = "test_span"
        mock_span.get_span_context.return_value = "test-context"
        mock_trace.get_current_span.return_value = mock_span
        mock_trace.get_tracer_provider.return_value.__class__.__name__ = "TestTracerProvider"
        yield mock_trace
@pytest.fixture
def mock_exporter():
    """Set up mock exporter for testing."""
    with patch('server.core.otlp_exporter') as mock_exp:
        mock_exp.endpoint = "http://localhost:4317"
        mock_exp.__class__.__name__ = "OTLPSpanExporter"
        yield mock_exp
class TestGetTraceInfo:
    """Tests for get_trace_info tool."""
    def test_get_trace_info_success(self, mock_tracer, mock_exporter):
        """Test successful retrieval of tracing information."""
        # Execute the tool
        result = core.get_trace_info()
        # Verify result
        assert result["status"] == "success"
        assert result["tracer"]["name"] == mock_tracer.name
        assert result["tracer"]["version"] == "TestTracerProvider"
        assert result["current_span"]["name"] == "test_span"
        assert result["current_span"]["context"] == "test-context"
        assert result["current_span"]["active"] == True
        assert result["exporter"]["type"] == "OTLPSpanExporter"
        assert result["exporter"]["endpoint"] == "http://localhost:4317"
    def test_get_trace_info_no_span(self, mock_tracer):
        """Test tracing info when no span is active."""
        # Mock no active span
        mock_tracer.get_current_span.return_value = None
        # Execute the tool
        result = core.get_trace_info()
        # Verify result
        assert result["status"] == "success"
        assert result["current_span"]["name"] == None
        assert result["current_span"]["context"] == None
        assert result["current_span"]["active"] == False
    def test_get_trace_info_error(self, mock_tracer):
        """Test error handling in get_trace_info."""
        # Mock exception
        mock_tracer.get_current_span.side_effect = Exception("Test error")
        # Execute the tool
        result = core.get_trace_info()
        # Verify result
        assert result["status"] == "error"
        assert "error" in result
        assert "Test error" in result["error"]
class TestConfigureTracing:
    """Tests for configure_tracing tool."""
    def test_configure_tracing_endpoint(self, monkeypatch):
        """Test configuring tracing with custom endpoint."""
        # Mock dependencies
        mock_exporter = MagicMock()
        mock_processor = MagicMock()
        mock_provider = MagicMock()
        mock_resource = MagicMock()
        monkeypatch.setattr('server.core.OTLPSpanExporter', lambda endpoint: mock_exporter)
        monkeypatch.setattr('server.core.BatchSpanProcessor', lambda exporter: mock_processor)
        monkeypatch.setattr('server.core.TracerProvider', lambda resource: mock_provider)
        monkeypatch.setattr('server.core.resource', mock_resource)
        # Set up mock attributes
        mock_exporter.endpoint = "http://custom:4317"
        mock_resource.attributes = {}
        # Execute the tool
        result = core.configure_tracing(exporter_endpoint="http://custom:4317")
        # Verify result
        assert result["status"] == "success"
        assert "config" in result
        assert result["config"]["exporter_endpoint"] == "http://custom:4317"
    def test_configure_tracing_service_info(self, monkeypatch):
        """Test configuring tracing with service information."""
        # Mock dependencies
        mock_resource = MagicMock()
        mock_resource_class = MagicMock(return_value=mock_resource)
        mock_provider = MagicMock()
        mock_attributes = {}
        monkeypatch.setattr('server.core.Resource', mock_resource_class)
        monkeypatch.setattr('server.core.TracerProvider', lambda resource: mock_provider)
        monkeypatch.setattr('server.core.otlp_exporter.endpoint', "http://localhost:4317")
        # Set up attributes
        def get_attr(key, default=None):
            return mock_attributes.get(key, default)
        mock_resource.attributes = MagicMock()
        mock_resource.attributes.get = get_attr
        # Execute the tool
        result = core.configure_tracing(
            service_name="test-service", 
            service_version="1.0.0"
        )
        # Verify result
        assert result["status"] == "success"
        assert "config" in result
        assert result["config"]["exporter_endpoint"] == "http://localhost:4317"
        # Verify Resource was called with attributes
        resource_call = mock_resource_class.call_args[0][0]
        assert "attributes" in resource_call
    def test_configure_tracing_error(self, monkeypatch):
        """Test error handling in configure_tracing."""
        # Mock exception
        def raise_error(*args, **kwargs):
            raise Exception("Test error")
        monkeypatch.setattr('server.core.OTLPSpanExporter', raise_error)
        # Execute the tool
        result = core.configure_tracing(exporter_endpoint="http://custom:4317")
        # Verify result
        assert result["status"] == "error"
        assert "error" in result
        assert "Test error" in result["error"]
</file>

<file path="tests/test_parser.py">
'''Tests for the code parser.'''
import unittest
from server.code_understanding.parser import MockTree, MockNode, CodeParser
class TestMockTree(unittest.TestCase):
    def test_walk_empty(self):
        tree = MockTree(None)
        walked = list(tree.walk())
        self.assertEqual(walked, [])
    def test_walk_with_root(self):
        root = MockNode(type='module', text='root')
        child = MockNode(type='child', text='child')
        root.children.append(child)
        tree = MockTree(root)
        walked = list(tree.walk())
        self.assertEqual([node.text for node in walked], ['root', 'child'])
class TestCodeParser(unittest.TestCase):
    def test_parse_valid_code(self):
        cp = CodeParser()
        tree = cp.parse("a = 42")
        self.assertIsNotNone(tree)
        self.assertEqual(tree.root_node.type, 'module')
if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_relationship_extractor.py">
"""Tests for the JavaScript relationship extractor."""
import unittest
import tempfile
import os
from pathlib import Path
from server.code_understanding.relationship_extractor import JavaScriptRelationshipExtractor
class TestJavaScriptRelationshipExtractor(unittest.TestCase):
    """Test cases for JavaScriptRelationshipExtractor."""
    def setUp(self):
        """Set up test environment."""
        self.temp_dir = tempfile.mkdtemp()
        self.root_dir = Path(self.temp_dir)
        self.extractor = JavaScriptRelationshipExtractor(str(self.root_dir))
    def tearDown(self):
        """Clean up test environment."""
        import shutil
        shutil.rmtree(self.temp_dir)
    def _create_test_files(self):
        """Create test JavaScript files."""
        # Create main.js
        main_content = """
import { helper } from './utils/helper';
import { asyncOperation } from './services/async';
export class Calculator {
    constructor() {
        this.state = { value: 0 };
    }
    add(x) {
        return helper.add(this.state.value, x);
    }
    async multiply(x) {
        return await asyncOperation.multiply(this.state.value, x);
    }
}
export default Calculator;
"""
        main_path = self.root_dir / 'main.js'
        main_path.parent.mkdir(exist_ok=True)
        main_path.write_text(main_content)
        # Create utils/helper.js
        helper_content = """
export const helper = {
    add(a, b) {
        return a + b;
    }
};
"""
        helper_path = self.root_dir / 'utils' / 'helper.js'
        helper_path.parent.mkdir(exist_ok=True)
        helper_path.write_text(helper_content)
        # Create services/async.js
        async_content = """
export const asyncOperation = {
    async multiply(a, b) {
        return new Promise(resolve => {
            setTimeout(() => resolve(a * b), 100);
        });
    }
};
"""
        async_path = self.root_dir / 'services' / 'async.js'
        async_path.parent.mkdir(exist_ok=True)
        async_path.write_text(async_content)
        return {
            'main': main_path,
            'helper': helper_path,
            'async': async_path
        }
    def test_analyze_file(self):
        """Test analyzing a single JavaScript file."""
        files = self._create_test_files()
        main_path = files['main']
        # Analyze main.js
        with open(main_path) as f:
            content = f.read()
        result = self.extractor.analyze_file(str(main_path), content)
        # Check imports
        self.assertIn('imports', result)
        imports = result['imports']
        self.assertEqual(len(imports), 2)
        self.assertIn('./utils/helper', imports)
        self.assertIn('./services/async', imports)
        # Check exports
        self.assertIn('exports', result)
        exports = result['exports']
        self.assertEqual(len(exports), 2)
        self.assertIn('Calculator', exports)
        self.assertIn('default', exports)
        # Check symbols
        self.assertIn('symbols', result)
        symbols = result['symbols']
        self.assertIn('Calculator', symbols)
        self.assertIn('state', symbols)
        # Check relationships
        self.assertIn('relationships', result)
        relationships = result['relationships']
        self.assertTrue(any(r['type'] == 'import' for r in relationships))
        self.assertTrue(any(r['type'] == 'export' for r in relationships))
        self.assertTrue(any(r['type'] == 'symbol' for r in relationships))
    def test_cross_file_references(self):
        """Test tracking cross-file references."""
        files = self._create_test_files()
        main_path = files['main']
        helper_path = files['helper']
        async_path = files['async']
        # Analyze all files
        for file_path in files.values():
            with open(file_path) as f:
                content = f.read()
            self.extractor.analyze_file(str(file_path), content)
        # Check cross-file references for main.js
        refs = self.extractor.get_cross_file_references(str(main_path))
        # Check outgoing references
        self.assertEqual(len(refs['outgoing']), 2)
        self.assertIn(str(helper_path), refs['outgoing'])
        self.assertIn(str(async_path), refs['outgoing'])
        # Check incoming references
        self.assertEqual(len(refs['incoming']), 0)
        # Check cross-file references for helper.js
        refs = self.extractor.get_cross_file_references(str(helper_path))
        self.assertEqual(len(refs['outgoing']), 0)
        self.assertEqual(len(refs['incoming']), 1)
        self.assertIn(str(main_path), refs['incoming'])
    def test_module_graph(self):
        """Test generating module dependency graph."""
        files = self._create_test_files()
        # Analyze all files
        for file_path in files.values():
            with open(file_path) as f:
                content = f.read()
            self.extractor.analyze_file(str(file_path), content)
        # Get module graph
        graph = self.extractor.get_module_graph()
        # Check nodes
        self.assertEqual(len(graph['nodes']), 3)
        node_paths = {n['id'] for n in graph['nodes']}
        for file_path in files.values():
            self.assertIn(str(file_path), node_paths)
        # Check edges
        self.assertEqual(len(graph['edges']), 2)
        edge_paths = {(e['from'], e['to']) for e in graph['edges']}
        self.assertIn((str(files['main']), str(files['helper'])), edge_paths)
        self.assertIn((str(files['main']), str(files['async'])), edge_paths)
    def test_error_handling(self):
        """Test handling of invalid files."""
        # Test non-existent file
        result = self.extractor.analyze_file('nonexistent.js', '')
        self.assertIn('error', result)
        # Test invalid JavaScript
        result = self.extractor.analyze_file('invalid.js', 'invalid javascript code')
        self.assertIn('error', result)
    def test_complex_imports(self):
        """Test handling of complex import patterns."""
        content = """
import { default as React } from 'react';
import * as utils from './utils';
import './styles.css';
import type { Props } from './types';
import { Component } from '@angular/core';
"""
        result = self.extractor.analyze_file('test.js', content)
        # Check imports
        imports = result['imports']
        self.assertEqual(len(imports), 5)
        self.assertIn('react', imports)
        self.assertIn('./utils', imports)
        self.assertIn('./styles.css', imports)
        self.assertIn('./types', imports)
        self.assertIn('@angular/core', imports)
    def test_complex_exports(self):
        """Test handling of complex export patterns."""
        content = """
export const constant = 42;
export function helper() {}
export class Component {}
export default class App {}
export { helper as util };
export * from './other';
"""
        result = self.extractor.analyze_file('test.js', content)
        # Check exports
        exports = result['exports']
        self.assertEqual(len(exports), 6)
        self.assertIn('constant', exports)
        self.assertIn('helper', exports)
        self.assertIn('Component', exports)
        self.assertIn('default', exports)
        self.assertIn('util', exports)
        self.assertIn('*', exports)
    def test_symbol_types(self):
        """Test symbol type inference."""
        content = """
const number = 42;
let string = 'hello';
var boolean = true;
const array = [1, 2, 3];
const object = { key: 'value' };
function func() {}
class Class {}
"""
        result = self.extractor.analyze_file('test.js', content)
        # Check symbol types
        symbols = result['symbols']
        self.assertEqual(symbols['number'], 'number')
        self.assertEqual(symbols['string'], 'string')
        self.assertEqual(symbols['boolean'], 'boolean')
        self.assertEqual(symbols['array'], 'array')
        self.assertEqual(symbols['object'], 'object')
        self.assertEqual(symbols['func'], 'function')
        self.assertEqual(symbols['Class'], 'class')
if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_relationships.py">
"""Tests for the relationship builder."""
import os
import pytest
from pathlib import Path
from unittest.mock import Mock, patch
from server.code_understanding.relationships import (
    RelationshipBuilder,
    FileContext,
    IGNORED_NAMES
)
from server.code_understanding.graph import Graph, Node, Edge, RelationType, NodeType
from server.code_understanding.mock_parser import MockParser
@pytest.fixture
def sample_code():
    """Sample Python code for testing."""
    return """
import os
from sys import path
def hello(name):
    print(f"Hello {name}")
    return path
class Greeter:
    def __init__(self, prefix="Hello"):
        self.prefix = prefix
    def greet(self, name):
        return f"{self.prefix} {name}"
    @staticmethod
    def say_hi():
        return "Hi there!"
def main():
    g = Greeter("Hey")
    hello("World")
    print(g.greet("Universe"))
    print(Greeter.say_hi())
"""
@pytest.fixture
def mock_parser():
    """Create a mock parser for testing."""
    parser = Mock()
    parser.parse.return_value = Mock()  # Mock tree
    return parser
@pytest.fixture
def mock_extractor():
    """Create a mock extractor for testing."""
    extractor = Mock()
    extractor.extract_symbols.return_value = (
        {
            'imports': [
                {'module': 'os', 'start_line': 1, 'end_line': 1},
                {'module': 'sys', 'symbol': 'path', 'start_line': 2, 'end_line': 2}
            ],
            'functions': [
                {'name': 'hello', 'start_line': 4, 'end_line': 6},
                {'name': 'main', 'start_line': 20, 'end_line': 25}
            ],
            'classes': [
                {
                    'name': 'Greeter',
                    'start_line': 8, 'end_line': 18,
                    'methods': [
                        {'name': '__init__', 'start_line': 9, 'end_line': 10},
                        {'name': 'greet', 'start_line': 12, 'end_line': 13},
                        {'name': 'say_hi', 'start_line': 15, 'end_line': 16}
                    ]
                }
            ],
            'variables': []
        },
        {
            'calls': [
                {'name': 'print', 'scope': 'hello', 'start_line': 5, 'end_line': 5},
                {'name': 'Greeter', 'scope': 'main', 'start_line': 21, 'end_line': 21},
                {'name': 'hello', 'scope': 'main', 'start_line': 22, 'end_line': 22},
                {'name': 'greet', 'scope': 'g', 'start_line': 23, 'end_line': 23},
                {'name': 'say_hi', 'scope': 'Greeter', 'start_line': 24, 'end_line': 24},
                {'name': 'print', 'scope': 'main', 'start_line': 23, 'end_line': 23},
                {'name': 'print', 'scope': 'main', 'start_line': 24, 'end_line': 24}
            ],
            'attributes': [
                {'name': 'prefix', 'scope': 'self', 'start_line': 10, 'end_line': 10}
            ],
            'variables': []
        }
    )
    return extractor
@pytest.fixture
def builder():
    """Create a relationship builder with a mock parser."""
    mock_parser = MockParser()
    builder = RelationshipBuilder()
    builder.parser = mock_parser
    return builder
def test_analyze_file(tmp_path, builder, sample_code):
    """Test analyzing a single file."""
    # Create test file
    file_path = tmp_path / "test.py"
    file_path.write_text(sample_code)
    # Analyze file
    builder.analyze_file(str(file_path))
    # Verify file context was created
    assert str(file_path) in builder.file_contexts
    context = builder.file_contexts[str(file_path)]
    assert context.path == str(file_path)
    assert context.code == sample_code
    # Verify relationships were built
    graph = builder.get_relationships()
    assert len(graph.nodes) > 0
    assert len(graph.edges) > 0
def test_analyze_directory(tmp_path, builder, sample_code):
    """Test analyzing a directory of Python files."""
    # Create test directory structure
    module1 = tmp_path / "module1.py"
    module1.write_text("def func1(): return 'Hello'")
    module2 = tmp_path / "module2.py"
    module2.write_text("from module1 import func1\ndef func2(): return func1()")
    # Create a subdirectory with another file
    subdir = tmp_path / "subdir"
    subdir.mkdir()
    module3 = subdir / "module3.py"
    module3.write_text("from ..module2 import func2\ndef func3(): return func2()")
    # Analyze directory
    builder.analyze_directory(str(tmp_path))
    # Verify all files were analyzed
    assert str(module1) in builder.file_contexts
    assert str(module2) in builder.file_contexts
    assert str(module3) in builder.file_contexts
def test_analyze_file_with_code(builder, sample_code):
    """Test analyzing a file with provided code string."""
    file_path = "test.py"
    builder.analyze_file(file_path, code=sample_code)
    # Verify file context was created
    assert file_path in builder.file_contexts
    context = builder.file_contexts[file_path]
    assert context.path == file_path
    assert context.code == sample_code
def test_analyze_file_not_found(builder):
    """Test handling of non-existent files."""
    with pytest.raises(FileNotFoundError):
        builder.analyze_file("non_existent_file.py")
def test_analyze_directory_not_found(builder):
    """Test handling of non-existent directories."""
    with pytest.raises(FileNotFoundError):
        builder.analyze_directory("non_existent_directory")
def test_clear(builder, tmp_path, sample_code):
    """Test clearing the relationship builder."""
    # Add some data
    file_path = tmp_path / "test.py"
    file_path.write_text(sample_code)
    builder.analyze_file(str(file_path))
    # Verify data exists
    assert len(builder.file_contexts) > 0
    assert len(builder.get_relationships().nodes) > 0
    # Clear the builder
    builder.clear()
    # Verify data was cleared
    assert len(builder.file_contexts) == 0
    assert len(builder.get_relationships().nodes) == 0
    assert len(builder.get_relationships().edges) == 0
def test_process_imports(builder):
    """Test processing import statements."""
    # Create current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={'file_path': "test.py"}
    )
    # Create a mock file context
    context = FileContext(
        path="test.py",
        code="import os\nfrom sys import path\nfrom typing import List as ListType",
        tree=Mock(),
        symbols={
            'imports': [
                {'type': 'import', 'module': 'os', 'start_line': 1, 'end_line': 1},
                {'type': 'import', 'module': 'sys', 'symbol': 'path', 'start_line': 2, 'end_line': 2},
                {'type': 'import', 'module': 'typing', 'symbol': 'List', 'alias': 'ListType', 'start_line': 3, 'end_line': 3}
            ]
        }
    )
    # Process imports
    builder._process_imports(context)
def test_process_classes(builder):
    """Test processing class definitions."""
    # Create a current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={
            'file_path': "test.py"
        }
    )
    context = FileContext(
        path="test.py",
        code="",
        tree=Mock(),
        symbols={
            'classes': [
                {
                    'name': 'TestClass',
                    'start_line': 1,
                    'end_line': 10,
                    'methods': [
                        {'name': 'method1', 'start_line': 2, 'end_line': 3},
                        {'name': 'method2', 'start_line': 4, 'end_line': 5}
                    ]
                }
            ]
        }
    )
    builder._process_classes(context)
    graph = builder.get_relationships()
    # Verify class node was created
    class_nodes = graph.get_nodes_by_type(NodeType.CLASS.value)
    assert len(class_nodes) == 1
    # Verify method nodes were created
    method_nodes = graph.get_nodes_by_type(NodeType.METHOD.value)
    assert len(method_nodes) == 2
    # Verify edges were created
    contains_edges = graph.get_edges(rel_type=RelationType.CONTAINS.value)
    assert len(contains_edges) == 3  # file -> class, class -> method1, class -> method2
def test_process_functions(builder):
    """Test processing function definitions."""
    # Create a current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={
            'file_path': "test.py"
        }
    )
    context = FileContext(
        path="test.py",
        code="",
        tree=Mock(),
        symbols={
            'functions': [
                {'name': 'func1', 'start_line': 1, 'end_line': 2},
                {'name': 'func2', 'start_line': 3, 'end_line': 4}
            ]
        }
    )
    builder._process_functions(context)
    graph = builder.get_relationships()
    # Verify function nodes were created
    function_nodes = graph.get_nodes_by_type(NodeType.FUNCTION.value)
    assert len(function_nodes) == 2
    # Verify edges were created
    contains_edges = graph.get_edges(rel_type=RelationType.CONTAINS.value)
    assert len(contains_edges) == 2  # file -> func1, file -> func2
def test_process_references(builder):
    """Test processing code references."""
    # Create current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={'file_path': "test.py"}
    )
    # Add nodes that will be referenced
    builder.graph.add_node(
        name='test.py:main',
        type=NodeType.FUNCTION.value,
        file_path='test.py'
    )
    builder.graph.add_node(
        name='test.py:func1',
        type=NodeType.FUNCTION.value,
        file_path='test.py'
    )
    builder.graph.add_node(
        name='test.py:func2',
        type=NodeType.FUNCTION.value,
        file_path='test.py'
    )
    # Create references
    references = [
        {'type': 'call', 'name': 'func1', 'scope': 'main', 'start_line': 1, 'end_line': 1},
        {'type': 'call', 'name': 'func2', 'scope': 'main', 'start_line': 2, 'end_line': 2}
    ]
    # Process references
    builder._process_references(references)
def test_process_references_with_complex_scopes(builder):
    """Test processing references with complex scopes."""
    # Create current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={'file_path': "test.py"}
    )
    # Add nodes that will be referenced
    builder.graph.add_node(
        name='test.py:ClassA',
        type=NodeType.CLASS.value,
        file_path='test.py'
    )
    builder.graph.add_node(
        name='test.py:method1',
        type=NodeType.METHOD.value,
        file_path='test.py'
    )
    builder.graph.add_node(
        name='test.py:method2',
        type=NodeType.METHOD.value,
        file_path='test.py'
    )
    builder.graph.add_node(
        name='test.py:ClassB',
        type=NodeType.CLASS.value,
        file_path='test.py'
    )
    builder.graph.add_node(
        name='test.py:method3',
        type=NodeType.METHOD.value,
        file_path='test.py'
    )
    # Create references
    references = [
        {
            'type': 'call',
            'name': 'method1',
            'scope': 'ClassA.method2',
            'start_line': 1,
            'end_line': 1
        },
        {
            'type': 'call',
            'name': 'method2',
            'scope': 'ClassB.method3',
            'start_line': 2,
            'end_line': 2
        },
        {
            'type': 'attribute',
            'name': 'attr1',
            'scope': 'ClassA.method1',
            'start_line': 3,
            'end_line': 3
        }
    ]
    # Process references
    builder._process_references(references)
def test_process_references_with_external_symbols(builder):
    """Test processing references to external symbols."""
    # Create current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={'file_path': "test.py"}
    )
    # Add local function node
    builder.graph.add_node(
        name='test.py:local_func',
        type=NodeType.FUNCTION.value,
        file_path='test.py'
    )
    # Create references
    references = [
        {
            'type': 'call',
            'name': 'external_func',
            'scope': 'local_func',
            'start_line': 1,
            'end_line': 1
        }
    ]
    # Process references
    builder._process_references(references)
def test_process_references_error_handling(builder):
    """Test error handling in _process_references."""
    # Create current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={'file_path': "test.py"}
    )
    # Create references with invalid data
    references = [
        {'type': 'call'},  # Missing name
        {'name': 'test'},  # Missing type
        None,  # Invalid data
    ]
    # Process references
    builder._process_references(references)  # Should not raise exception
def test_process_imports_error_handling(builder):
    """Test error handling in _process_imports."""
    # Create current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={'file_path': "test.py"}
    )
    # Test with invalid import data
    invalid_imports = [
        {'type': 'import'},  # Missing module
        {'module': 'os'},  # Missing type
        None,  # Invalid data
    ]
    # Process imports
    builder._process_imports(invalid_imports)  # Should not raise exception
def test_ignored_names():
    """Test that ignored names are properly defined."""
    assert 'self' in IGNORED_NAMES
    assert 'cls' in IGNORED_NAMES
    assert len(IGNORED_NAMES) == 2  # Only these two names should be ignored
def test_file_context_initialization():
    """Test FileContext initialization."""
    context = FileContext(
        path="test.py",
        code="test code",
        tree=Mock()
    )
    # Verify default dictionaries are empty
    assert context.symbols['imports'] == []
    assert context.symbols['functions'] == []
    assert context.symbols['classes'] == []
    assert context.symbols['variables'] == []
    assert context.references['imports'] == []
    assert context.references['calls'] == []
    assert context.references['attributes'] == []
    assert context.references['variables'] == []
def test_process_classes_with_inheritance(builder):
    """Test processing class definitions with inheritance."""
    # Create a current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={
            'file_path': "test.py"
        }
    )
    context = FileContext(
        path="test.py",
        code="",
        tree=Mock(),
        symbols={
            'classes': [
                {
                    'name': 'BaseClass',
                    'start_line': 1,
                    'end_line': 5,
                    'methods': [
                        {'name': 'base_method', 'start_line': 2, 'end_line': 3}
                    ]
                },
                {
                    'name': 'DerivedClass',
                    'start_line': 7,
                    'end_line': 12,
                    'bases': ['BaseClass'],
                    'methods': [
                        {'name': 'derived_method', 'start_line': 8, 'end_line': 9}
                    ]
                }
            ]
        }
    )
    builder._process_classes(context)
    graph = builder.get_relationships()
    # Verify class nodes were created
    class_nodes = graph.get_nodes_by_type(NodeType.CLASS.value)
    assert len(class_nodes) == 2
    # Verify method nodes were created
    method_nodes = graph.get_nodes_by_type(NodeType.METHOD.value)
    assert len(method_nodes) == 2
    # Verify edges were created
    contains_edges = graph.get_edges(rel_type=RelationType.CONTAINS.value)
    assert len(contains_edges) == 4  # file -> base, file -> derived, base -> method, derived -> method
    inherits_edges = graph.get_edges(rel_type=RelationType.INHERITS.value)
    assert len(inherits_edges) == 1  # derived -> base
def test_process_classes_with_multiple_inheritance(builder):
    """Test processing class definitions with multiple inheritance."""
    # Create a current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={
            'file_path': "test.py"
        }
    )
    context = FileContext(
        path="test.py",
        code="",
        tree=Mock(),
        symbols={
            'classes': [
                {
                    'name': 'ClassA',
                    'start_line': 1,
                    'end_line': 5,
                    'methods': [
                        {'name': 'method_a', 'start_line': 2, 'end_line': 3}
                    ]
                },
                {
                    'name': 'ClassB',
                    'start_line': 7,
                    'end_line': 11,
                    'methods': [
                        {'name': 'method_b', 'start_line': 8, 'end_line': 9}
                    ]
                },
                {
                    'name': 'ClassC',
                    'start_line': 13,
                    'end_line': 18,
                    'bases': ['ClassA', 'ClassB'],
                    'methods': [
                        {'name': 'method_c', 'start_line': 14, 'end_line': 15}
                    ]
                }
            ]
        }
    )
    builder._process_classes(context)
    graph = builder.get_relationships()
    # Verify class nodes were created
    class_nodes = graph.get_nodes_by_type(NodeType.CLASS.value)
    assert len(class_nodes) == 3
    # Verify method nodes were created
    method_nodes = graph.get_nodes_by_type(NodeType.METHOD.value)
    assert len(method_nodes) == 3
    # Verify edges were created
    contains_edges = graph.get_edges(rel_type=RelationType.CONTAINS.value)
    assert len(contains_edges) == 6  # file -> A,B,C + each class -> its method
    inherits_edges = graph.get_edges(rel_type=RelationType.INHERITS.value)
    assert len(inherits_edges) == 2  # C -> A, C -> B
def test_process_functions_with_parameters(builder):
    """Test processing function definitions with parameters."""
    # Create a current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={
            'file_path': "test.py"
        }
    )
    context = FileContext(
        path="test.py",
        code="",
        tree=Mock(),
        symbols={
            'functions': [
                {
                    'name': 'complex_function',
                    'start_line': 1,
                    'end_line': 10,
                    'parameters': [
                        {'name': 'param1', 'start_line': 1, 'end_line': 1},
                        {'name': 'param2: str', 'start_line': 1, 'end_line': 1},
                        {'name': 'param3: int = 42', 'start_line': 1, 'end_line': 1}
                    ]
                }
            ]
        }
    )
    builder._process_functions(context)
    graph = builder.get_relationships()
    # Verify function node was created
    function_nodes = graph.get_nodes_by_type(NodeType.FUNCTION.value)
    assert len(function_nodes) == 1
    # Verify parameter nodes were created
    parameter_nodes = graph.get_nodes_by_type(NodeType.PARAMETER.value)
    assert len(parameter_nodes) == 3
    # Verify edges were created
    contains_edges = graph.get_edges(rel_type=RelationType.CONTAINS.value)
    assert len(contains_edges) == 4  # file -> function + function -> param1,2,3
def test_cross_file_references(tmp_path, builder):
    """Test handling of references across multiple files."""
    # Create test files
    file1 = tmp_path / "file1.py"
    file1.write_text("""
def func1():
    return "Hello"
class ClassA:
    def method1(self):
        return func1()
    """)
    file2 = tmp_path / "file2.py"
    file2.write_text("""
from file1 import func1, ClassA
def func2():
    return func1()
class ClassB(ClassA):
    def method2(self):
        return self.method1()
    """)
    # Analyze both files
    builder.analyze_file(str(file1))
    builder.analyze_file(str(file2))
    graph = builder.get_relationships()
    # Verify cross-file relationships
    edges = graph.get_edges(rel_type=RelationType.IMPORTS.value)
    assert len(edges) > 0  # file2 imports from file1
    edges = graph.get_edges(rel_type=RelationType.INHERITS.value)
    assert len(edges) > 0  # ClassB inherits from ClassA
    edges = graph.get_edges(rel_type=RelationType.CALLS.value)
    assert len(edges) > 0  # func2 calls func1, method2 calls method1
def test_relationship_builder_with_empty_file(builder):
    """Test relationship builder with an empty file."""
    context = FileContext(
        path="empty.py",
        code="",
        tree=Mock(),
        symbols={
            'imports': [],
            'functions': [],
            'classes': [],
            'variables': []
        },
        references={
            'calls': [],
            'attributes': [],
            'variables': []
        }
    )
    # Process empty context
    builder._process_imports(context.symbols['imports'])
    builder._process_classes(context)
    builder._process_functions(context)
    builder._process_references(context)
    graph = builder.get_relationships()
    assert len(graph.nodes) == 0
    assert len(graph.edges) == 0
def test_analyze_directory_error_handling(tmp_path):
    """Test error handling in analyze_directory."""
    builder = RelationshipBuilder()
    builder.parser = MockParser()
    # Test non-existent directory
    with pytest.raises(FileNotFoundError):
        builder.analyze_directory(str(tmp_path / "nonexistent"))
    # Test file instead of directory
    file_path = tmp_path / "test.py"
    file_path.write_text("def test(): pass")
    builder.analyze_directory(str(file_path))  # Should not raise, just skip non-directories
def test_process_classes_error_handling(builder):
    """Test error handling in _process_classes."""
    # Create a FileContext with invalid class data
    context = FileContext(
        path="test.py",
        code="",
        tree=Mock(),
        symbols={
            'classes': [
                {'type': 'class'},  # Missing name
                {'name': 'Test'},  # Missing type
                None,  # Invalid data
            ]
        }
    )
    builder._process_classes(context)  # Should not raise exception
def test_process_inheritance_error_handling(builder):
    """Test error handling in _process_inheritance."""
    # Create current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={'file_path': "test.py"}
    )
    # Create a FileContext with invalid class data
    context = FileContext(
        path="test.py",
        code="",
        tree=Mock(),
        symbols={
            'classes': [
                {'type': 'class', 'name': 'Test', 'bases': [None]},  # Invalid base
                {'type': 'class', 'name': 'Test2', 'bases': ["NonExistentClass"]},  # Base class not found
            ]
        }
    )
    builder._process_inheritance(context)  # Should not raise exception
def test_process_functions_error_handling(builder):
    """Test error handling in _process_functions."""
    # Create a FileContext with invalid function data
    context = FileContext(
        path="test.py",
        code="",
        tree=Mock(),
        symbols={
            'functions': [
                {'type': 'function'},  # Missing name
                {'name': 'test'},  # Missing type
                None,  # Invalid data
            ]
        }
    )
    builder._process_functions(context)  # Should not raise exception
def test_process_references_error_handling(builder):
    """Test error handling in _process_references."""
    # Create current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={'file_path': "test.py"}
    )
    # Create a FileContext with invalid reference data
    context = FileContext(
        path="test.py",
        code="",
        tree=Mock(),
        references=[
            {'type': 'call'},  # Missing name
            {'name': 'test'},  # Missing type
            None,  # Invalid data
        ]
    )
    builder._process_references(context)  # Should not raise exception
def test_process_imports_error_handling(builder):
    """Test error handling in _process_imports."""
    # Create current file node
    builder.current_file_node = builder.graph.find_or_create_node(
        name="test.py",
        type=NodeType.MODULE,
        properties={'file_path': "test.py"}
    )
    # Test with invalid import data
    invalid_imports = [
        {'type': 'import'},  # Missing module
        {'module': 'os'},  # Missing type
        None,  # Invalid data
    ]
    builder._process_imports(invalid_imports)  # Should not raise exception
</file>

<file path="tests/test_semantic_analysis.py">
'''python
import pytest
from code_understanding.semantic_analysis import perform_type_inference, build_cfg
def test_perform_type_inference():
    dummy_code = 'x = 1'
    result = perform_type_inference(dummy_code)
    assert isinstance(result, dict)
    assert 'types' in result
def test_build_cfg():
    dummy_code = 'if True:\n    pass'
    result = build_cfg(dummy_code)
    assert isinstance(result, dict)
    assert 'cfg' in result
'''
</file>

<file path="tests/test_semantic_analyzer.py">
"""Tests for the JavaScript semantic analyzer."""
import unittest
import tempfile
import os
from pathlib import Path
from server.code_understanding.semantic_analyzer import SemanticAnalyzer
class TestSemanticAnalyzer(unittest.TestCase):
    """Test cases for the SemanticAnalyzer class."""
    def setUp(self):
        """Set up test environment."""
        self.analyzer = SemanticAnalyzer()
    def test_basic_type_inference(self):
        """Test basic type inference for literals and variables."""
        code = """
        const number = 42;
        const string = "hello";
        const boolean = true;
        const array = [1, 2, 3];
        const object = { key: "value" };
        """
        result = self.analyzer.analyze_file('test.js', code)
        # Check types
        self.assertEqual(str(result['types']['number']), 'number')
        self.assertEqual(str(result['types']['string']), 'string')
        self.assertEqual(str(result['types']['boolean']), 'boolean')
        self.assertEqual(str(result['types']['array']), 'number[]')
        self.assertEqual(str(result['types']['object']), 'object')
    def test_function_type_inference(self):
        """Test type inference for functions."""
        code = """
        function add(a, b) {
            return a + b;
        }
        const multiply = (x, y) => x * y;
        class Calculator {
            divide(a, b) {
                return a / b;
            }
        }
        """
        result = self.analyzer.analyze_file('test.js', code)
        # Check function types
        self.assertEqual(str(result['types']['add']), 'function')
        self.assertEqual(str(result['types']['multiply']), 'function')
        self.assertEqual(str(result['types']['Calculator']), 'Calculator')
        # Check function contexts
        self.assertEqual(result['contexts']['add']['type'], 'function')
        self.assertEqual(len(result['contexts']['add']['parameters']), 2)
        # Check class method context
        self.assertEqual(result['contexts']['Calculator']['type'], 'class')
        self.assertIn('divide', result['contexts']['Calculator']['methods'])
    def test_class_analysis(self):
        """Test analysis of classes and their members."""
        code = """
        class Person {
            constructor(name, age) {
                this.name = name;
                this.age = age;
            }
            getName() {
                return this.name;
            }
            getAge() {
                return this.age;
            }
        }
        const person = new Person("John", 30);
        """
        result = self.analyzer.analyze_file('test.js', code)
        # Check class type
        self.assertEqual(str(result['types']['Person']), 'Person')
        self.assertEqual(str(result['types']['person']), 'Person')
        # Check class context
        person_context = result['contexts']['Person']
        self.assertEqual(person_context['type'], 'class')
        self.assertIn('constructor', person_context['methods'])
        self.assertIn('getName', person_context['methods'])
        self.assertIn('getAge', person_context['methods'])
    def test_scope_analysis(self):
        """Test scope analysis and variable lookup."""
        code = """
        const global = "global";
        function outer() {
            const outer_var = "outer";
            function inner() {
                const inner_var = "inner";
                console.log(global, outer_var, inner_var);
            }
            inner();
        }
        outer();
        """
        result = self.analyzer.analyze_file('test.js', code)
        # Check variable types
        self.assertEqual(str(result['types']['global']), 'string')
        self.assertEqual(str(result['types']['outer']), 'function')
        # Check function contexts
        outer_context = result['contexts']['outer']
        self.assertEqual(outer_context['type'], 'function')
        self.assertIn('outer_var', outer_context['scope']['variables'])
    def test_array_type_inference(self):
        """Test type inference for arrays and array operations."""
        code = """
        const numbers = [1, 2, 3];
        const strings = ["a", "b", "c"];
        const mixed = [1, "two", true];
        function processArray(arr) {
            return arr.map(x => x);
        }
        const result = processArray(numbers);
        """
        result = self.analyzer.analyze_file('test.js', code)
        # Check array types
        self.assertEqual(str(result['types']['numbers']), 'number[]')
        self.assertEqual(str(result['types']['strings']), 'string[]')
        self.assertEqual(str(result['types']['mixed']), 'any[]')
        self.assertEqual(str(result['types']['result']), 'number[]')
    def test_object_type_inference(self):
        """Test type inference for objects and object operations."""
        code = """
        const person = {
            name: "John",
            age: 30,
            address: {
                street: "123 Main St",
                city: "New York"
            }
        };
        function updatePerson(p) {
            p.age += 1;
            return p;
        }
        const updated = updatePerson(person);
        """
        result = self.analyzer.analyze_file('test.js', code)
        # Check object types
        self.assertEqual(str(result['types']['person']), 'object')
        self.assertEqual(str(result['types']['updated']), 'object')
        # Check function context
        update_context = result['contexts']['updatePerson']
        self.assertEqual(update_context['type'], 'function')
        self.assertEqual(len(update_context['parameters']), 1)
    def test_error_handling(self):
        """Test handling of invalid code."""
        # Test invalid JavaScript
        invalid_code = "invalid javascript code"
        result = self.analyzer.analyze_file('test.js', invalid_code)
        self.assertIn('error', result)
        self.assertEqual(result['types'], {})
        self.assertEqual(result['contexts'], {})
        # Test empty file
        empty_code = ""
        result = self.analyzer.analyze_file('test.js', empty_code)
        self.assertEqual(result['types'], {})
        self.assertEqual(result['contexts'], {})
    def test_builtin_types(self):
        """Test handling of built-in JavaScript types."""
        code = """
        const date = new Date();
        const regex = /test/;
        const promise = new Promise((resolve) => resolve());
        const map = new Map();
        const set = new Set();
        """
        result = self.analyzer.analyze_file('test.js', code)
        # Check built-in type inference
        self.assertEqual(str(result['types']['date']), 'Date')
        self.assertEqual(str(result['types']['regex']), 'RegExp')
        self.assertEqual(str(result['types']['promise']), 'Promise')
        self.assertEqual(str(result['types']['map']), 'Map')
        self.assertEqual(str(result['types']['set']), 'Set')
    def test_type_inheritance(self):
        """Test type inheritance and class relationships."""
        code = """
        class Animal {
            constructor(name) {
                this.name = name;
            }
            speak() {
                return "Some sound";
            }
        }
        class Dog extends Animal {
            constructor(name, breed) {
                super(name);
                this.breed = breed;
            }
            speak() {
                return "Woof!";
            }
        }
        const dog = new Dog("Rex", "German Shepherd");
        """
        result = self.analyzer.analyze_file('test.js', code)
        # Check class types
        self.assertEqual(str(result['types']['Animal']), 'Animal')
        self.assertEqual(str(result['types']['Dog']), 'Dog')
        self.assertEqual(str(result['types']['dog']), 'Dog')
        # Check class contexts
        dog_context = result['contexts']['Dog']
        self.assertEqual(dog_context['type'], 'class')
        self.assertIn('speak', dog_context['methods'])
        self.assertIn('breed', dog_context['properties'])
if __name__ == '__main__':
    unittest.main()
</file>

<file path="tests/test_system_utilities.py">
"""Tests for system utilities and miscellaneous features."""
import os
import sys
import pytest
import platform
import re
import time
from unittest.mock import patch, MagicMock
# Import the server module
sys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))
import server
def test_system_info():
    """Test retrieving system information."""
    result = server.system_info()
    # Verify the result contains expected fields
    assert "platform" in result
    assert "python_version" in result
    assert "system" in result
    assert "machine" in result
    assert "processor" in result
    assert "cpu_count" in result
    assert "hostname" in result
    # Verify some specific values
    assert result["python_version"] == platform.python_version()
    assert result["system"] == platform.system()
def test_calculate():
    """Test the calculate utility function."""
    # Test basic arithmetic
    result = server.calculate("2 + 3 * 4")
    assert result["result"] == 14
    # Test more complex expressions
    result = server.calculate("sqrt(16) + pow(2, 3)")
    assert result["result"] == 12.0
    # Test invalid expression
    result = server.calculate("invalid expression")
    assert "error" in result
    # Test potentially dangerous expressions
    result = server.calculate("__import__('os').system('echo hack')")
    assert "error" in result
@patch('builtins.open', new_callable=MagicMock)
def test_edit_block(mock_open):
    """Test the edit_block functionality."""
    # Setup mock
    mock_file_handle = MagicMock()
    mock_open.return_value.__enter__.return_value = mock_file_handle
    # Test edit block functionality
    edit_block = """
    @@ test_file.py
    # This is a test file
    def test_function():
        return "Hello World"
    """
    result = server.edit_block(edit_block)
    # Verify the function tried to write to the file
    assert result["success"] is True
    mock_open.assert_called_once()
    assert "test_file.py" in mock_open.call_args[0][0]
    mock_file_handle.write.assert_called()
def test_list_processes():
    """Test listing system processes."""
    result = server.list_processes()
    # Verify result structure
    assert "processes" in result
    assert isinstance(result["processes"], list)
    assert len(result["processes"]) > 0
    # Check first process has expected fields
    first_process = result["processes"][0]
    assert "pid" in first_process
    assert "name" in first_process
    assert "username" in first_process
@pytest.mark.skipif(sys.platform == "win32", reason="kill_process behaves differently on Windows")
def test_kill_process():
    """Test killing a process by PID."""
    # Start a process
    if sys.platform == "win32":
        cmd = "ping -n 30 localhost"
    else:
        cmd = "sleep 30"
    result = server.execute_command(cmd, timeout=1, allow_background=True)
    assert result["pid"] is not None
    pid = result["pid"]
    # Kill the process with SIGTERM
    kill_result = server.kill_process(pid, signal_type="TERM")
    # Verify the result
    assert kill_result["success"] is True
    # Check process is no longer running
    time.sleep(0.5)  # Give some time for the process to terminate
    try:
        os.kill(pid, 0)  # This will raise an error if the process is gone
        process_still_running = True
    except OSError:
        process_still_running = False
    assert not process_still_running
@pytest.mark.skipif(not hasattr(server, 'list_sessions'), reason="list_sessions function not available")
def test_list_sessions_empty():
    """Test listing active command sessions when none are running."""
    # Clear any existing sessions
    with server.session_lock:
        server.active_sessions.clear()
    # Get session list
    sessions_result = server.list_sessions()
    # Verify result
    assert "sessions" in sessions_result
    assert len(sessions_result["sessions"]) == 0
def test_process_output_streaming():
    """Test that process output is properly streamed through queues."""
    # Create a command that produces output over time
    if sys.platform == "win32":
        cmd = "for /L %i in (1,1,5) do @(echo Line %i & timeout /t 1 > nul)"
    else:
        cmd = "for i in $(seq 1 5); do echo Line $i; sleep 0.5; done"
    # Execute command
    result = server.execute_command(cmd, timeout=1, allow_background=True)
    assert result["pid"] is not None
    pid = result["pid"]
    # Read output multiple times to verify streaming
    outputs = []
    for _ in range(5):
        time.sleep(1)
        output_result = server.read_output(pid)
        if output_result["stdout"]:
            outputs.append(output_result["stdout"])
    # Cleanup
    server.force_terminate(pid)
    # Verify we got multiple different outputs
    assert len(outputs) > 0
    # At least some outputs should be different as the command generates output over time
    assert len(set(outputs)) > 1
</file>

<file path=".gitignore">
# Python-generated files
__pycache__/
*.py[oc]
build/
dist/
wheels/
*.egg-info

# Virtual environments
.venv
</file>

<file path=".python-version">
3.13
</file>

<file path="build_grammars.py">
#!/usr/bin/env python3
import os
import subprocess
import sys
from pathlib import Path
def build_grammars():
    """Build tree-sitter grammars for supported languages."""
    # Create build directory if it doesn't exist
    build_dir = Path('build')
    build_dir.mkdir(exist_ok=True)
    # Clone tree-sitter-javascript if not exists
    js_grammar_dir = Path('vendor/tree-sitter-javascript')
    if not js_grammar_dir.exists():
        subprocess.run([
            'git', 'clone',
            'https://github.com/tree-sitter/tree-sitter-javascript.git',
            str(js_grammar_dir)
        ], check=True)
    # Build JavaScript grammar
    print("Building JavaScript grammar...")
    subprocess.run([
        'tree-sitter', 'generate',
        str(js_grammar_dir / 'grammar.js'),
        '--out-dir', str(build_dir)
    ], check=True)
    # Build shared library
    print("Building shared library...")
    if sys.platform == 'win32':
        subprocess.run([
            'gcc', '-shared', '-o', str(build_dir / 'my-languages.dll'),
            '-I', str(js_grammar_dir / 'src'),
            str(build_dir / 'javascript.c'),
            '-lstdc++', '-fPIC'
        ], check=True)
    else:
        subprocess.run([
            'gcc', '-shared', '-o', str(build_dir / 'my-languages.so'),
            '-I', str(js_grammar_dir / 'src'),
            str(build_dir / 'javascript.c'),
            '-lstdc++', '-fPIC'
        ], check=True)
    print("Grammar build complete!")
if __name__ == '__main__':
    build_grammars()
</file>

<file path="build.py">
import os
import subprocess
def build_language():
    """Build the tree-sitter language bindings for Python."""
    # Get the current directory
    current_dir = os.path.dirname(os.path.abspath(__file__))
    # Create the build directory if it doesn't exist
    build_dir = os.path.join(current_dir, 'build')
    os.makedirs(build_dir, exist_ok=True)
    # Change to the tree-sitter-python directory
    python_grammar_dir = os.path.join(current_dir, 'vendor', 'tree-sitter-python')
    os.chdir(python_grammar_dir)
    # Run tree-sitter generate
    subprocess.run(['tree-sitter', 'generate'], check=True)
    # Run tree-sitter test
    subprocess.run(['tree-sitter', 'test'], check=True)
    # Build the shared library
    subprocess.run(['cc', '-fPIC', '-c', 'src/parser.c', '-I', 'src'], check=True)
    subprocess.run(['cc', '-fPIC', '-c', 'src/scanner.c', '-I', 'src'], check=True)
    subprocess.run(['cc', '-shared', '-o', os.path.join(current_dir, 'server', 'code_understanding', 'python.so'), 'parser.o', 'scanner.o'], check=True)
    print("Successfully built tree-sitter Python language bindings")
if __name__ == '__main__':
    build_language()
</file>

<file path="cli.py">
#!/usr/bin/env python3
import click
import json
import requests
import sys
import os
from typing import Dict, Any
from rich.console import Console
from rich.table import Table
from rich.syntax import Syntax
from rich.panel import Panel
console = Console()
class MCPClient:
    def __init__(self, host: str = "http://localhost", port: int = 7443):
        self.base_url = f"{host}:{port}"
    def call_tool(self, tool_name: str, **params) -> Dict[str, Any]:
        """Call an MCP tool with parameters"""
        try:
            response = requests.post(
                f"{self.base_url}/tools/{tool_name}",
                json=params,
                timeout=30
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            console.print(f"[red]Error calling tool {tool_name}: {str(e)}[/red]")
            sys.exit(1)
@click.group()
def cli():
    """Terminal Command Runner MCP CLI"""
    pass
@cli.group()
def command():
    """Command execution and management"""
    pass
@command.command()
@click.argument('cmd')
@click.option('--timeout', '-t', default=10, help='Command timeout in seconds')
@click.option('--background/--no-background', default=True, help='Allow running in background')
def execute(cmd: str, timeout: int, background: bool):
    """Execute a command"""
    client = MCPClient()
    result = client.call_tool(
        'execute_command',
        command=cmd,
        timeout=timeout,
        allow_background=background
    )
    if result.get('error'):
        console.print(f"[red]Error: {result['error']}[/red]")
        return
    console.print(Panel.fit(
        f"[green]Command executed[/green]\n"
        f"PID: {result.get('pid')}\n"
        f"Exit code: {result.get('exit_code')}\n"
        f"Runtime: {result.get('runtime')}s"
    ))
    if result.get('stdout'):
        console.print("\n[bold]Output:[/bold]")
        console.print(result['stdout'])
    if result.get('stderr'):
        console.print("\n[bold red]Errors:[/bold red]")
        console.print(result['stderr'])
@command.command()
def list():
    """List active command sessions"""
    client = MCPClient()
    result = client.call_tool('list_sessions')
    if not result.get('sessions'):
        console.print("[yellow]No active sessions[/yellow]")
        return
    table = Table(title="Active Sessions")
    table.add_column("PID", justify="right")
    table.add_column("Command")
    table.add_column("Start Time")
    for session in result['sessions']:
        table.add_row(
            str(session['pid']),
            session['command'],
            session['start_time']
        )
    console.print(table)
@cli.group()
def file():
    """File operations"""
    pass
@file.command()
@click.argument('path')
def read(path: str):
    """Read file contents"""
    client = MCPClient()
    result = client.call_tool('read_file', path=path)
    if result.get('error'):
        console.print(f"[red]Error: {result['error']}[/red]")
        return
    syntax = Syntax(result['content'], "python", theme="monokai")
    console.print(syntax)
@file.command()
@click.argument('path')
@click.argument('content')
def write(path: str, content: str):
    """Write content to a file"""
    client = MCPClient()
    result = client.call_tool('write_file', path=path, content=content)
    if result.get('error'):
        console.print(f"[red]Error: {result['error']}[/red]")
        return
    console.print(f"[green]Successfully wrote to {path}[/green]")
@cli.group()
def system():
    """System operations"""
    pass
@system.command()
def info():
    """Get system information"""
    client = MCPClient()
    result = client.call_tool('system_info')
    if result.get('error'):
        console.print(f"[red]Error: {result['error']}[/red]")
        return
    console.print(Panel.fit(
        "\n".join([
            f"[bold]Platform:[/bold] {result.get('platform', 'Unknown')}",
            f"[bold]Python Version:[/bold] {result.get('python_version', 'Unknown')}",
            f"[bold]CPU Count:[/bold] {result.get('cpu_count', 'Unknown')}",
            f"[bold]Hostname:[/bold] {result.get('hostname', 'Unknown')}"
        ]),
        title="System Information"
    ))
@cli.group()
def dev():
    """Development tools"""
    pass
@dev.command()
@click.argument('path', default=".")
def analyze(path: str):
    """Analyze codebase"""
    client = MCPClient()
    result = client.call_tool('analyze_codebase', path=path)
    if result.get('error'):
        console.print(f"[red]Error: {result['error']}[/red]")
        return
    results = result.get('results', {})
    # Show complexity hotspots
    if results.get('complexity_hotspots'):
        table = Table(title="Complexity Hotspots")
        table.add_column("File")
        table.add_column("Score", justify="right")
        for hotspot in results['complexity_hotspots']:
            table.add_row(
                hotspot['file'],
                str(hotspot['complexity']['score'])
            )
        console.print(table)
    # Show security issues
    if results.get('security_issues'):
        table = Table(title="Security Issues")
        table.add_column("File")
        table.add_column("Line", justify="right")
        table.add_column("Type")
        table.add_column("Severity")
        for issue in results['security_issues']:
            table.add_row(
                issue['file'],
                str(issue['line']),
                issue['type'],
                issue['severity']
            )
        console.print(table)
@dev.command()
@click.argument('path', default=".")
@click.option('--fix/--no-fix', default=False, help='Automatically fix issues')
def lint(path: str, fix: bool):
    """Run code linting"""
    client = MCPClient()
    result = client.call_tool('lint_code', path=path, fix=fix)
    if result.get('error'):
        console.print(f"[red]Error: {result['error']}[/red]")
        return
    if not result.get('issues'):
        console.print("[green]No issues found![/green]")
        return
    table = Table(title="Linting Issues")
    table.add_column("File")
    table.add_column("Line", justify="right")
    table.add_column("Message")
    for issue in result['issues']:
        table.add_row(
            issue['file'],
            str(issue['line']),
            issue['message']
        )
    console.print(table)
if __name__ == '__main__':
    cli()
</file>

<file path="cursor-tools.config.json">
{
  "plan": {
    "fileProvider": "gemini",
    "thinkingProvider": "perplexity",
    "thinkingModel": "r1-1776"
  },
  "web": {
    "provider": "gemini",
    "model": "gemini-2.5-pro-exp"
  }
}
</file>

<file path="debug_js_ast.py">
import os
import sys
import json
import logging
# Add the project root to Python path
sys.path.append(os.path.abspath('.'))
from server.code_understanding.analyzer import CodeAnalyzer
# Set up logging to see detailed debug information
logging.basicConfig(level=logging.DEBUG, 
                   format='%(levelname)s:%(name)s:%(message)s')
def print_node_structure(node, indent=0):
    """Recursively print the structure of a node."""
    if not node:
        return
    indent_str = '  ' * indent
    node_type = getattr(node, 'type', 'Unknown')
    node_text = getattr(node, 'text', '')[:50]  # Truncate text for readability
    print(f"{indent_str}Node: {node_type}")
    print(f"{indent_str}Text: {node_text}")
    if hasattr(node, 'fields') and node.fields:
        print(f"{indent_str}Fields: {node.fields}")
    if hasattr(node, 'children'):
        for i, child in enumerate(node.children):
            print(f"{indent_str}Child {i}:")
            print_node_structure(child, indent + 1)
# Create a simple JavaScript code with different statements
js_code = """
const fs = require('fs');
import path from 'path';
import { readFileSync } from 'fs';
function greet(name) {
  return `Hello, ${name}!`;
}
const farewell = (name) => {
  return `Goodbye, ${name}!`;
};
class MyClass {
  constructor(value) {
    this.value = value;
  }
  getValue() {
    return this.value;
  }
}
const instance = new MyClass(123);
let myVar = 'test';
const myConst = 456;
"""
# Parse the code to get the AST tree directly
from server.code_understanding.parser import CodeParser
parser = CodeParser()
tree = parser.parse(js_code, language='javascript')
print("\n--- AST Structure ---")
print_node_structure(tree.root_node)
# Analyze the code with our analyzer
print("\n--- Running Analysis ---")
analyzer = CodeAnalyzer()
result = analyzer.analyze_code(js_code, language='javascript')
print("\n--- Analysis Result ---")
print(f"Imports: {json.dumps(result.get('imports', []), indent=2)}")
print(f"Variables: {json.dumps(result.get('variables', []), indent=2)}")
print(f"Functions: {json.dumps(result.get('functions', []), indent=2)}")
print(f"Classes: {json.dumps(result.get('classes', []), indent=2)}")
</file>

<file path="debugger.py">
#!/usr/bin/env python3
import cmd
import inspect
import json
import os
import pdb
import sys
import threading
import time
from typing import Any, Dict, List, Optional
from rich.console import Console
from rich.syntax import Syntax
from rich.table import Table
from rich.panel import Panel
console = Console()
class MCPDebugger(cmd.Cmd):
    """Interactive debugger for MCP tools"""
    intro = 'Welcome to MCP Debugger. Type help or ? to list commands.'
    prompt = '(mcp-debug) '
    def __init__(self, tool_registry: Dict[str, Any]):
        """Initialize the debugger with tool registry"""
        super().__init__()
        self.tool_registry = tool_registry
        self.current_tool = None
        self.breakpoints = {}
        self.watch_vars = {}
        self.history = []
        self.step_mode = False
    def do_list_tools(self, arg):
        """List all available tools"""
        table = Table(title="Available Tools")
        table.add_column("Tool Name")
        table.add_column("Description")
        for name, tool in self.tool_registry.items():
            table.add_row(name, tool.get('description', 'No description'))
        console.print(table)
    def do_inspect(self, arg):
        """Inspect a specific tool's implementation"""
        if not arg:
            console.print("[red]Please specify a tool name[/red]")
            return
        tool = self.tool_registry.get(arg)
        if not tool:
            console.print(f"[red]Tool '{arg}' not found[/red]")
            return
        func = tool.get('function')
        if not func:
            console.print("[red]Tool implementation not found[/red]")
            return
        source = inspect.getsource(func)
        syntax = Syntax(source, "python", theme="monokai")
        console.print(syntax)
    def do_break(self, arg):
        """Set a breakpoint in a tool"""
        if not arg:
            console.print("[red]Please specify: tool_name:line_number[/red]")
            return
        try:
            tool_name, line = arg.split(':')
            line = int(line)
        except ValueError:
            console.print("[red]Invalid format. Use: tool_name:line_number[/red]")
            return
        tool = self.tool_registry.get(tool_name)
        if not tool:
            console.print(f"[red]Tool '{tool_name}' not found[/red]")
            return
        if tool_name not in self.breakpoints:
            self.breakpoints[tool_name] = set()
        self.breakpoints[tool_name].add(line)
        console.print(f"[green]Breakpoint set in {tool_name} at line {line}[/green]")
    def do_watch(self, arg):
        """Watch a variable in a tool"""
        if not arg:
            console.print("[red]Please specify: tool_name:variable_name[/red]")
            return
        try:
            tool_name, var_name = arg.split(':')
        except ValueError:
            console.print("[red]Invalid format. Use: tool_name:variable_name[/red]")
            return
        tool = self.tool_registry.get(tool_name)
        if not tool:
            console.print(f"[red]Tool '{tool_name}' not found[/red]")
            return
        if tool_name not in self.watch_vars:
            self.watch_vars[tool_name] = set()
        self.watch_vars[tool_name].add(var_name)
        console.print(f"[green]Watching variable '{var_name}' in {tool_name}[/green]")
    def do_info(self, arg):
        """Show debugging information"""
        if not arg:
            self._show_general_info()
            return
        if arg == 'breakpoints':
            self._show_breakpoints()
        elif arg == 'watches':
            self._show_watches()
        elif arg == 'history':
            self._show_history()
        else:
            console.print(f"[red]Unknown info type: {arg}[/red]")
    def _show_general_info(self):
        """Show general debugging information"""
        info = Panel.fit(
            "\n".join([
                f"[bold]Active Tool:[/bold] {self.current_tool or 'None'}",
                f"[bold]Step Mode:[/bold] {'Enabled' if self.step_mode else 'Disabled'}",
                f"[bold]Breakpoints:[/bold] {sum(len(bp) for bp in self.breakpoints.values())}",
                f"[bold]Watches:[/bold] {sum(len(w) for w in self.watch_vars.values())}",
                f"[bold]History Entries:[/bold] {len(self.history)}"
            ]),
            title="Debugger Status"
        )
        console.print(info)
    def _show_breakpoints(self):
        """Show all breakpoints"""
        table = Table(title="Breakpoints")
        table.add_column("Tool")
        table.add_column("Line Numbers")
        for tool, lines in self.breakpoints.items():
            table.add_row(tool, ", ".join(str(line) for line in sorted(lines)))
        console.print(table)
    def _show_watches(self):
        """Show all watch variables"""
        table = Table(title="Watch Variables")
        table.add_column("Tool")
        table.add_column("Variables")
        for tool, vars in self.watch_vars.items():
            table.add_row(tool, ", ".join(sorted(vars)))
        console.print(table)
    def _show_history(self):
        """Show execution history"""
        table = Table(title="Execution History")
        table.add_column("Time")
        table.add_column("Tool")
        table.add_column("Event")
        table.add_column("Details")
        for entry in self.history[-10:]:  # Show last 10 entries
            table.add_row(
                entry['time'],
                entry['tool'],
                entry['event'],
                entry.get('details', '')
            )
        console.print(table)
    def do_step(self, arg):
        """Enable/disable step-by-step execution"""
        self.step_mode = not self.step_mode
        status = "enabled" if self.step_mode else "disabled"
        console.print(f"[green]Step-by-step execution {status}[/green]")
    def do_continue(self, arg):
        """Continue execution after a break"""
        if not self.current_tool:
            console.print("[yellow]No tool is currently being debugged[/yellow]")
            return
        self.step_mode = False
        console.print("[green]Continuing execution[/green]")
    def do_locals(self, arg):
        """Show local variables in current scope"""
        if not self.current_tool:
            console.print("[yellow]No tool is currently being debugged[/yellow]")
            return
        # This would be populated with actual local variables during debugging
        locals_dict = {}  # Placeholder
        table = Table(title="Local Variables")
        table.add_column("Name")
        table.add_column("Type")
        table.add_column("Value")
        for name, value in locals_dict.items():
            table.add_row(
                name,
                type(value).__name__,
                str(value)
            )
        console.print(table)
    def do_stack(self, arg):
        """Show the current call stack"""
        if not self.current_tool:
            console.print("[yellow]No tool is currently being debugged[/yellow]")
            return
        # This would be populated with actual stack frames during debugging
        frames = []  # Placeholder
        table = Table(title="Call Stack")
        table.add_column("Frame")
        table.add_column("Function")
        table.add_column("Line")
        table.add_column("File")
        for i, frame in enumerate(frames):
            table.add_row(
                str(i),
                frame.get('function', 'unknown'),
                str(frame.get('line', '?')),
                frame.get('file', 'unknown')
            )
        console.print(table)
    def do_quit(self, arg):
        """Exit the debugger"""
        return True
    def default(self, line):
        """Handle unknown commands"""
        console.print(f"[red]Unknown command: {line}[/red]")
        console.print("Type 'help' or '?' for a list of commands")
    def emptyline(self):
        """Handle empty lines"""
        pass
    def _log_event(self, tool: str, event: str, details: Optional[str] = None):
        """Log a debugging event"""
        self.history.append({
            'time': time.strftime('%H:%M:%S'),
            'tool': tool,
            'event': event,
            'details': details
        })
    def debug_tool(self, tool_name: str, *args, **kwargs):
        """Debug a tool execution"""
        tool = self.tool_registry.get(tool_name)
        if not tool:
            console.print(f"[red]Tool '{tool_name}' not found[/red]")
            return
        self.current_tool = tool_name
        self._log_event(tool_name, 'start', f"args: {args}, kwargs: {kwargs}")
        try:
            # This is where we'd integrate with the actual tool execution
            # For now, it's just a placeholder
            console.print(f"[green]Debugging tool: {tool_name}[/green]")
            self.cmdloop()
        finally:
            self._log_event(tool_name, 'end')
            self.current_tool = None
def create_debugger(tool_registry: Dict[str, Any]) -> MCPDebugger:
    """Create and return a debugger instance"""
    return MCPDebugger(tool_registry)
</file>

<file path="decorators.py">
import functools
import inspect
import sys
import threading
from typing import Any, Callable, Dict, Optional
from debugger import MCPDebugger
_debugger: Optional[MCPDebugger] = None
_debug_lock = threading.Lock()
def get_debugger() -> Optional[MCPDebugger]:
    """Get the global debugger instance"""
    return _debugger
def set_debugger(debugger: MCPDebugger):
    """Set the global debugger instance"""
    global _debugger
    with _debug_lock:
        _debugger = debugger
def debuggable(tool_name: str, description: str = ""):
    """Decorator to make a tool debuggable
    Args:
        tool_name: Name of the tool
        description: Optional description of the tool
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            debugger = get_debugger()
            if debugger and '--debug' in sys.argv:
                # Register tool if not already registered
                if tool_name not in debugger.tool_registry:
                    debugger.tool_registry[tool_name] = {
                        'function': func,
                        'description': description or func.__doc__
                    }
                # Start debugging session
                debugger.debug_tool(tool_name, *args, **kwargs)
            return func(*args, **kwargs)
        return wrapper
    return decorator
</file>

<file path="docker-compose.yml">
version: "3.8"
services:
  prometheus:
    image: prom/prometheus:v2.45.0
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
      - ./monitoring/prometheus-rules.yml:/etc/prometheus/prometheus-rules.yml
    ports:
      - "9090:9090"
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--web.console.libraries=/usr/share/prometheus/console_libraries"
      - "--web.console.templates=/usr/share/prometheus/consoles"
    restart: unless-stopped
  alertmanager:
    image: prom/alertmanager:v0.25.0
    volumes:
      - ./monitoring/alertmanager.yml:/etc/alertmanager/config.yml
    ports:
      - "9093:9093"
    command:
      - "--config.file=/etc/alertmanager/config.yml"
      - "--storage.path=/alertmanager"
    restart: unless-stopped
  grafana:
    image: grafana/grafana:10.2.0
    depends_on:
      - prometheus
    ports:
      - "3000:3000"
    volumes:
      - ./monitoring/grafana-dashboard.json:/etc/grafana/provisioning/dashboards/mcp-dashboard.json
      - ./monitoring/grafana-datasource.yml:/etc/grafana/provisioning/datasources/datasource.yml
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    restart: unless-stopped
  otel-collector:
    image: otel/opentelemetry-collector:0.88.0
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./monitoring/otel-collector-config.yaml:/etc/otel-collector-config.yaml
    ports:
      - "4318:4318" # OTLP HTTP receiver
      - "8888:8888" # Prometheus metrics exposed by the collector
      - "8889:8889" # Prometheus exporter metrics
    depends_on:
      - prometheus
    restart: unless-stopped
</file>

<file path="llm_server.md">
# LLM Tools MCP Server

This is a separate MCP server that provides LLM-related tools and functionality. It runs on port 7444 and can be used alongside the main MCP server.

## Features

### Code Generation
- Generate code using various LLM models (Claude, GPT, CodeLlama, StarCoder)
- Support for different programming languages
- Context-aware code generation
- System prompt customization

### LLM Context Management
- Track and optimize LLM context usage
- Token estimation and limits
- Content optimization suggestions
- Model-specific context handling

### Output Processing
- Filter and format command outputs for LLM consumption
- Pattern-based content filtering
- Smart line sampling
- Important information preservation

## Available Tools

### `generate_code`
Generate code using various LLM models.

Parameters:
- `prompt`: The code generation prompt
- `model`: LLM model to use (default: "claude-3-sonnet")
- `context`: Optional context information
- `system_prompt`: Optional custom system prompt

### `manage_llm_context`
Manage and optimize LLM context usage.

Parameters:
- `content`: Text content to analyze
- `model`: Target LLM model
- `max_tokens`: Maximum token limit

### `context_length`
Track LLM context usage.

Parameters:
- `text`: Text to analyze

### `filter_output`
Process and format command outputs.

Parameters:
- `content`: Output content to filter
- `max_lines`: Maximum number of lines to include
- `important_patterns`: List of regex patterns to always include

## Setup

1. Install dependencies:
```bash
uv pip install -e ".[llm]"
```

2. Start the server:
```bash
mcp run llm_server.py
```

## Configuration

The server can be configured through environment variables:

- `MCP_LOG_LEVEL`: Logging level (default: "DEBUG")
- `ENABLE_TELEMETRY`: Enable OpenTelemetry tracing (default: "0")

## Integration

The LLM server can be used alongside the main MCP server:

```python
from mcp.client import MCPClient

# Main server client
main_client = MCPClient(port=7443)

# LLM server client
llm_client = MCPClient(port=7444)

# Use tools from both servers
main_client.call_tool("execute_command", command="ls")
llm_client.call_tool("generate_code", prompt="Write a function to sort a list")
```

## Dependencies

The LLM server requires additional dependencies that are not needed by the main server:

- `torch`: PyTorch for local model support
- `transformers`: Hugging Face Transformers
- `anthropic`: Claude API client
- `openai`: OpenAI API client

These dependencies are optional and can be installed using the `llm` extra:

```bash
uv pip install -e ".[llm]"
```

## Monitoring

The server includes OpenTelemetry integration for monitoring:

- Distributed tracing
- Metrics collection
- Error tracking
- Performance monitoring

Metrics are exported to the OpenTelemetry collector at `http://localhost:4317`.
</file>

<file path="metrics.py">
"""
Metrics module for the MCP server.
"""
from opentelemetry import metrics
from opentelemetry.sdk.metrics import MeterProvider
from opentelemetry.sdk.metrics.export import PeriodicExportingMetricReader
from opentelemetry.exporter.otlp.proto.grpc.metric_exporter import OTLPMetricExporter
# Initialize metrics provider
meter_provider = MeterProvider(
    metric_readers=[PeriodicExportingMetricReader(
        OTLPMetricExporter(endpoint="http://localhost:4317")
    )]
)
metrics.set_meter_provider(meter_provider)
# Create meter
meter = metrics.get_meter("mcp")
# Create metrics
tool_duration = meter.create_histogram(
    name="mcp.tool.duration",
    description="Duration of MCP tool execution",
    unit="s"
)
tool_calls = meter.create_counter(
    name="mcp.tool.calls",
    description="Number of MCP tool calls",
    unit="1"
)
tool_errors = meter.create_counter(
    name="mcp.tool.errors",
    description="Number of MCP tool errors",
    unit="1"
)
active_sessions_counter = meter.create_up_down_counter(
    name="mcp.sessions.active",
    description="Number of active MCP sessions",
    unit="1"
)
def get_meter():
    """Get the MCP meter."""
    return meter
</file>

<file path="pyproject.toml">
[project]
name = "python-server-mcp"
version = "0.1.0"
description = "Add your description here"
readme = "README.md"
requires-python = ">=3.13"
dependencies = [
    "mcp[cli]>=1.5.0",
    "opentelemetry-api>=1.31.1",
    "opentelemetry-exporter-otlp>=1.31.1",
    "opentelemetry-sdk>=1.31.1",
    "psutil>=7.0.0",
    "pytest>=7.4.0",
    "pytest-cov>=4.1.0",
    "pytest-mock>=3.11.1",
    "PyYAML>=6.0.1",
    "tree-sitter>=0.24.0",
]

[project.optional-dependencies]
llm = [
    "torch>=2.2.0",
    "transformers>=4.38.0",
    "anthropic>=0.18.0",
    "openai>=1.12.0",
]

[tool.setuptools]
packages = ["server"]

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
addopts = "--verbose --cov=server --cov-report=term-missing --cov-report=html"
filterwarnings = [
    "ignore::DeprecationWarning",
    "ignore::UserWarning",
]

[tool.coverage.run]
source = ["server"]
omit = [
    "tests/*",
    "**/__init__.py",
]

[tool.coverage.report]
exclude_lines = [
    "pragma: no cover",
    "def __repr__",
    "if __name__ == .__main__.:",
    "raise NotImplementedError",
    "pass",
    "raise ImportError",
]
fail_under = 90

[dependency-groups]
dev = [
    "tree-sitter>=0.24.0",
]
</file>

<file path="README.md">
# Terminal Command Runner MCP Server

A powerful Model Control Protocol (MCP) server that provides terminal command execution and file system management capabilities for AI assistants through a RESTful API interface.

## 🌟 Features

### Terminal Management
- Execute commands with configurable timeouts
- Manage long-running processes in the background
- Fetch output from active command sessions
- List all active sessions and system processes
- Terminate or kill processes
- Command blacklisting for security

### File System Operations
- Read and write files
- Create directories
- List directory contents
- Move/rename files
- Search for files using glob patterns
- Get detailed file information

### Advanced Features
- Precise text editing with search and replace
- System information retrieval
- Mathematical expression evaluation

## 🚀 Quick Start

### Prerequisites
- Python 3.13+
- [uv](https://github.com/astral-sh/uv) package manager (recommended)

### Installation

1. Clone the repository:
```bash
git clone https://github.com/neoforge-dev/neoforge-mcp-server.git
cd python-server-mcp
```

2. Create a virtual environment and install dependencies:
```bash
uv venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
uv pip install -e .
```

3. Start the MCP server:
```bash
python server.py
```

The server will start on http://0.0.0.0:8000.

## 🧪 Testing

The project includes a comprehensive test suite to verify functionality:

```bash
# Install test dependencies
uv pip install -r requirements.txt

# Run all tests
./run_tests.py

# Run tests with coverage report
./run_tests.py --coverage

# Run tests with verbose output
./run_tests.py --verbose

# Run specific test file
./run_tests.py --test tests/test_command_execution.py
```

The test suite covers:
- Command execution and process management
- File system operations
- System utilities
- Security features

## 🔧 Configuration

Configure Cursor to use this MCP service by adding it to your `~/.cursor/mcp.json` file:

```json
{
  "mcpServers": {
    "NeoMCP": {
      "command": "/path/to/uv",
      "args": [
        "run",
        "--with",
        "mcp[cli]",
        "mcp",
        "run",
        "/path/to/python-server-mcp/server.py"
      ]
    }
  }
}
```

## 📖 API Reference

### Terminal Tools
- `execute_command`: Run commands with configurable timeouts
- `read_output`: Get output from running processes
- `force_terminate`: Stop a running command
- `list_sessions`: Show all active command sessions
- `list_processes`: View all system processes
- `kill_process`: Kill processes by PID
- `block_command`: Add commands to the blacklist
- `unblock_command`: Remove commands from the blacklist

### File System Tools
- `read_file`: Read file contents
- `write_file`: Write data to a file
- `create_directory`: Create new directories
- `list_directory`: List contents of a directory
- `move_file`: Move or rename files and directories
- `search_files`: Find files matching patterns
- `get_file_info`: Get detailed file information

### Edit Tools
- `edit_block`: Apply precise text replacements using diff-like syntax

### System Tools
- `system_info` (resource): Get detailed system information
- `calculate`: Evaluate mathematical expressions

## 🔒 Security Considerations

- The server implements command blacklisting to prevent dangerous commands
- File size limits for read operations
- Expression evaluation safeguards
- Default command safety checks

## 🤝 Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## 📄 License

This project is licensed under the MIT License - see the LICENSE file for details.
</file>

<file path="requirements.txt">
# Core dependencies
mcp>=0.1.0
pytest>=7.4.0
pytest-cov>=4.1.0
pytest-xdist>=3.3.1
pytest-mock>=3.11.1
coverage>=7.3.0
hypothesis>=6.82.6
click>=8.1.0
requests>=2.31.0
rich>=13.7.0
opentelemetry-api>=1.21.0
opentelemetry-sdk>=1.21.0
opentelemetry-exporter-prometheus>=0.52b1
opentelemetry-exporter-otlp-proto-http>=1.21.0
prometheus-client>=0.19.0
psutil>=5.9.0
bandit>=1.7.8
ruff>=0.3.0
PyYAML>=6.0.1
tree-sitter>=0.20.1

# LLM dependencies (for llm_server.py)
torch>=2.2.0
transformers>=4.38.0
anthropic>=0.18.0
openai>=1.12.0
</file>

<file path="run_servers.py">
"""
Script to run both MCP servers (core and LLM).
"""
import multiprocessing
import uvicorn
import os
import signal
import sys
def run_core_server():
    """Run the core MCP server on port 7443."""
    from server.core import mcp
    uvicorn.run(mcp.sse_app, host="0.0.0.0", port=7443)
def run_llm_server():
    """Run the LLM MCP server on port 7444."""
    from server.llm import mcp
    uvicorn.run(mcp.sse_app, host="0.0.0.0", port=7444)
def signal_handler(signum, frame):
    """Handle termination signals."""
    print("\nShutting down servers...")
    sys.exit(0)
if __name__ == "__main__":
    # Set up signal handlers
    signal.signal(signal.SIGINT, signal_handler)
    signal.signal(signal.SIGTERM, signal_handler)
    # Start servers in separate processes
    core_process = multiprocessing.Process(target=run_core_server)
    llm_process = multiprocessing.Process(target=run_llm_server)
    try:
        print("Starting Core MCP server on port 7443...")
        core_process.start()
        print("Starting LLM MCP server on port 7444...")
        llm_process.start()
        # Wait for processes to complete
        core_process.join()
        llm_process.join()
    except KeyboardInterrupt:
        print("\nShutting down servers...")
    finally:
        # Clean up processes
        core_process.terminate()
        llm_process.terminate()
        core_process.join()
        llm_process.join()
</file>

<file path="run_tests.py">
#!/usr/bin/env python3
"""Test runner for Terminal Command Runner MCP."""
import os
import sys
import argparse
import subprocess
def run_tests(coverage=False, verbose=False, specific_test=None):
    """Run the test suite with the specified options."""
    # Build the command
    cmd = ["python", "-m", "pytest"]
    # Add options
    if coverage:
        cmd.extend(["--cov=server", "--cov-report=term"])
    if verbose:
        cmd.append("-v")
    # Add specific test if provided
    if specific_test:
        cmd.append(specific_test)
    # Run the tests
    print(f"Running command: {' '.join(cmd)}")
    result = subprocess.run(cmd)
    return result.returncode
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run Terminal Command Runner MCP tests")
    parser.add_argument("--coverage", "-c", action="store_true", help="Run with coverage report")
    parser.add_argument("--verbose", "-v", action="store_true", help="Run with verbose output")
    parser.add_argument("--test", "-t", help="Run specific test file or test")
    args = parser.parse_args()
    exit_code = run_tests(
        coverage=args.coverage,
        verbose=args.verbose,
        specific_test=args.test
    )
    sys.exit(exit_code)
</file>

<file path="test_manually.py">
import os
import tempfile
import shutil
import sys
import json
# Import the server module
import server
def test_read_file():
    """Test reading file contents."""
    # Create a temporary directory and file
    temp_dir = tempfile.mkdtemp()
    try:
        # Create a sample file
        sample_file = os.path.join(temp_dir, "sample.txt")
        with open(sample_file, "w") as f:
            f.write("This is a sample test file.\nIt has multiple lines.\nThird line.")
        # Test with a valid file
        result = server.read_file(sample_file)
        print(f"Read file result: {result}")
        assert result["success"] is True
        assert "This is a sample test file." in result["content"]
        assert "Third line." in result["content"]
        assert result["size"] > 0
        # Test with a non-existent file
        result = server.read_file("/path/to/nonexistent/file")
        print(f"Read non-existent file result: {result}")
        assert result["success"] is False
        assert "error" in result
        print("read_file test passed!")
    finally:
        # Clean up
        shutil.rmtree(temp_dir)
def test_write_file():
    """Test writing content to a file."""
    temp_dir = tempfile.mkdtemp()
    try:
        test_file_path = os.path.join(temp_dir, "test_write.txt")
        test_content = "Hello, this is a test file content.\nSecond line."
        # Write to file
        result = server.write_file(test_file_path, test_content)
        print(f"Write file result: {result}")
        # Verify result
        assert result["success"] is True
        assert os.path.exists(test_file_path)
        # Verify file content
        with open(test_file_path, "r") as f:
            content = f.read()
            assert content == test_content
        print("write_file test passed!")
    finally:
        # Clean up
        shutil.rmtree(temp_dir)
def test_create_directory():
    """Test creating a directory."""
    temp_dir = tempfile.mkdtemp()
    try:
        new_dir_path = os.path.join(temp_dir, "new_directory")
        # Create directory
        result = server.create_directory(new_dir_path)
        print(f"Create directory result: {result}")
        # Verify result
        assert result["success"] is True
        assert os.path.exists(new_dir_path)
        assert os.path.isdir(new_dir_path)
        # Test creating a directory that already exists
        result = server.create_directory(new_dir_path)
        print(f"Create existing directory result: {result}")
        assert result["success"] is False  # Should fail
        print("create_directory test passed!")
    finally:
        # Clean up
        shutil.rmtree(temp_dir)
def test_list_directory():
    """Test listing directory contents."""
    temp_dir = tempfile.mkdtemp()
    try:
        # Create some test files and directories
        os.mkdir(os.path.join(temp_dir, "test_dir"))
        with open(os.path.join(temp_dir, "test1.txt"), "w") as f:
            f.write("Test file 1")
        with open(os.path.join(temp_dir, "test2.txt"), "w") as f:
            f.write("Test file 2")
        with open(os.path.join(temp_dir, ".hidden"), "w") as f:
            f.write("Hidden file")
        # List directory without showing hidden files
        result = server.list_directory(temp_dir, show_hidden=False)
        print(f"List directory (no hidden) result: {result}")
        # Verify result
        assert result["success"] is True
        assert len(result["contents"]) == 3  # test_dir, test1.txt, test2.txt
        assert not any(item["name"] == ".hidden" for item in result["contents"])
        # List directory showing hidden files
        result = server.list_directory(temp_dir, show_hidden=True)
        print(f"List directory (with hidden) result: {result}")
        # Verify result
        assert result["success"] is True
        assert len(result["contents"]) == 4  # test_dir, test1.txt, test2.txt, .hidden
        assert any(item["name"] == ".hidden" for item in result["contents"])
        print("list_directory test passed!")
    finally:
        # Clean up
        shutil.rmtree(temp_dir)
def test_move_file():
    """Test moving/renaming a file."""
    temp_dir = tempfile.mkdtemp()
    try:
        # Create a test file
        source_path = os.path.join(temp_dir, "source.txt")
        with open(source_path, "w") as f:
            f.write("Test file content")
        # Move the file
        destination_path = os.path.join(temp_dir, "destination.txt")
        result = server.move_file(source_path, destination_path)
        print(f"Move file result: {result}")
        # Verify result
        assert result["success"] is True
        assert not os.path.exists(source_path)
        assert os.path.exists(destination_path)
        # Verify file content
        with open(destination_path, "r") as f:
            content = f.read()
            assert content == "Test file content"
        print("move_file test passed!")
    finally:
        # Clean up
        shutil.rmtree(temp_dir)
def test_search_files():
    """Test searching for files matching a pattern."""
    temp_dir = tempfile.mkdtemp()
    try:
        # Create test files
        os.makedirs(os.path.join(temp_dir, "subdir"))
        files = [
            "test1.txt",
            "test2.log",
            "hello.txt",
            "subdir/nested.txt",
            "subdir/data.csv"
        ]
        for file_path in files:
            full_path = os.path.join(temp_dir, file_path)
            with open(full_path, "w") as f:
                f.write(f"Content of {file_path}")
        # Search for .txt files recursively
        result = server.search_files(temp_dir, "*.txt", recursive=True)
        print(f"Search files result (recursive): {result}")
        # Verify result
        assert result["success"] is True
        assert len(result["matches"]) == 3  # test1.txt, hello.txt, subdir/nested.txt
        # Search with non-recursive
        result = server.search_files(temp_dir, "*.txt", recursive=False)
        print(f"Search files result (non-recursive): {result}")
        # Verify result - should only find files in the top directory
        assert result["success"] is True
        assert len(result["matches"]) == 2  # test1.txt, hello.txt
        print("search_files test passed!")
    finally:
        # Clean up
        shutil.rmtree(temp_dir)
def test_get_file_info():
    """Test getting file information."""
    temp_dir = tempfile.mkdtemp()
    try:
        # Create a sample file
        sample_file = os.path.join(temp_dir, "sample.txt")
        with open(sample_file, "w") as f:
            f.write("This is a sample test file.")
        # Get info for a regular file
        result = server.get_file_info(sample_file)
        print(f"Get file info result: {result}")
        # Verify result
        assert result["success"] is True
        assert result["exists"] is True
        assert result["type"] == "file"
        assert result["size"] > 0
        assert "modified" in result
        assert "permissions" in result
        # Get info for a directory
        result = server.get_file_info(temp_dir)
        print(f"Get directory info result: {result}")
        # Verify result
        assert result["success"] is True
        assert result["exists"] is True
        assert result["type"] == "directory"
        # Get info for a non-existent file
        result = server.get_file_info("/path/to/nonexistent/file")
        print(f"Get non-existent file info result: {result}")
        # Verify result
        assert result["success"] is True  # The operation succeeded even though file doesn't exist
        assert result["exists"] is False
        print("get_file_info test passed!")
    finally:
        # Clean up
        shutil.rmtree(temp_dir)
def run_tests():
    """Run all tests"""
    tests = [
        test_read_file,
        test_write_file,
        test_create_directory,
        test_list_directory,
        test_move_file,
        test_search_files,
        test_get_file_info
    ]
    passed = 0
    failed = 0
    for test in tests:
        try:
            print(f"\nRunning test: {test.__name__}")
            test()
            passed += 1
        except Exception as e:
            print(f"Test {test.__name__} failed: {e}")
            failed += 1
    print(f"\nTest Results: {passed} passed, {failed} failed")
    return failed == 0
if __name__ == "__main__":
    success = run_tests()
    sys.exit(0 if success else 1)
</file>

<file path="test_system_manually.py">
import os
import sys
import time
import subprocess
import threading
# Import the server module
import server
def test_system_info():
    """Test retrieving system information."""
    result = server.system_info()
    print(f"System info result: {result}")
    # Verify the result contains expected fields
    assert "platform" in result
    assert "python_version" in result
    assert "system" in result
    assert "machine" in result
    assert "processor" in result
    print("system_info test passed!")
def test_calculate():
    """Test the calculate utility function."""
    # Test basic arithmetic
    result = server.calculate("2 + 3 * 4")
    print(f"Calculate arithmetic result: {result}")
    assert result["success"] is True
    assert result["result"] == 14
    # Test more complex expressions
    result = server.calculate("sqrt(16) + pow(2, 3)")
    print(f"Calculate complex result: {result}")
    assert result["success"] is True
    assert result["result"] == 12.0
    # Test invalid expression
    result = server.calculate("invalid expression")
    print(f"Calculate invalid result: {result}")
    assert "error" in result
    # Test potentially dangerous expressions
    result = server.calculate("__import__('os').system('echo hack')")
    print(f"Calculate dangerous result: {result}")
    assert "error" in result
    print("calculate test passed!")
def test_edit_block():
    """Test the edit_block functionality."""
    # Create a temporary file for testing
    test_file = "test_edit_block.txt"
    try:
        with open(test_file, "w") as f:
            f.write("This is a test file\nfor edit_block functionality\nIt has three lines")
        # Test edit block functionality
        edit_block = f"""@@ {test_file}
This is the NEW CONTENT
with completely different text
than the original"""
        result = server.edit_block(edit_block)
        print(f"Edit block result: {result}")
        # Verify the result
        assert result["success"] is True
        # Verify the file content was updated
        with open(test_file, "r") as f:
            content = f.read()
        assert "NEW CONTENT" in content
        assert "completely different text" in content
        print("edit_block test passed!")
    finally:
        # Clean up
        if os.path.exists(test_file):
            os.remove(test_file)
def test_list_processes():
    """Test listing system processes."""
    result = server.list_processes()
    print(f"List processes result (showing first 3): {result['processes'][:3]}")
    # Verify result structure
    assert "processes" in result
    assert isinstance(result["processes"], list)
    assert len(result["processes"]) > 0
    # Check first process has expected fields
    first_process = result["processes"][0]
    assert "pid" in first_process
    print("list_processes test passed!")
def test_command_execution():
    """Test basic command execution and control."""
    # Execute a simple command
    result = server.execute_command("echo 'test command'")
    print(f"Execute command result: {result}")
    assert "test command" in result["stdout"]
    assert result["exit_code"] == 0
    # Execute a command with background processing
    if sys.platform == "win32":
        cmd = "ping -n 5 localhost"
    else:
        cmd = "sleep 5"
    result = server.execute_command(cmd, timeout=1, allow_background=True)
    print(f"Background command result: {result}")
    # Verify it's running in the background
    assert result["pid"] is not None
    # Get the PID
    pid = result["pid"]
    # Read output while it's running
    time.sleep(1)
    output = server.read_output(pid)
    print(f"Read output result: {output}")
    # Force terminate the process
    terminate_result = server.force_terminate(pid)
    print(f"Terminate result: {terminate_result}")
    assert terminate_result["success"] is True
    # Verify it's no longer running
    time.sleep(0.5)
    try:
        os.kill(pid, 0)
        process_running = True
    except OSError:
        process_running = False
    assert not process_running
    print("command_execution test passed!")
def test_block_unblock_command():
    """Test adding and removing commands from the blacklist."""
    test_command = "test_block_command"
    # Ensure command is not in the blacklist initially
    if test_command in server.blacklisted_commands:
        server.blacklisted_commands.remove(test_command)
    # Block the command
    result = server.block_command(test_command)
    print(f"Block command result: {result}")
    assert result["success"] is True
    # Verify command is now in the blacklist
    assert test_command in server.blacklisted_commands
    # Unblock the command
    result = server.unblock_command(test_command)
    print(f"Unblock command result: {result}")
    assert result["success"] is True
    # Verify command is no longer in the blacklist
    assert test_command not in server.blacklisted_commands
    print("block_unblock_command test passed!")
def run_tests():
    """Run all tests"""
    tests = [
        test_system_info,
        test_calculate,
        test_edit_block,
        test_list_processes,
        test_command_execution,
        test_block_unblock_command
    ]
    passed = 0
    failed = 0
    for test in tests:
        try:
            print(f"\nRunning test: {test.__name__}")
            test()
            passed += 1
        except Exception as e:
            print(f"Test {test.__name__} failed: {e}")
            import traceback
            traceback.print_exc()
            failed += 1
    print(f"\nTest Results: {passed} passed, {failed} failed")
    return failed == 0
if __name__ == "__main__":
    success = run_tests()
    sys.exit(0 if success else 1)
</file>

<file path="workspace.py">
#!/usr/bin/env python3
import json
import os
import shutil
import tempfile
import time
from dataclasses import dataclass, asdict
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Set
from rich.console import Console
from rich.table import Table
console = Console()
@dataclass
class WorkspaceConfig:
    """Configuration for a workspace"""
    name: str
    description: str
    created_at: str
    updated_at: str
    tools: List[str]
    environment: Dict[str, str]
    paths: Set[str]
    settings: Dict[str, any]
class WorkspaceManager:
    """Manages MCP workspaces"""
    def __init__(self, base_path: str = None):
        """Initialize workspace manager"""
        self.base_path = base_path or os.path.expanduser('~/.mcp/workspaces')
        self.current_workspace = None
        self._ensure_base_path()
    def _ensure_base_path(self):
        """Ensure base path exists"""
        os.makedirs(self.base_path, exist_ok=True)
    def create_workspace(self, name: str, description: str = "") -> WorkspaceConfig:
        """Create a new workspace"""
        workspace_path = os.path.join(self.base_path, name)
        if os.path.exists(workspace_path):
            raise ValueError(f"Workspace '{name}' already exists")
        # Create workspace structure
        os.makedirs(workspace_path)
        os.makedirs(os.path.join(workspace_path, 'tools'))
        os.makedirs(os.path.join(workspace_path, 'data'))
        os.makedirs(os.path.join(workspace_path, 'logs'))
        os.makedirs(os.path.join(workspace_path, 'temp'))
        # Create workspace config
        config = WorkspaceConfig(
            name=name,
            description=description,
            created_at=datetime.now().isoformat(),
            updated_at=datetime.now().isoformat(),
            tools=[],
            environment={},
            paths=set(),
            settings={}
        )
        # Save config
        self._save_config(config)
        console.print(f"[green]Created workspace: {name}[/green]")
        return config
    def list_workspaces(self) -> List[WorkspaceConfig]:
        """List all workspaces"""
        workspaces = []
        for name in os.listdir(self.base_path):
            path = os.path.join(self.base_path, name)
            if os.path.isdir(path):
                try:
                    config = self._load_config(name)
                    workspaces.append(config)
                except Exception as e:
                    console.print(f"[yellow]Warning: Could not load workspace '{name}': {e}[/yellow]")
        return workspaces
    def get_workspace(self, name: str) -> Optional[WorkspaceConfig]:
        """Get workspace by name"""
        try:
            return self._load_config(name)
        except FileNotFoundError:
            return None
    def delete_workspace(self, name: str):
        """Delete a workspace"""
        workspace_path = os.path.join(self.base_path, name)
        if not os.path.exists(workspace_path):
            raise ValueError(f"Workspace '{name}' does not exist")
        if self.current_workspace and self.current_workspace.name == name:
            self.current_workspace = None
        shutil.rmtree(workspace_path)
        console.print(f"[green]Deleted workspace: {name}[/green]")
    def activate_workspace(self, name: str) -> WorkspaceConfig:
        """Activate a workspace"""
        config = self._load_config(name)
        if not config:
            raise ValueError(f"Workspace '{name}' does not exist")
        self.current_workspace = config
        # Set environment variables
        os.environ.update(config.environment)
        # Add paths to Python path
        for path in config.paths:
            if path not in sys.path:
                sys.path.append(path)
        console.print(f"[green]Activated workspace: {name}[/green]")
        return config
    def deactivate_workspace(self):
        """Deactivate current workspace"""
        if not self.current_workspace:
            return
        # Remove environment variables
        for key in self.current_workspace.environment:
            if key in os.environ:
                del os.environ[key]
        # Remove paths from Python path
        for path in self.current_workspace.paths:
            if path in sys.path:
                sys.path.remove(path)
        name = self.current_workspace.name
        self.current_workspace = None
        console.print(f"[green]Deactivated workspace: {name}[/green]")
    def add_tool(self, name: str, tool_path: str):
        """Add a tool to current workspace"""
        if not self.current_workspace:
            raise RuntimeError("No workspace is active")
        if not os.path.exists(tool_path):
            raise ValueError(f"Tool path does not exist: {tool_path}")
        # Copy tool to workspace
        workspace_tool_path = os.path.join(
            self.base_path,
            self.current_workspace.name,
            'tools',
            os.path.basename(tool_path)
        )
        shutil.copy2(tool_path, workspace_tool_path)
        # Update config
        if name not in self.current_workspace.tools:
            self.current_workspace.tools.append(name)
            self._save_config(self.current_workspace)
        console.print(f"[green]Added tool '{name}' to workspace[/green]")
    def remove_tool(self, name: str):
        """Remove a tool from current workspace"""
        if not self.current_workspace:
            raise RuntimeError("No workspace is active")
        if name not in self.current_workspace.tools:
            raise ValueError(f"Tool '{name}' not found in workspace")
        # Remove tool file
        tool_path = os.path.join(
            self.base_path,
            self.current_workspace.name,
            'tools',
            name
        )
        if os.path.exists(tool_path):
            os.remove(tool_path)
        # Update config
        self.current_workspace.tools.remove(name)
        self._save_config(self.current_workspace)
        console.print(f"[green]Removed tool '{name}' from workspace[/green]")
    def set_environment(self, key: str, value: str):
        """Set environment variable in current workspace"""
        if not self.current_workspace:
            raise RuntimeError("No workspace is active")
        self.current_workspace.environment[key] = value
        os.environ[key] = value
        self._save_config(self.current_workspace)
        console.print(f"[green]Set environment variable: {key}[/green]")
    def add_path(self, path: str):
        """Add path to current workspace"""
        if not self.current_workspace:
            raise RuntimeError("No workspace is active")
        path = os.path.abspath(path)
        if not os.path.exists(path):
            raise ValueError(f"Path does not exist: {path}")
        self.current_workspace.paths.add(path)
        if path not in sys.path:
            sys.path.append(path)
        self._save_config(self.current_workspace)
        console.print(f"[green]Added path to workspace: {path}[/green]")
    def remove_path(self, path: str):
        """Remove path from current workspace"""
        if not self.current_workspace:
            raise RuntimeError("No workspace is active")
        path = os.path.abspath(path)
        if path in self.current_workspace.paths:
            self.current_workspace.paths.remove(path)
            if path in sys.path:
                sys.path.remove(path)
            self._save_config(self.current_workspace)
        console.print(f"[green]Removed path from workspace: {path}[/green]")
    def update_settings(self, settings: Dict[str, any]):
        """Update workspace settings"""
        if not self.current_workspace:
            raise RuntimeError("No workspace is active")
        self.current_workspace.settings.update(settings)
        self._save_config(self.current_workspace)
        console.print("[green]Updated workspace settings[/green]")
    def get_temp_dir(self) -> str:
        """Get temporary directory for current workspace"""
        if not self.current_workspace:
            raise RuntimeError("No workspace is active")
        temp_dir = os.path.join(
            self.base_path,
            self.current_workspace.name,
            'temp'
        )
        os.makedirs(temp_dir, exist_ok=True)
        return temp_dir
    def _load_config(self, name: str) -> WorkspaceConfig:
        """Load workspace configuration"""
        config_path = os.path.join(self.base_path, name, 'config.json')
        if not os.path.exists(config_path):
            raise FileNotFoundError(f"Workspace config not found: {config_path}")
        with open(config_path, 'r') as f:
            data = json.load(f)
            data['paths'] = set(data['paths'])
            return WorkspaceConfig(**data)
    def _save_config(self, config: WorkspaceConfig):
        """Save workspace configuration"""
        config_path = os.path.join(self.base_path, config.name, 'config.json')
        data = asdict(config)
        data['paths'] = list(data['paths'])
        data['updated_at'] = datetime.now().isoformat()
        with open(config_path, 'w') as f:
            json.dump(data, f, indent=2)
    def show_info(self):
        """Show information about current workspace"""
        if not self.current_workspace:
            console.print("[yellow]No workspace is active[/yellow]")
            return
        # Create workspace info table
        info_table = Table(title="Workspace Information")
        info_table.add_column("Property")
        info_table.add_column("Value")
        info_table.add_row("Name", self.current_workspace.name)
        info_table.add_row("Description", self.current_workspace.description)
        info_table.add_row("Created", self.current_workspace.created_at)
        info_table.add_row("Updated", self.current_workspace.updated_at)
        console.print(info_table)
        # Create tools table
        if self.current_workspace.tools:
            tools_table = Table(title="Tools")
            tools_table.add_column("Name")
            for tool in sorted(self.current_workspace.tools):
                tools_table.add_row(tool)
            console.print(tools_table)
        # Create environment table
        if self.current_workspace.environment:
            env_table = Table(title="Environment Variables")
            env_table.add_column("Key")
            env_table.add_column("Value")
            for key, value in sorted(self.current_workspace.environment.items()):
                env_table.add_row(key, value)
            console.print(env_table)
        # Create paths table
        if self.current_workspace.paths:
            paths_table = Table(title="Paths")
            paths_table.add_column("Path")
            for path in sorted(self.current_workspace.paths):
                paths_table.add_row(path)
            console.print(paths_table)
        # Create settings table
        if self.current_workspace.settings:
            settings_table = Table(title="Settings")
            settings_table.add_column("Key")
            settings_table.add_column("Value")
            for key, value in sorted(self.current_workspace.settings.items()):
                settings_table.add_row(key, str(value))
            console.print(settings_table)
def create_workspace_manager(base_path: Optional[str] = None) -> WorkspaceManager:
    """Create and return a workspace manager instance"""
    return WorkspaceManager(base_path)
</file>

</files>
