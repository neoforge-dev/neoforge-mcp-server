# LLM MCP Server Configuration

# Server identification
name: "llm_mcp"
version: "1.0.0"
port: 7444

# Logging
log_level: "INFO"
log_file: "llm_mcp.log"
log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
log_rotation: "1 day"
log_retention: 7

# Security
enable_auth: true
auth_token: null
allowed_origins: ["*"]

# Resource limits
max_processes: 4
max_memory_percent: 90.0
max_cpu_percent: 90.0
max_disk_percent: 90.0
max_runtime: 3600
check_resources: true
resource_limits:
  cpu_percent: 95.0
  memory_percent: 95.0
  disk_percent: 95.0

# Connection settings
connection_timeout: 30
keep_alive_timeout: 60
max_requests_per_connection: 1000

# File operations
enable_file_operations: true
max_file_size: 104857600 # 100MB
allowed_extensions: []

# Local model settings
enable_local_models: true
model_path: "models"
local_model_path: "models/local"
model_cache_size: 100

# LLM generation settings
max_tokens: 2048
temperature: 0.7
top_p: 0.9
frequency_penalty: 0.0
presence_penalty: 0.0
stop_sequences: []

# API keys
anthropic_api_key: null
openai_api_key: null

# Monitoring
enable_metrics: true
metrics_port: 9090
enable_health_checks: true
health_check_interval: 30

# Development
debug: false
reload: false
reload_dirs: []

# Cache settings
enable_cache: true
cache_size: 1000
cache_ttl: 3600

# Rate limiting
enable_rate_limiting: true
rate_limit: 100
rate_limit_window: 60

# SSL/TLS settings
enable_ssl: false
ssl_cert: null
ssl_key: null

# Proxy settings
enable_proxy: false
proxy_protocol: false
trusted_proxies: []

# Compression settings
enable_compression: true
compression_level: 6

# Timeout settings
request_timeout: 30
response_timeout: 30
connect_timeout: 5

# Worker settings
worker_class: "uvicorn.workers.UvicornWorker"
worker_connections: 1000
worker_timeout: 30

# API documentation
enable_docs: true
docs_url: "/docs"
redoc_url: "/redoc"
openapi_url: "/openapi.json"
