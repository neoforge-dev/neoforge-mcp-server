# Server identification
name: "neollm_mcp"
version: "1.0.0"
port: 7448

# Logging
log_level: "INFO"
log_file: "neollm_mcp.log"
log_format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
log_rotation: "1 day"
log_retention: 7

# Security
enable_auth: true
auth_token: null
allowed_origins: ["*"]
api_keys: {}

# Resource limits
max_processes: 4
max_memory_percent: 90.0
max_cpu_percent: 90.0
max_disk_percent: 90.0
max_runtime: 3600
check_resources: true
resource_limits:
  cpu_percent: 95.0
  memory_percent: 95.0
  disk_percent: 95.0

# Connection settings
connection_timeout: 30
keep_alive_timeout: 60
max_requests_per_connection: 1000

# File operations
enable_file_operations: true
max_file_size: 104857600 # 100MB
allowed_extensions: []

# LLM settings
enable_local_models: true
local_model_path: "TinyLlama/TinyLlama-1.1B-Chat-v1.0"
max_tokens: 2048
temperature: 0.7
top_p: 0.95
top_k: 50
repetition_penalty: 1.1
enable_quantization: false
enable_caching: true
cache_size: 1000
enable_streaming: true

# Monitoring
enable_metrics: true
metrics_port: 9090
enable_tracing: true
tracing_endpoint: "http://localhost:4317"

# Development settings
debug: false
reload: false
reload_dirs: []

# API documentation
enable_docs: true
docs_url: "/docs"
redoc_url: "/redoc"
openapi_url: "/openapi.json"
